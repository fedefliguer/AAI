[["index.html", "Aprendizaje automático interpretable Una guia para hacer que los modelos de caja negra sean explicables. Prefacio", " Aprendizaje automático interpretable Una guia para hacer que los modelos de caja negra sean explicables. Christoph Molnar 2021-08-17 Prefacio El aprendizaje automático tiene un gran potencial para mejorar productos, procesos e investigación. Pero las computadoras generalmente no explican sus predicciones, lo cual es una barrera para la adopción del aprendizaje automático. Este libro trata de hacer que los modelos de aprendizaje automático y sus decisiones sean interpretables. Después de explorar los conceptos de interpretabilidad, aprenderás sobre modelos simples e interpretables como árboles de decisión, reglas de decisión y regresión lineal. Los capítulos posteriores se centran en métodos generales independientes del modelo para interpretar modelos de caja negra como la importancia de la característica y los efectos locales acumulados, y explicar las predicciones individuales con valores de Shapley y LIME. Todos los métodos de interpretación se explican en profundidad y se analizan críticamente. ¿Cómo funcionan detrás de escena? ¿Cuales son sus fortalezas y debilidades? ¿Cómo se pueden interpretar sus resultados? Este libro te permitirá seleccionar y aplicar correctamente el método de interpretación más adecuado para tu proyecto de aprendizaje automático. El libro se enfoca en modelos de aprendizaje automático para datos tabulares (también llamados datos relacionales o estructurados) y no se enfoca en visión artifical, ni en procesamiento de lenguaje natural. Se recomienda leer el libro para profesionales del aprendizaje automático, científicos de datos, estadísticos y cualquier otra persona interesada en hacer que los modelos de aprendizaje automático sean interpretables. Puedes comprar la versión en PDF y en e-book (epub, mobi) del libro en leanpub.com. Puedes comprar la versión en papel en lulu.com. Sobre el autor: Mi nombre es Christoph Molnar, Soy estadístico y un profesional del aprendizaje automático. Mi objetivo es hacer al aprendizaje automático interpretable. Mail: christoph.molnar.ai@gmail.com Website: https://christophm.github.io/ Seguime en Twitter! @ChristophMolnar Portada por @YvonneDoinel Sobre el traductor: Mi nombre es Federico Fliguer, Soy economista y científico de datos. Encontré este libro durante mi carrera de posgrado, y decidí traducirlo para difundirlo entre la creciente comunidad de habla hispana interesada en el aprendizaje automático. ¡Espero que lo disfrutes! Licencia Este libro está licenciado bajo la licencia Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.  "],["intro.html", "Capítulo 1 Introducción", " Capítulo 1 Introducción Este libro te explica cómo hacer que los modelos de aprendizaje automático (supervisados) sean interpretables. Los capítulos contienen algunas fórmulas matemáticas, pero deberías poder comprender las ideas detrás de los métodos, incluso sin las fórmulas. Este libro no es para personas que intentan aprender el aprendizaje automático desde cero. Si eres nuevo en el aprendizaje automático, hay muchos libros y otros recursos para aprender los conceptos básicos. Recomiendo el libro Los elementos del aprendizaje estadístico de Hastie, Tibshirani y Friedman (2009) 1 y el curso en línea Machine Learning de Andrew Ng en la plataforma de aprendizaje en línea coursera.com para comenzar con el aprendizaje automático. ¡Tanto el libro como el curso están disponibles de forma gratuita! Se publican nuevos métodos para la interpretación de modelos de aprendizaje automático a una velocidad vertiginosa. Mantenerse al día con todo lo que se publica sería una locura y simplemente imposible. Es por eso que no encontrarás los métodos más novedosos y sofisticados en este libro, sino los métodos establecidos y los conceptos básicos de la capacidad de interpretación del aprendizaje automático. Estos conceptos básicos te preparan para hacer que los modelos de aprendizaje automático sean interpretables. La internalización de los conceptos básicos, además, te permitirá comprender y evaluar mejor cualquier documento nuevo sobre interpretabilidad publicado en arxiv.org en los últimos 5 minutos desde que comenzaste a leer este libro (podría estar exagerando la tasa de publicación). Este libro comienza con algunas historias cortas (distópicas) que no son necesarias para entender el libro, pero con suerte te entretendrán y te harán pensar. Luego, el libro explora los conceptos de interpretabilidad del aprendizaje automático. Discutiremos cuándo la interpretabilidad es importante y qué diferentes tipos de explicaciones hay. Los términos utilizados a lo largo de todo el libro se pueden consultar en el Capítulo de terminología. La mayoría de los modelos y métodos explicados se presentan utilizando ejemplos de datos reales que se describen en el Capítulo de conjuntos de datos. Una forma de hacer que el aprendizaje automático sea interpretable es usar modelos interpretables, como modelos lineales o árboles de decisión. La otra opción es el uso de herramientas de interpretación modelo-agnósticas que se pueden aplicar a cualquier modelo supervisado de aprendizaje automático. El capítulo Métodos modelo-agnósticos trata con métodos tales como gráficas de dependencia parcial (PDP) e importancia de la característica de permutación. Los métodos modelo-agnósticos funcionan cambiando la entrada del modelo de aprendizaje automático y midiendo los cambios en la salida de predicción. Los métodos independientes al modelo que devuelven observaciones como explicaciones se analizan en el capítulo Explicaciones basadas en ejemplos. Los métodos independientes al modelo se pueden diferenciar aún más, en función de si explican el comportamiento global del modelo en todas las observaciones o si explican predicciones individuales. Los siguientes métodos explican el comportamiento general del modelo: Gráficos de dependencia parcial, Efectos locales acumulados, Interacción de características, Importancia de características , Modelos sustitutos globales y Prototipos y críticas. Para explicar las predicciones individuales, en cambio, tenemos Modelos sustitutos locales, Explicaciones del valor de Shapley, Explicaciones contrafactuales (y estrechamente relacionados: Ejemplos adversarios). Algunos métodos se pueden usar para explicar ambos aspectos del comportamiento del modelo, tanto el carácter global como las predicciones individuales: Expectativa condicional individual e Instancias influyentes. El libro termina con una perspectiva optimista sobre cómo podría ser el futuro del aprendizaje automático interpretable. Puedes leer el libro de principio a fin o saltar directamente a los métodos que le interesen. ¡Espero que disfrutes la lectura!  Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009). "],["horadelcuento.html", "1.1 Hora del cuento", " 1.1 Hora del cuento Comenzaremos con algunas historias cortas. Cada historia es un ejemplo algo exagerado para el aprendizaje automático interpretable. Si tienes prisa, puedes saltear las historias. Si quieres entretenerte y (des)motivarte, ¡sigue leyendo! El formato está inspirado en los cuentos técnicos de Jack Clark en su Boletín informativo Importación AI. Si te gustan este tipo de historias o si estás interesado en la inteligencia artificial, te recomiendo que te registres. Un rayo nunca golpea dos veces 2030: un laboratorio médico en Suiza ¡Definitivamente no es la peor forma de morir! Tom resumió, tratando de encontrar algo positivo en la tragedia. Sacó la bomba intravenosa. Simplemente murió por las razones equivocadas, agregó Lena. ¡Y con la bomba de morfina equivocada! ¡Solo estamos creando más trabajo para nosotros!, Se quejó Tom mientras desenroscaba la placa posterior de la bomba. Después de quitar todos los tornillos, levantó la placa y la dejó a un lado. Conectó un cable al puerto de diagnóstico. No te estás quejando de tener un trabajo, ¿verdad? Lena le dedicó una sonrisa burlona. Por supuesto que no. ¡Nunca! exclamó con un tono sarcástico. Arrancó la computadora de la bomba. Lena enchufó el otro extremo del cable a su tableta. Muy bien, los diagnósticos se están ejecutando, anunció. Tengo mucha curiosidad por saber qué salió mal. Fue la alta concentración de este material de morfina. Hombre. Quiero decir, normalmente, una bomba rota emite muy poco o nada en absoluto. Pero nunca, ya sabes, algo así, explicó Tom. Lo sé. No tienes que convencerme  Oye, mira eso. Lena levantó su tableta. ¿Ves este pico aquí? Esa es la potencia de la mezcla de analgésicos. ¡Mira! Esta línea muestra el nivel de referencia. El pobre tipo tenía una mezcla de analgésicos en su sistema sanguíneo que podría matarlo 17 veces. Inyectado por nuestra bomba aquí. Y aquí  se deslizó, aquí puedes ver el momento de la muerte del paciente. Entonces, ¿alguna idea de lo que pasó, jefe? Tom le preguntó a su supervisor. Hmm Los sensores parecen estar bien. Frecuencia cardíaca, niveles de oxígeno, glucosa,  Los datos se recopilaron como se esperaba. Algunos valores faltantes en los datos de oxígeno en la sangre, pero eso no es inusual. Mira aquí. Los sensores también han detectado la frecuencia cardíaca lenta del paciente y los niveles extremadamente bajos de cortisol causados por el derivado de la morfina y otros agentes bloqueadores del dolor. Ella continuó pasando el informe de diagnóstico. Tom miraba cautivado la pantalla. Fue su primera investigación de una falla real del dispositivo. Ok, aquí está nuestra primera pieza del rompecabezas. El sistema no pudo enviar una advertencia al canal de comunicación del hospital. La advertencia se activó, pero se rechazó a nivel de protocolo. Podría ser culpa nuestra, pero también podría ser culpa del hospital. Envíe los registros al equipo de IT, le dijo Lena a Tom. Tom asintió con los ojos todavía fijos en la pantalla. Lena continuó: Es extraño. La advertencia también debería haber causado que la bomba se apagara. Pero obviamente no lo hizo. Eso debe ser un error. Algo que control de calidad perdió. Algo realmente malo. Tal vez esté relacionado con el problema del protocolo. Entonces, el sistema de emergencia de la bomba de alguna manera se averió, pero ¿por qué la bomba se llenó de bananas e inyectó tanto analgésico en John Doe? Tom se preguntó. Buena pregunta. Tienes razón. Dejando de lado la falla de emergencia del protocolo, la bomba no debería haber administrado esa cantidad de medicamento. El algoritmo debería haberse detenido por sí solo mucho antes, dado el bajo nivel de cortisol y otras señales de advertencia, explicó Lena. ¿Quizás algo de mala suerte, como una cosa entre un millón, como ser alcanzado por un rayo? Tom le preguntó. No, Tom. Si hubieras leído la documentación que te envié, habrías sabido que la bomba se entrenó primero en experimentos con animales, luego en humanos, para aprender a inyectar la cantidad perfecta de analgésicos en función de la información sensorial. El algoritmo de la bomba puede ser opaco y complejo, pero no es aleatorio. Eso significa que en la misma situación la bomba se comportaría exactamente de la misma manera nuevamente. Nuestro paciente moriría de nuevo. Una combinación o interacción no deseada de las entradas sensoriales debe haber desencadenado el comportamiento erróneo de la bomba. Es por eso que tenemos que profundizar más y descubrir qué sucedió aquí, explicó Lena. Ya veo , respondió Tom, perdido en sus pensamientos. ¿No iba a morir el paciente pronto de todos modos? ¿Por cáncer o algo así? Lena asintió mientras leía el informe del análisis. Tom se levantó y fue a la ventana. Miró hacia afuera, con los ojos fijos en un punto a la distancia. Tal vez la máquina le hizo un favor, ya sabes, al liberarlo del dolor. No más sufrimiento. Tal vez simplemente hizo lo correcto. Como un rayo, pero, ya sabes, uno bueno. Me refiero a la lotería, pero no al azar. Pero por una razón. Si yo fuera la bomba, habría hecho lo mismo. Finalmente levantó la cabeza y lo miró. Seguía mirando algo afuera. Ambos guardaron silencio por unos momentos. Lena volvió a bajar la cabeza y continuó el análisis. No, Tom. Es un error  Solo un maldito error. Perder confianza 2050: una estación de metro en Singapur Se apresuró a la estación Bishan. Sus pensamientos ya estaban en el trabajo. Las pruebas para la nueva arquitectura neuronal deberían completarse por ahora. Ella dirigió el rediseño del Sistema fiscal de predicción de afinidad para entidades individuales del gobierno, que predice si una persona esconde dinero que debe pagar en impuestos. Su equipo ha creado una elegante pieza de ingeniería. Si tiene éxito, el sistema no solo serviría a la oficina de impuestos, sino que también se incorporaría a otros sistemas, como el sistema de alarma contra el terrorismo y el registro comercial. Un día, el gobierno podría incluso integrar las predicciones en el Civic Trust Score. El Civic Trust Score estima cuán confiable es una persona. La estimación afecta cada parte de su vida diaria, como obtener un préstamo o cuánto tiempo tiene que esperar para obtener un nuevo pasaporte. Mientras bajaba la escalera mecánica, se imaginó cómo se vería una integración del sistema de su equipo en el sistema de puntuación de confianza cívica. Rutinariamente pasaba la mano por el lector sin reducir su velocidad de marcha. Su mente estaba ocupada, hasta que sonaron las alarmas en su cerebro. Demasiado tarde. Con la nariz primero, corrió hacia la puerta de entrada del metro y cayó con el trasero al suelo. Se suponía que la puerta se abría  pero no fue así. Atónita, se levantó y miró la pantalla junto a la puerta. Por favor, inténtalo en otro momento, sugirió una carita sonriente en la pantalla. Una persona pasó y, ignorándola, pasó la mano sobre el lector. La puerta se abrió y él entró. La puerta se cerró de nuevo. Se limpió la nariz. Le dolía, pero al menos no sangraba. Trató de abrir la puerta, pero fue rechazada nuevamente. Fue extraño Tal vez su cuenta de transporte público no tenía suficiente crédito. Miró su reloj inteligente para verificar el saldo de la cuenta. Inicio de sesión denegado. ¡Comuníquese con su Oficina de asesoramiento para ciudadanos! su reloj le informó. Una sensación de náuseas la golpeó como un puño en el estómago. Ella sospechaba lo que había sucedido. Para confirmar su teoría, abrió Sniper Guild, un juego de disparos en primera persona. La aplicación se cerró de nuevo automáticamente, lo que confirmó su teoría. Se mareó y volvió a sentarse en el suelo. Solo había una explicación posible: Su puntuación de confianza cívica había bajado. Sustancialmente. Una pequeña caída significaba inconvenientes menores, como no obtener vuelos de primera clase o tener que esperar un poco más para obtener los documentos oficiales. Un puntaje de confianza bajo era raro y quien lo tenía estaba clasificado como una amenaza para la sociedad: Una medida para tratar con estas personas era mantenerlas alejadas de lugares públicos como el metro. El gobierno, además, restringió las transacciones financieras de sujetos con bajos puntajes de confianza cívica. También comenzaron a monitorear activamente su comportamiento en las redes sociales e incluso llegaron a restringir cierto contenido, como los juegos violentos. Se había vuelto más difícil aumentar el puntaje de confianza cívica cuanto más bajo era éste. Las personas con un puntaje muy bajo generalmente nunca se recuperaban. No podía pensar en ninguna razón por la cual su puntuación debería haber caído. La puntuación se basó en el aprendizaje automático. El Civic Trust Score System funcionó como un motor bien engrasado que dirigía la sociedad. El rendimiento del sistema de puntuación de confianza siempre se supervisó de cerca. El aprendizaje automático había mejorado mucho desde principios de siglo. Se había vuelto tan eficiente que las decisiones tomadas por el Trust Score System ya no podían ser cuestionadas. Un sistema infalible. Rió desesperada. Sistema infalible. El sistema rara vez había fallado. Pero falló. Ella debe ser uno de esos casos especiales; un error del sistema; a partir de ahora un paria. Nadie se atrevió a cuestionar el sistema. Estaba demasiado integrado en el gobierno, en la sociedad misma, para ser cuestionado. En los pocos países democráticos restantes, estaba prohibido formar movimientos antidemocráticos, no porque fueran inherentemente maliciosos, sino porque desestabilizarían el sistema actual. La misma lógica se aplica a las ahora algocracias. La crítica en los algoritmos estaba prohibida debido al peligro para el status quo. La confianza algorítmica era el tejido del orden social. Por el bien común, se aceptaron tácitamente raras puntuaciones falsas de confianza. Cientos de otros sistemas de predicción y bases de datos ingresaron al puntaje, lo que hace imposible saber qué causó la caída en su puntaje. Sintió que un gran agujero oscuro se abría dentro y debajo de ella. Con horror, miró al vacío. Su sistema de afinidad fiscal finalmente se integró en el Sistema de puntuación de confianza cívica, pero nunca llegó a saberlo. Clips de papel de Fermi Año 612 AMS (después del asentamiento en Marte): un museo en Marte La historia es aburrida, le susurró Xola a su amiga. Xola, una chica de cabello azul, perseguía uno de los drones del proyector que zumbaba en la habitación con la mano izquierda. La historia es importante, dijo la maestra con voz molesta, mirando a las chicas. Xola se sonrojó. No esperaba que su maestra la escuchara. Xola, ¿qué acabas de aprender? la maestra le preguntó. ¿Que la gente antigua usó todos los recursos del Planeta Terrestre y luego murió? ella preguntó cuidadosamente. No. Calentaron el clima y no fueron las personas, fueron las computadoras y las máquinas. Y es el planeta Tierra, no el planeta Terrestre, agregó otra chica llamada Lin. Xola asintió de acuerdo. Con un toque de orgullo, la maestra sonrió y asintió. Ambos tienen razón. ¿Sabes por qué sucedió? ¿Porque la gente era miope y codiciosa? Xola preguntó. ¡La gente no podía parar sus máquinas! Espetó Lin. Una vez más, ambos tienen razón, decidió la maestra, Pero es mucho más complicado que eso. La mayoría de las personas en ese momento no sabían lo que estaba sucediendo. Algunos vieron los cambios drásticos, pero no pudieron revertirlos. La pieza más famosa de este período es un poema de un autor anónimo. Captura mejor lo que sucedió en ese momento. ¡Escucha cuidadosamente! La maestra comenzó el poema. Una docena de pequeños drones se reposicionaron frente a los niños y comenzaron a proyectar el video directamente en sus ojos. Mostraba a una persona en un traje de pie en un bosque con solo tocones de árboles. La persona comenzó a hablar: Las máquinas computan; las máquinas predicen. Somos parte de esto. Perseguimos un óptimo entrenado. Lo óptimo es unidimensional, local y sin restricciones. Silicio y carne, persiguiendo la exponencialidad. El crecimiento es nuestra mentalidad. Cuando todas las recompensas sean recogidas, y sus efectos secundarios descuidados; Cuando se extraigan todas las monedas, y la naturaleza haya quedado atrás; Estaremos en problemas, Después de todo, el crecimiento exponencial es una burbuja. La tragedia del desarrollo de los comunes, Explotando, Ante nuestros ojos. Cálculos fríos y avaricia helada, Llena la tierra de calor. Todo se está muriendo, Y estamos cumpliendo. Al igual que los caballos con anteojeras, corremos la carrera de nuestra propia creación, Hacia el gran filtro de la civilización. Y así marchamos sin descanso. Como somos parte de la máquina. Abrazando la entropía. Un recuerdo oscuro, dijo la maestra para romper el silencio en la sala. Se cargará en su biblioteca. Tu tarea es memorizarla hasta la próxima semana. Xola suspiró. Ella logró atrapar uno de los pequeños drones. El dron estaba caliente por la CPU y los motores. A Xola le gustó cómo le calentó las manos. {r, message = FALSE, warning = FALSE, echo = FALSE} devtools::load_all() "],["qué-es-el-aprendizaje-automático.html", "1.2 ¿Qué es el aprendizaje automático?", " 1.2 ¿Qué es el aprendizaje automático? El aprendizaje automático es un conjunto de métodos que usan las computadoras para hacer y mejorar predicciones o comportamientos basados en datos. Por ejemplo, para predecir el valor de una casa, la computadora puede aprender patrones de ventas pasadas de casas. El libro se centra en el aprendizaje automático supervisado, que cubre todos los problemas de predicción en los que tenemos un conjunto de datos para el que ya conocemos el resultado de interés (por ejemplo, precios anteriores de la vivienda) y queremos predecir el resultado de los nuevos datos. Se excluyen del aprendizaje supervisado, por ejemplo, las tareas de agrupación (aprendizaje no supervisado) donde no tenemos un resultado específico de interés, pero queremos encontrar grupos de observaciones. También se excluyen cosas como el aprendizaje por refuerzo, donde un agente aprende a optimizar cierta recompensa actuando en un entorno (por ejemplo, una computadora que juega Tetris). El objetivo del aprendizaje supervisado es aprender un modelo predictivo que relacione características de los datos (por ejemplo: tamaño de la casa, ubicación, tipo de piso, ) con una salida (por ejemplo: el precio de la casa). Si el resultado es categórico, el objetivo se llama clasificación, y si es numérico, se llama regresión. El algoritmo de aprendizaje automático aprende un modelo mediante la estimación de parámetros (como pesos) o estructuras de aprendizaje (como árboles). El algoritmo se guía por una función de puntuación o pérdida que se minimiza. En el ejemplo del valor de la vivienda, la máquina minimiza la diferencia entre el precio estimado de la vivienda y el precio previsto. Un modelo de aprendizaje automático totalmente entrenado se puede utilizar para hacer predicciones para nuevas instancias. Estimación de precios de la vivienda, recomendaciones de productos, detección de letreros, predicción de incumplimiento crediticio y detección de fraude: Todos estos ejemplos tienen en común que pueden resolverse mediante el aprendizaje automático. Las tareas son diferentes, pero el enfoque es el mismo: Paso 1: recopilación de datos. Mientras más, mejor. Los datos deben contener el resultado que desea predecir e información adicional a partir de la cual realizar la predicción. Para un detector de letrero de calle (¿Hay un letrero de calle en la imagen?), debes recopilar imágenes de la calle y etiquetar si un letrero de calle es visible o no. Para un predictor de incumplimiento de crédito, necesitas datos pasados sobre préstamos reales, información sobre si los clientes estaban en incumplimiento con sus préstamos y datos que lo ayudarán a hacer predicciones, como ingresos, incumplimientos de créditos pasados, etc. Para un programa de estimación automática del valor de la vivienda, podés recopilar datos de ventas de viviendas anteriores e información sobre los bienes inmuebles, como el tamaño, la ubicación, etc. Paso 2: ingreso de esta información en un algoritmo de aprendizaje automático que genera un modelo de detector de signos, un modelo de calificación crediticia o un estimador del valor de la vivienda. Paso 3: uso del modelo con nuevos datos. Integrar el modelo en un producto o proceso, como un automóvil sin conductor, un proceso de solicitud de crédito o un sitio web del mercado inmobiliario. Las máquinas superan a los humanos en muchas tareas, como jugar al ajedrez (o más recientemente Go) o predecir el clima. Incluso si la máquina es tan buena como un ser humano o un poco peor en una tarea, sigue habiendo grandes ventajas en términos de velocidad, reproducibilidad y escala. Una vez implementado, un modelo de aprendizaje automático puede completar una tarea mucho más rápido que los humanos, ofrece resultados consistentes y se puede copiar infinitamente. La replicación de un modelo de aprendizaje automático en otra máquina es rápida y barata. El entrenamiento de un humano para una tarea puede llevar décadas (especialmente cuando son jóvenes) y es muy costoso. Una desventaja importante del uso del aprendizaje automático es que los conocimientos sobre los datos y la tarea que resuelve la máquina están ocultos en modelos cada vez más complejos. Necesita millones de números para describir una red neuronal profunda, y no hay forma de entender el modelo en su totalidad. Otros modelos, como el bosque aleatorio -random forest-, consisten en cientos de árboles de decisión que votan por predicciones. Para comprender cómo se tomó la decisión, deberías examinar los votos y las estructuras de cada uno de los cientos de árboles. Eso simplemente no funciona, no importa cuán inteligente seas o cuán buena sea tu memoria de trabajo. Los modelos con mejor rendimiento son a menudo mezclas de varios modelos (también llamados conjuntos) imposibles de interpretar, aún bajo la posibilidad de que cada modelo se pudiera interpretar. Si te enfocas solo en el rendimiento, obtendrás automáticamente modelos cada vez más opacos. Solo echa un vistazo a entrevistas con ganadores en la plataforma de competencia de aprendizaje automático kaggle.com: Los modelos ganadores eran en su mayoría conjuntos de modelos o modelos muy complejos, como árboles potenciados o redes neuronales profundas. "],["terminología.html", "1.3 Terminología", " 1.3 Terminología Para evitar confusiones debido a la ambigüedad, aquí hay algunas definiciones de los términos utilizados en este libro: Un Algoritmo es un conjunto de reglas que una máquina sigue para lograr un objetivo particular2. Un algoritmo puede considerarse como una receta que define las entradas, la salida y todos los pasos necesarios para pasar de las entradas a la salida. Las recetas de cocción son algoritmos en los que los ingredientes son las entradas, la comida cocida es la salida y los pasos de preparación y cocción son las instrucciones del algoritmo. Aprendizaje automático es un conjunto de métodos que permiten a las computadoras aprender de los datos para hacer y mejorar predicciones (por ejemplo, cáncer, ventas semanales, incumplimiento de crédito). El aprendizaje automático es un cambio de paradigma de la programación normal, donde todas las instrucciones se deben dar explícitamente a la computadora a la programación indirecta que se realiza mediante el suministro de datos. Un Aprendiz o Algoritmo de aprendizaje automático es el programa utilizado para aprender un modelo de aprendizaje automático a partir de datos. Otro nombre es inductor (por ejemplo, inductor de árbol). Un Modelo de aprendizaje automático es el programa aprendido que asigna entradas a predicciones. Esto puede ser un conjunto de pesos para un modelo lineal o para una red neuronal. Otros nombres para la palabra bastante inespecífica modelo son predictor o, según la tarea, clasificador o modelo de regresión. En las fórmulas, el modelo de aprendizaje automático entrenado se llama \\(\\hat{f}\\) o \\(\\hat{f}(x)\\). FIGURA 1.1: Un algoritmo de aprendizaje automático aprende de datos supervisados de entrenamiento. El modelo se usa para hacer predicciones. Un Modelo de caja negra es un sistema que no revela sus mecanismos internos. En el aprendizaje automático, la caja negra describe modelos que no se pueden entender al observar sus parámetros (por ejemplo, una red neuronal). El opuesto de una caja negra a veces se denomina caja blanca, y es llamada en este libro como modelo interpretable. Los métodos modelo-agnósticos para la interpretabilidad tratan los modelos de aprendizaje automático como cajas negras, incluso si no lo son. Aprendizaje automático interpretable se refiere a métodos y modelos que hacen que el comportamiento y las predicciones de los sistemas de aprendizaje automático sean comprensibles para los humanos. Un conjunto de datos es una tabla con los datos de los cuales la máquina aprende. El conjunto de datos contiene las características y el objetivo a predecir. Cuando se usa para el aprendizaje de un modelo, el conjunto de datos se denomina datos de entrenamiento. Una instancia es una fila en el conjunto de datos. Otros nombres para instancia son: punto de datos, ejemplo, observación. Una instancia consta de los valores de característica \\(x^{(i)}\\) y, si se conoce, el resultado objetivo \\(y_i\\). Las características son las entradas utilizadas para la predicción o clasificación. Una característica es una columna en el conjunto de datos. A lo largo del libro, se supone que las características son interpretables, lo que significa que es fácil entender lo que significan, como la temperatura en un día determinado o la altura de una persona. La interpretabilidad de las características es una gran suposición, pero si es difícil entender las características de entrada, es aún más difícil entender lo que hace el modelo. La matriz con todas las características se llama X y la notación \\(x^{(i)}\\) se usa para cada instancia en particular). El vector de una sola característica para todas las instancias es \\(x_j\\) y el valor para la característica j y la instancia i es \\(x^{(i)}_j\\). El Objetivo (o target) es la columna que la máquina aprende a predecir. En las fórmulas matemáticas, el objetivo generalmente se llama \\(y\\) o \\(y_i\\) para una sola instancia. Una Tarea de aprendizaje automático es la combinación de un conjunto de datos con características y un objetivo. Dependiendo del tipo de objetivo, la tarea puede ser, por ejemplo, clasificación, regresión, análisis de supervivencia, agrupamiento o detección de valores atípicos. La Predicción es el valor que el modelo de aprendizaje automático pronostica, en función de las características dadas. En este libro, la predicción del modelo se denota por \\(\\hat{f}(x^{(i)})\\) o \\(\\hat{y}\\).  Definición de algoritmo. https://www.merriam-webster.com/dictionary/algorithm. (2017) "],["interpretabilidad.html", "Capítulo 2 Interpretabilidad", " Capítulo 2 Interpretabilidad No existe una definición matemática de interpretabilidad. Una definición (no matemática) que me gusta de Miller (2017)3 es: Interpretabilidad es el grado en que un humano puede comprender la causa de una decisión. Otra es: Interpretabilidad es el grado a lo que un humano puede predecir constantemente el resultado del modelo4. Cuanto mayor sea la interpretabilidad de un modelo de aprendizaje automático, más fácil será para alguien comprender por qué se han tomado ciertas decisiones o predicciones. Un modelo es más interpretable que otro si, comparativamente, sus decisiones son más fáciles de comprender para un humano. Usaré los términos interpretable y explicable en forma indistinta. Al igual que Miller (2017), creo que tiene sentido distinguir entre los términos interpretabilidad / explicabilidad y explicación. Usaré explicación para explicaciones de predicciones individuales. Lee la sección sobre explicaciones para aprender lo que los humanos vemos como una buena explicación. Miller, Tim. Explanation in artificial intelligence: Insights from the social sciences. arXiv Preprint arXiv:1706.07269. (2017). Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. Examples are not enough, learn to criticize! Criticism for interpretability. Advances in Neural Information Processing Systems (2016). "],["interpretabilidad-importancia.html", "2.1 Importancia de la interpretabilidad", " 2.1 Importancia de la interpretabilidad Si un modelo de aprendizaje automático funciona bien, ¿por qué no confiamos en el modelo e ignoramos por qué tomó una determinada decisión? El problema es que una sola métrica, como la precisión de la clasificación, es una descripción incompleta de la mayoría de las tareas del mundo real. (Doshi-Velez y Kim 20175) Profundicemos en las razones por las que la interpretabilidad es tan importante. Cuando se trata de modelado predictivo, debes hacer una compensación: ¿solo deseas saber qué se predice? Por ejemplo, la probabilidad de que un cliente abandone un servicio o qué tan efectivo será un medicamento para un paciente. ¿O quieres saber por qué se hizo la predicción, posiblemente pagando la interpretabilidad con una caída en el rendimiento predictivo? En algunos casos, no te importa por qué se tomó una decisión, es suficiente saber que el rendimiento predictivo en un conjunto de datos de prueba fue bueno. Pero en otros casos, conocer el por qué puede ayudarlo a aprender más sobre el problema, los datos y la razón por la cual un modelo puede fallar. Es posible que algunos modelos no requieran explicaciones porque se usan en un entorno de bajo riesgo, lo que significa que un error no tendrá consecuencias graves (por ejemplo, un sistema de recomendación de películas) o que el método ya ha sido ampliamente estudiado y evaluado (por ejemplo, reconocimiento óptico de caracteres). La necesidad de interpretabilidad surge de una incompletitud en la formalización del problema (Doshi-Velez y Kim 2017), lo que significa que para ciertos problemas o tareas no es suficiente obtener la predicción (el qué). El modelo también debe explicar cómo llegó a la predicción (el por qué), porque una predicción correcta solo resuelve parcialmente su problema original. Las siguientes razones impulsan la demanda de interpretabilidad y explicaciones (Doshi-Velez y Kim 2017 y Miller 2017). Curiosidad y aprendizaje humanos: los humanos tenemos un modelo mental de nuestro entorno que se actualiza cuando ocurre algo inesperado. Esta actualización se realiza buscando una explicación para el evento inesperado. Por ejemplo, un humano se siente enfermo y pregunta: ¿Por qué me siento tan enfermo?. Se entera de que se enferma cada vez que come frutos rojos. Actualiza su modelo mental y decide que esos frutos causaron la enfermedad y, por lo tanto, deben evitarse. Cuando se usan modelos opacos de aprendizaje automático en la investigación, los hallazgos científicos permanecen completamente ocultos si el modelo solo da predicciones sin explicaciones. Para facilitar el aprendizaje y satisfacer la curiosidad de por qué ciertas predicciones o comportamientos son creados por máquinas, la interpretación y las explicaciones son cruciales. Por supuesto, los humanos no necesitamos explicaciones para todo lo que sucede. Para la mayoría de las personas, no hay problemas en no entender cómo funciona una computadora. Eventos inesperados nos hacen curiosos. Por ejemplo: ¿Por qué mi computadora se apaga inesperadamente? Estrechamente relacionado con el aprendizaje está el deseo humano de encontrar significado en el mundo. Queremos armonizar las contradicciones o inconsistencias entre los elementos de nuestras estructuras de conocimiento. ¿Por qué mi perro me mordió a pesar de que nunca antes lo había hecho? Un humano podría preguntar. Existe una contradicción entre el conocimiento del comportamiento pasado del perro y la experiencia desagradable recién hecha de la mordedura. La explicación del veterinario concilia la contradicción del dueño del perro: El perro estaba estresado y mordió. Cuanto más la decisión de una máquina afecta la vida de una persona, más importante es que la máquina explique su comportamiento. Si un modelo de aprendizaje automático rechaza una solicitud de préstamo, esto puede ser completamente inesperado para los solicitantes. Solo pueden conciliar esta inconsistencia entre la expectativa y la realidad con algún tipo de explicación. En realidad, las explicaciones no tienen que explicar completamente la situación, sino que deben abordar una causa principal. Otro ejemplo es la recomendación algorítmica de productos. Personalmente, siempre pienso en por qué ciertos productos o películas me han sido recomendados algorítmicamente. A menudo es bastante claro: la publicidad me sigue en Internet porque recientemente compré una lavadora, y sé que en los próximos días me seguirán anuncios de lavadoras. Sí, tiene sentido sugerir guantes si ya tengo un gorro de nieve en mi carrito de compras. El algoritmo recomienda esta película, porque los usuarios a quienes les gustaron otras películas que me gustaron también disfrutaron la película recomendada. Cada vez más, las compañías de Internet están agregando explicaciones a sus recomendaciones. Un buen ejemplo es la recomendación de productos de Amazon, que se basa en combinaciones de productos que se compran con frecuencia: FIGURA 2.1: Productos recomendados cuando se compra pintura en Amazon. En muchas disciplinas científicas hay un cambio de métodos cualitativos a cuantitativos (por ejemplo, sociología, psicología), y también hacia el aprendizaje automático (biología, genómica). El objetivo de la ciencia es obtener conocimiento, pero muchos problemas se resuelven con grandes conjuntos de datos y modelos de aprendizaje automático de caja negra. El modelo en sí se convierte en la fuente de conocimiento en lugar de los datos. La interpretabilidad hace posible extraer este conocimiento adicional capturado por el modelo. Los modelos de aprendizaje automático asumen tareas del mundo real que requieren medidas de seguridad y pruebas. Imagina que un automóvil autónomo detecta automáticamente a los ciclistas en función de un sistema de aprendizaje profundo. Deseas estar 100% seguro de que la abstracción que ha aprendido el sistema está libre de errores, porque no podrías tolerar ni un atropello. Una explicación podría revelar que la característica aprendida más importante es reconocer las dos ruedas de una bicicleta, pero existen casos de borde, como bicicletas con bolsas laterales que cubren parcialmente las ruedas. Por defecto, los modelos de aprendizaje automático recogen sesgos de los datos de entrenamiento. Esto puede convertir sus modelos de aprendizaje automático en racistas que discriminan determinados grupos. La interpretabilidad es una herramienta de depuración útil para detectar sesgos en modelos de aprendizaje automático. Puede suceder que el modelo de aprendizaje automático que hayas entrenado para la aprobación automática o el rechazo de las solicitudes de crédito discrimine a una minoría. Su objetivo principal es otorgar préstamos solo a personas que eventualmente los pagarán. Lo incompleto de la formulación del problema en este caso radica en el hecho de que no solo desea minimizar los impagos de préstamos, sino que también está obligado a no discriminar sobre la base de ciertos datos demográficos. Esta es una restricción adicional que forma parte de la formulación de su problema (otorgar préstamos de manera riesgosa y conforme) que no está cubierta por la función de pérdida para la que se optimizó el modelo de aprendizaje automático. El proceso de integración de máquinas y algoritmos en nuestra vida diaria requiere interpretabilidad para aumentar la aceptación social. Las personas atribuyen creencias, deseos, intenciones, etc. a los objetos. En un famoso experimento, Heider y Simmel (1944)6 mostraron a los participantes videos de formas en las que un círculo abría una puerta para ingresar a una habitación (que era simplemente un rectángulo). Los participantes describieron las acciones de las formas como describirían las acciones de un agente humano, asignando intenciones e incluso emociones y rasgos de personalidad a las formas. Los robots son un buen ejemplo, como mi aspiradora, a la que llamé Doge. Si Doge se atasca, pienso: Doge quiere seguir limpiando, pero me pide ayuda porque se atascó. Más tarde, cuando Doge termina de limpiar y busca en la base de operaciones para recargar, pienso: Doge desea recargar y tiene la intención de encontrar la base de operaciones. También atribuyo rasgos de personalidad: Doge es un poco tonto, pero de una manera linda. Estos son mis pensamientos, especialmente cuando descubro que Doge ha derribado una planta mientras aspiraba la casa. Una máquina o algoritmo que explica sus predicciones encontrará más aceptación. Véase también el capítulo sobre explicaciones, que argumenta que las explicaciones son un proceso social. Las explicaciones se utilizan para gestionar las interacciones sociales. Al crear un significado compartido de algo, el explicador influye en las acciones, emociones y creencias del receptor de la explicación. Para que una máquina interactúe con nosotros, puede que tenga que moldear nuestras emociones y creencias. Las máquinas tienen que persuadirnos para que puedan lograr su objetivo. No aceptaría completamente mi robot aspirador si no explicara su comportamiento, al menos hasta cierto punto. La aspiradora crea un significado compartido de, por ejemplo, un accidente (como quedarse atascado en la alfombra del baño  otra vez) al explicar que se atascó en lugar de simplemente detenerse a trabajar sin comentarios. Curiosamente, puede haber un desalineamiento entre el objetivo de la máquina explicadora (crear confianza) y el objetivo del destinatario (comprender la predicción o el comportamiento). Quizás la explicación completa de por qué Doge se atascó podría ser que la batería estaba muy baja, que una de las ruedas no funciona correctamente y que hay un error que hace que el robot vaya al mismo lugar una y otra vez a pesar de que había un obstaculo. Estas razones (y algunas más) hicieron que el robot se atascara, aunque algo estaba en el camino, y eso fue suficiente para que confiara en su comportamiento y se produjera el accidente. Por cierto, Doge se quedó atascado en el baño nuevamente. Tenemos que quitar las alfombras cada vez antes de dejar que Doge aspire. FIGURA 2.2: Doge, nuestra aspiradora, se atascó. Como explicación del accidente, Doge nos dijo que debe estar en una superficie plana. Los modelos de aprendizaje automático solo se pueden depurar y auditar cuando se pueden interpretar. Incluso en entornos de bajo riesgo, como las recomendaciones de películas, la capacidad de interpretación es valiosa en la fase de investigación y desarrollo, así como después de la implementación. Más tarde, cuando se usa un modelo en un producto, las cosas pueden salir mal. Una interpretación para una predicción errónea ayuda a comprender la causa del error. Ofrece una dirección sobre cómo arreglar el sistema. Considera un ejemplo de un clasificador entre perros siberianos y lobos, que clasifica erróneamente a algunos siberianos como lobos. Al utilizar métodos de aprendizaje automático interpretables, descubrirás que la clasificación errónea se debió a la nieve en la imagen. El clasificador aprendió a usar la nieve como una característica para clasificar las imágenes como lobo, lo que podría tener sentido en términos de separar a los lobos de los siberianos en el conjunto de datos de entrenamiento, pero no en el uso en el mundo real. Si puedes asegurarte de que el modelo de aprendizaje automático pueda explicar las decisiones, también puedes verificar los siguientes rasgos con mayor facilidad (Doshi-Velez y Kim 2017): Equidad: garantiza que las predicciones sean imparciales y no discriminen implícita o explícitamente a ciertos grupos. Un modelo interpretable puede decirte por qué ha decidido que cierta persona no debería obtener un préstamo, y es más fácil para un humano juzgar si la decisión se basa en un sesgo demográfico aprendido (por ejemplo, racial). Privacidad: garantiza que la información confidencial de los datos esté protegida. Fiabilidad o robustez: garantiza que pequeños cambios en la entrada no conduzcan a grandes cambios en la predicción. Causalidad: comprueba que solo se recogen las relaciones causales. Confianza: es más fácil para los humanos confiar en un sistema que explica sus decisiones en comparación con una caja negra. Cuando no necesitamos interpretabilidad. Los siguientes escenarios ilustran los casos en que no necesitamos o incluso no queremos la interpretabilidad de los modelos de aprendizaje automático. La interpretabilidad no es necesaria si el modelo no tiene un impacto significativo. Imagina a alguien llamado Mike trabajando en un proyecto paralelo de aprendizaje automático para predecir a dónde irán sus amigos para sus próximas vacaciones en base a datos de Facebook. A Mike le gusta sorprender a sus amigos con suposiciones educadas sobre dónde irán de vacaciones. No hay ningún problema real si el modelo está equivocado (en el peor de los casos, solo un poco de vergüenza para Mike), ni hay un problema si Mike no puede explicar el resultado de su modelo. Está perfectamente bien no tener interpretabilidad en este caso. La situación cambiaría si Mike comenzara a construir un negocio en torno a estas predicciones de destinos de vacaciones. Si el modelo está equivocado, el negocio podría perder dinero, o el modelo podría funcionar peor para algunas personas debido al prejuicio racial aprendido. Tan pronto como el modelo tenga un impacto significativo, ya sea financiero o social, la interpretabilidad se vuelve relevante. La interpretabilidad no es necesaria cuando el problema está bien estudiado. Algunas aplicaciones se han estudiado lo suficientemente bien como para que haya suficiente experiencia práctica con el modelo y los problemas con el modelo se hayan resuelto con el tiempo. Un buen ejemplo es un modelo de aprendizaje automático para el reconocimiento óptico de caracteres que procesa imágenes de sobres y extrae direcciones. Hay años de experiencia con estos sistemas y está claro que funcionan. Además, no estamos realmente interesados en obtener información adicional sobre la tarea en cuestión. La interpretabilidad podría permitir a las personas o programas manipular el sistema. Los problemas con los usuarios que engañan a un sistema son el resultado de una falta de coincidencia entre los objetivos del creador y el usuario de un modelo. La calificación crediticia es un sistema de este tipo porque los bancos quieren asegurarse de que los préstamos solo se otorguen a los solicitantes que puedan devolverlos, y los solicitantes aspiran a obtener el préstamo incluso si el banco no quiere darles uno. Este desajuste entre los objetivos introduce incentivos para que los solicitantes jueguen con el sistema para aumentar sus posibilidades de obtener un préstamo. Si un solicitante sabe que tener más de dos tarjetas de crédito afecta negativamente su puntaje, simplemente devuelve su tercera tarjeta de crédito para mejorar su puntaje y solicita una nueva tarjeta después de que el préstamo haya sido aprobado. Si bien su puntaje mejoró, la probabilidad real de pagar el préstamo se mantuvo sin cambios. El sistema solo se puede manipular si las entradas son representantes de una característica causal, pero en realidad no causan el resultado. Siempre que sea posible, se deben evitar las funciones de proxy ya que hacen que los modelos sean manipulables. Por ejemplo, Google desarrolló un sistema llamado Google Flu Trends para predecir los brotes de gripe. El sistema correlacionó las búsquedas de Google con los brotes de gripe, y tuvo un mal desempeño. La distribución de las consultas de búsqueda cambió y Google Flu Trends se perdió muchos brotes de gripe. Las búsquedas en Google no causan la gripe. Cuando las personas buscan síntomas como fiebre, se trata simplemente de una correlación con los brotes de gripe reales. Idealmente, los modelos solo usarían características causales porque no serían manipulables. Doshi-Velez, Finale, and Been Kim. Towards a rigorous science of interpretable machine learning, no. Ml: 113. http://arxiv.org/abs/1702.08608 ( 2017). Heider, Fritz, and Marianne Simmel. An experimental study of apparent behavior. The American Journal of Psychology 57 (2). JSTOR: 24359. (1944). "],["taxonomía-de-los-métodos-de-interpretación.html", "2.2 Taxonomía de los métodos de interpretación", " 2.2 Taxonomía de los métodos de interpretación Los métodos para la interpretación de aprendizaje automático se pueden clasificar de acuerdo con varios criterios. ¿Intrínseco o post hoc? Este criterio distingue si la interpretabilidad se logra restringiendo la complejidad del modelo de aprendizaje automático (intrínseco) o aplicando métodos que analizan el modelo después del entrenamiento (post hoc). La interpretabilidad intrínseca se refiere a modelos de aprendizaje automático que se consideran interpretables debido a su estructura simple, como árboles de decisión cortos o modelos lineales dispersos. La interpretabilidad post hoc se refiere a la aplicación de métodos de interpretación después del entrenamiento modelo. La importancia de la característica de permutación es, por ejemplo, un método de interpretación post hoc. Los métodos post hoc también se pueden aplicar a modelos intrínsecamente interpretables. Por ejemplo, la importancia de la característica de permutación se puede calcular para los árboles de decisión. La organización de los capítulos de este libro está determinada por la distinción entre modelos intrínsecamente interpretables y métodos de interpretación post hoc (y modelo-agnósticos). Resultado del método de interpretación Los diversos métodos de interpretación pueden diferenciarse aproximadamente de acuerdo con sus resultados. Estadísticas de resumen de características: Muchos métodos de interpretación proporcionan estadísticas de resumen para cada covariable. Algunos métodos devuelven un solo número por característica, como la importancia de la característica, o un resultado más complejo, como las fortalezas de interacción de características por pares. Visualización de resumen de características: La mayoría de las estadísticas de resumen de características también se pueden visualizar. Algunos resúmenes de características en realidad solo tienen sentido si se visualizan y una tabla sería una elección incorrecta. La dependencia parcial de una característica es tal caso. Las gráficas de dependencia parcial son curvas que muestran una característica y el resultado promedio pronosticado. La mejor manera de presentar dependencias parciales es dibujar la curva en lugar de imprimir las coordenadas. Elementos internos del modelo (p. Ej., Pesos aprendidos): La interpretación de modelos intrínsecamente interpretables entra en esta categoría. Algunos ejemplos son los pesos en modelos lineales o la estructura de árbol aprendida (las características y los umbrales utilizados para las divisiones) de los árboles de decisión. Las líneas se desdibujan entre las partes internas del modelo y la estadística de resumen de características en, por ejemplo, modelos lineales, porque los pesos son tanto las partes internas del modelo como las estadísticas de resumen de las características al mismo tiempo. Otro método que genera modelos internos es la visualización de detectores de características aprendidos en redes neuronales convolucionales. Los métodos de interpretación que generan elementos internos del modelo son, por definición, específicos del modelo (consulte el siguiente criterio). Punto de datos: Esta categoría incluye todos los métodos que devuelven observaciones (ya existentes o recién creados) para hacer que un modelo sea interpretable. Un método se llama explicaciones contrafácticas. Para explicar la predicción de una instancia de datos, el método encuentra una observación similar al cambiar algunas de las características para las cuales el resultado predicho cambia de manera relevante. Otro ejemplo es la identificación de prototipos de clases predichas. Para ser útiles, los métodos de interpretación que generan nuevos puntos de datos requieren que los propios puntos de datos puedan ser interpretados. Esto funciona bien para imágenes y textos, pero es menos útil para datos tabulares con cientos de características. Modelo intrínsecamente interpretable: Una solución para interpretar modelos de caja negra es aproximarlos (global o localmente) con un modelo interpretable. El modelo interpretable en sí mismo se interpreta mirando los parámetros internos del modelo o las estadísticas de resumen de sus características. ¿Modelo específico o modelo agnóstico? Las herramientas de interpretación modelo-específicas están limitadas a ciertos modelos. La interpretación de los pesos de regresión en un modelo lineal es una interpretación de este tipo ya que, por definición, siempre es específica del modelo. Herramientas que solo funcionan para la interpretación de, por ejemplo, las redes neuronales son específicas del modelo. Las herramientas independientes del modelo se pueden usar en cualquier modelo de aprendizaje automático y se aplican después de que el modelo haya sido entrenado (post hoc). Estos métodos generalmente funcionan mediante el análisis de pares de entrada y salida de características. Por definición, estos métodos no pueden tener acceso a los modelos internos, como los pesos o la información estructural. ¿Local o global? ¿El método de interpretación explica una predicción individual o el comportamiento completo del modelo? ¿O algún punto intermedio? Lee más sobre el criterio de alcance en la siguiente sección. "],["alcance-de-la-interpretabilidad.html", "2.3 Alcance de la interpretabilidad", " 2.3 Alcance de la interpretabilidad Un algoritmo entrena un modelo que produce las predicciones. Cada paso puede evaluarse en términos de transparencia o interpretabilidad. 2.3.1 Transparencia del algoritmo ¿Cómo crea el algoritmo el modelo? La transparencia del algoritmo se trata de cómo el algoritmo aprende un modelo desde los datos, y de qué tipo de relaciones puede incorporar. Si utilizas redes neuronales convolucionales para clasificar imágenes, puedes explicar que el algoritmo aprende detectores de borde y filtros en las capas más bajas. Esto es una comprensión de cómo funciona el algoritmo, pero no para el modelo específico que se aprende al final, y tampoco para la forma en la que se hacen las predicciones individuales. La transparencia del algoritmo solo requiere el conocimiento del algoritmo y no de los datos o el modelo aprendido. Este libro se centra en la interpretabilidad del modelo y no en la transparencia del algoritmo. Algoritmos como el método de mínimos cuadrados para modelos lineales están bien estudiados y entendidos. Se caracterizan por una alta transparencia. Los enfoques de aprendizaje profundo (empujar un gradiente a través de redes con millones de pesos) se entienden menos y el funcionamiento interno es el foco de la investigación en curso. Se consideran menos transparentes. 2.3.2 Interpretabilidad global y holística del modelo ¿Cómo hace predicciones el modelo entrenado? Puedes describir un modelo como interpretable si puedes comprender todo el modelo de una vez (Lipton 20167). Para explicar el resultado del modelo global necesitas el modelo entrenado, el conocimiento del algoritmo y los datos. Este nivel de interpretabilidad se trata de comprender cómo toma decisiones el modelo, en función de una visión holística de sus características y de cada uno de los componentes aprendidos, como los pesos, parámetros y estructuras. ¿Qué características son importantes y qué tipo de interacciones entre ellas tienen lugar? La interpretación global del modelo ayuda a comprender la distribución de su resultado objetivo en función de las características. La interpretabilidad del modelo global es muy difícil de lograr en la práctica. Es improbable que cualquier modelo que exceda un puñado de parámetros o pesos se ajuste a la memoria a corto plazo del ser humano promedio. Sostengo que realmente no puedes imaginar un modelo lineal con 5 características, porque significaría dibujar mentalmente el hiperplano estimado en un espacio de 5 dimensiones. Cualquier espacio de características con más de 3 dimensiones es simplemente inconcebible para los humanos. Por lo general, cuando las personas intentan comprender un modelo, solo consideran partes de él, como los pesos en los modelos lineales. 2.3.3 Interpretabilidad del modelo global en un nivel modular ¿Cómo afectan las predicciones las partes del modelo? Un modelo de Naive Bayes -clasificador bayesiano ingenuo- con cientos de características sería demasiado grande para mantenerlo en nuestra memoria de trabajo. E incluso si logramos memorizar todos los pesos, no podríamos hacer predicciones rápidamente para nuevas observaciones. Además, debes tener la distribución conjunta de todas las características en su cabeza para estimar la importancia de cada característica y cómo las características afectan las predicciones en promedio. Una tarea imposible. Pero puedes entender fácilmente un solo peso. Si bien la interpretación global del modelo generalmente está fuera del alcance, existe una buena posibilidad de comprender al menos algunos modelos a nivel modular. No todos los modelos son interpretables a nivel de parámetro. Para los modelos lineales, las partes interpretables son los pesos, para los árboles serían las divisiones (características seleccionadas más puntos de corte) y las predicciones de los nodos foliares. Los modelos lineales, por ejemplo, se ven como si pudieran interpretarse perfectamente en un nivel modular, pero la interpretación de un solo peso está entrelazada con todos los demás pesos. La interpretación de un solo peso siempre viene con la nota al pie de página de que las otras características de entrada permanecen en el mismo valor, que no es el caso en muchas aplicaciones reales. Un modelo lineal que predice el valor de una casa, que tiene en cuenta tanto el tamaño de la casa como el número de habitaciones, puede tener un peso negativo para la característica de la cantidad de habitaciones. Puede suceder porque ya existe la característica de tamaño de la casa altamente correlacionada. En un mercado donde la gente prefiere habitaciones más grandes, una casa con menos habitaciones podría valer más que una casa con más habitaciones si ambas tienen el mismo tamaño. Los pesos solo tienen sentido en el contexto de las otras características del modelo. Pero los pesos en un modelo lineal aún se pueden interpretar mejor que los pesos de una red neuronal profunda. 2.3.4 Interpretabilidad local para una única predicción ¿Por qué el modelo hizo una cierta predicción para una instancia? Puedes ampliar una sola instancia y examinar lo que el modelo predice para esta entrada, y explicar por qué. Si observas una predicción individual, el comportamiento del modelo complejo podría comportarse de manera más agradable. Localmente, la predicción podría depender solo linealmente o monotónicamente de algunas características, en lugar de tener una dependencia compleja de ellas. Por ejemplo, el valor de una casa puede depender no linealmente de su tamaño. Pero si solo estás mirando una casa particular de 100 metros cuadrados, existe la posibilidad de que para ese subconjunto de datos, la predicción del modelo dependa linealmente del tamaño. Puedes descubrir esto simulando cómo cambia el precio previsto cuando aumenta o disminuye el tamaño en 10 metros cuadrados. Por lo tanto, las explicaciones locales pueden ser más precisas que las explicaciones globales. Este libro presenta métodos que pueden hacer que las predicciones individuales sean más interpretables en la sección sobre métodos modelo-agnósticos. 2.3.5 Interpretabilidad local para un grupo de predicciones ¿Por qué el modelo hizo predicciones específicas para un grupo de instancias? Las predicciones del modelo para varias observaciones pueden explicarse con métodos de interpretación de modelos globales (a nivel modular) o con explicaciones particulares por observación. Los métodos globales se pueden aplicar tomando un grupo de observaciones, tratándolo como si fuera el conjunto de datos completo y utilizando los métodos globales en este subconjunto. Los métodos de explicación individuales se pueden utilizar en cada instancia y luego enumerar o agregar para todo el grupo. Lipton, Zachary C. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, (2016). "],["evaluación-de-la-interpretabilidad.html", "2.4 Evaluación de la interpretabilidad", " 2.4 Evaluación de la interpretabilidad No existe un consenso real sobre qué es la interpretabilidad en el aprendizaje automático. Tampoco está claro cómo medirla. Pero hay una investigación inicial sobre esto y un intento de formular algunos enfoques para la evaluación, como se describe en la siguiente sección. Doshi-Velez y Kim (2017) proponen tres niveles principales para la evaluación de la interpretabilidad: Evaluación del nivel de aplicación (tarea real): Pon la explicación en el producto y haz que el usuario final lo pruebe. Imagina un software de detección de fracturas con un componente de aprendizaje automático que localiza y marca fracturas en rayos X. A nivel de aplicación, los radiólogos probarían el software de detección de fracturas directamente para evaluar el modelo. Esto requiere una buena configuración experimental y una comprensión de cómo evaluar la calidad. Una buena base para esto es siempre qué tan bueno sería un humano para explicar la misma decisión. Evaluación a nivel humano (tarea simple) Es una evaluación de nivel de aplicación simplificada. La diferencia es que estos experimentos no se llevan a cabo con expertos en el dominio, sino con personas laicas. Esto hace que los experimentos sean más baratos (especialmente si los expertos en el dominio son radiólogos) y es más fácil encontrar más evaluadores. Un ejemplo sería mostrarle a un usuario diferentes explicaciones y el usuario elegiría la mejor. La evaluación del nivel de función (tarea proxy) no requiere humanos. Esto funciona mejor cuando la clase de modelo utilizada ya ha sido evaluada por otra persona en una evaluación a nivel humano. Por ejemplo, podría saberse que los usuarios finales entienden los árboles de decisión. En este caso, un proxy para la calidad de la explicación puede ser la profundidad del árbol. Los árboles más cortos obtendrían una mejor puntuación de explicabilidad. Tendría sentido agregar la restricción de que el rendimiento predictivo del árbol siga siendo bueno y no disminuzca demasiado en comparación con un árbol más grande. El próximo capítulo se centra en la evaluación de explicaciones para predicciones individuales. ¿Cuáles son las propiedades relevantes de las explicaciones que consideraríamos para su evaluación? "],["properties.html", "2.5 Propiedades de las explicaciones", " 2.5 Propiedades de las explicaciones Queremos explicar las predicciones de un modelo de aprendizaje automático. Para lograr esto, confiamos en algún método de explicación, que es un algoritmo que genera explicaciones. Una explicación generalmente relaciona los valores de características de una instancia con la predicción de su modelo de una manera humanamente comprensible. Otros tipos de explicaciones consisten en un conjunto de instancias de datos (por ejemplo, en el caso del modelo vecino k-más cercano). Por ejemplo, podríamos predecir el riesgo de cáncer utilizando una SVM y explicar las predicciones utilizando el método sustituto local, que genera árboles de decisión como explicaciones. O podríamos usar un modelo de regresión lineal en lugar de una SVM. El modelo de regresión lineal ya está equipado con un método de explicación (interpretación de los pesos). Echamos un vistazo más de cerca a las propiedades de los métodos de explicación y explicaciones (Robnik-Sikonja y Bohanec, 20188). Estas propiedades se pueden usar para juzgar qué tan bueno es un método. No está claro para todas estas propiedades cómo medirlas correctamente, por lo que uno de los desafíos es formalizar cómo podrían calcularse. Propiedades de los métodos de explicación Poder expresivo es el lenguaje o estructura de las explicaciones que el método puede generar. Un método de explicación podría generar reglas SI-ENTONCES (IF-THEN), árboles de decisión, una suma ponderada, lenguaje natural u otra cosa. Translucidez describe cuánto se basa el método de explicación en analizar el modelo de aprendizaje automático, como sus parámetros. Por ejemplo, los métodos de explicación que se basan en modelos intrínsecamente interpretables como el modelo de regresión lineal (específico del modelo) son altamente translúcidos. Los métodos que solo se basan en manipular entradas y observar las predicciones tienen cero translucidez. Dependiendo del escenario, diferentes niveles de translucidez pueden ser deseables. La ventaja de la alta translucidez es que el método puede confiar en más información para generar explicaciones. La ventaja de la baja translucidez es que el método de explicación es más portátil. Portabilidad describe la gama de modelos de aprendizaje automático con los que se puede utilizar el método de explicación. Los métodos con baja translucidez tienen una mayor portabilidad porque tratan el modelo de aprendizaje automático como una caja negra. Los modelos sustitutos pueden ser el método de explicación con la mayor portabilidad. Métodos que solo funcionan, por ejemplo, para explicar las redes neuronales tienen baja portabilidad. Complejidad algorítmica describe la complejidad computacional del método que genera la explicación. Es importante tener en cuenta esta propiedad cuando el tiempo de cálculo es un cuello de botella en la generación de explicaciones. Propiedades de explicaciones individuales Precisión: ¿Qué tan bien una explicación predice datos nuevos? La alta precisión es especialmente importante si la explicación se usa para las predicciones, y no para el modelo en sí. La baja precisión puede estar bien si la precisión del modelo de aprendizaje automático también es baja, y si el objetivo es explicar lo que hace el modelo de caja negra. En este caso, solo la fidelidad es importante. Fidelidad: ¿Qué tan bien se aproxima la explicación a la predicción del modelo de caja negra? La alta fidelidad es una de las propiedades más importantes de una explicación, porque una explicación con baja fidelidad es inútil para explicar el modelo de aprendizaje automático. La precisión y la fidelidad están estrechamente relacionadas. Si el modelo de caja negra tiene una alta precisión y la explicación tiene una alta fidelidad, la explicación también tiene una alta precisión. Algunas explicaciones ofrecen solo fidelidad local, lo que significa que la explicación solo se aproxima bien a la predicción del modelo para un subconjunto de datos (por ejemplo, modelos sustitutos locales) o incluso solo para una observación individual (por ejemplo, Valores de Shapley). Consistencia: ¿Cuánto difiere una explicación entre los modelos que han sido entrenados en la misma tarea y que producen predicciones similares? Por ejemplo, entreno una SVM y un modelo de regresión lineal en la misma tarea y ambos producen predicciones muy similares. Calculo explicaciones usando un método de mi elección y analizo cuán diferentes son las explicaciones. Si las explicaciones son muy similares, las explicaciones son muy consistentes. Encuentro esta propiedad algo complicada, ya que los dos modelos podrían usar características diferentes, pero obtener predicciones similares (también llamado Efecto Rashomon). En este caso, no es deseable una alta consistencia porque las explicaciones tienen que ser muy diferentes. Es deseable una alta consistencia si los modelos realmente dependen de relaciones similares. Estabilidad: ¿Qué tan similares son las explicaciones para instancias similares? Mientras que la coherencia compara explicaciones entre modelos, la estabilidad compara explicaciones entre instancias similares para un modelo fijo. Alta estabilidad significa que ligeras variaciones en las características de una observación no cambian sustancialmente la explicación (a menos que estas ligeras variaciones también cambien fuertemente la predicción). La falta de estabilidad puede ser el resultado de una alta variación del método de explicación. En otras palabras, el método de explicación se ve fuertemente afectado por ligeros cambios en los valores de las características de la observación a explicar. La falta de estabilidad también puede ser causada por componentes no deterministas del método de explicación, como un paso de muestreo de datos: un ejemplo de esto es el uso del método sustituto local. La alta estabilidad siempre es deseable. Comprensibilidad: ¿Qué tan bien entendemos los humanos las explicaciones? Esto se parece a una propiedad más entre muchas, pero es el elefante en la habitación. Difícil de definir y medir, pero extremadamente importante para acertar. Muchas personas están de acuerdo en que la comprensión depende de la audiencia. Las ideas para medir la comprensibilidad incluyen medir el tamaño de la explicación (número de características con un peso distinto de cero en un modelo lineal, número de reglas de decisión, ) o probar qué tan bien las personas pueden predecir el comportamiento del modelo de aprendizaje automático a partir de las explicaciones. También se debe considerar la comprensión de las características utilizadas en la explicación. Una transformación compleja de características podría ser menos comprensible que las características originales. Certeza: ¿La explicación refleja la certeza del modelo de aprendizaje automático? Muchos modelos de aprendizaje automático solo dan predicciones sin una declaración sobre la confianza de los modelos de que la predicción es correcta. Si el modelo predice un 4% de probabilidad de cáncer para un paciente, ¿es igual de cierto que un 4% de probabilidad para otro paciente con diferentes valores de características, pero igual valor predicho? Una explicación que incluye la certeza del modelo es muy útil. Grado de importancia: ¿Qué tan bien refleja la explicación la importancia de las características o partes de la explicación? Por ejemplo, si se genera una regla de decisión como explicación para una predicción individual, ¿está claro cuál de las condiciones de la regla fue la más importante? Novedad: ¿La explicación refleja si una instancia de datos a explicar proviene de una nueva región muy alejada de la distribución de datos de entrenamiento? En tales casos, el modelo puede ser inexacto y la explicación puede ser inútil. El concepto de novedad está relacionado con el concepto de certeza. Cuanto mayor sea la novedad, más probable es que el modelo tenga poca certeza debido a la falta de datos. Representatividad: ¿Cuántas instancias cubre una explicación? Las explicaciones pueden abarcar todo el modelo (p. Ej., Interpretación de pesos en un modelo de regresión lineal) o representar solo una predicción individual (p. Ej., Valores de Shapley). Robnik-Sikonja, Marko, and Marko Bohanec. Perturbation-based explanations of prediction models. Human and Machine Learning. Springer, Cham. 159-175. (2018). "],["amigables.html", "2.6 Explicaciones amigables para los humanos", " 2.6 Explicaciones amigables para los humanos Profundicemos y descubramos lo que los humanos vemos como buenas explicaciones y cuáles son las implicaciones para el aprendizaje automático interpretable. La investigación en humanidades puede ayudarnos a descubrirlo. Miller (2017) ha realizado un gran estudio de publicaciones sobre explicaciones, y este capítulo se basa en su resumen. En este capítulo quiero convencerte de lo siguiente: Como explicación de un evento, los humanos preferimos explicaciones cortas (solo 1 o 2 causas) que contrastan la situación actual con una situación en la que el evento no hubiera ocurrido. Las causas especialmente anormales proporcionan buenas explicaciones. Las explicaciones son interacciones sociales entre el explicador y el explicado (receptor de la explicación) y, por lo tanto, el contexto social tiene una gran influencia en el contenido real de la explicación. Cuando necesitas explicaciones con TODOS los factores para una predicción o comportamiento particular, no deseas una explicación amigable para los humanos, sino una atribución causal completa. Probablemente desees una atribución causal si estás legalmente obligado a especificar todas las características influyentes o si depuras el modelo de aprendizaje automático. En este caso, ignora los siguientes puntos. En todos los demás casos, donde los destinatarios de la explicación son laicos o tienen poco tiempo tiempo, las siguientes secciones te deberían resultar interesantes. 2.6.1 ¿Qué es una explicación? Una explicación es la respuesta a una pregunta de por qué (Miller 2017). ¿Por qué el tratamiento no funcionó en el paciente? ¿Por qué fue rechazado mi préstamo? ¿Por qué todavía no hemos sido contactados por la vida alienígena? Las dos primeras preguntas pueden responderse con una explicación cotidiana, mientras que la tercera proviene de la categoría Fenómenos científicos más generales y preguntas filosóficas. Nos centramos en las explicaciones de tipo cotidiano, porque son relevantes para el aprendizaje automático interpretable. Las preguntas que comienzan con cómo generalmente se pueden reformular como preguntas de por qué: ¿Cómo se rechazó mi préstamo? puede convertirse en ¿Por qué se rechazó mi préstamo?. A continuación, el término explicación se refiere al proceso social y cognitivo de explicación, pero también al producto de estos procesos. El explicador puede ser un ser humano o una máquina. 2.6.2 ¿Qué es una buena explicación? Esta sección condensa aún más el resumen de Miller sobre explicaciones buenas y agrega implicaciones concretas para el aprendizaje automático interpretable. Las explicaciones son contrastantes (Lipton 19909). Los humanos generalmente no preguntamos por qué se hizo una determinada predicción, sino por qué se hizo esta predicción en lugar de otra predicción. Tendemos a pensar en casos contrafácticos, es decir, ¿Cómo habría sido la predicción si la entrada X hubiera sido diferente?. Para una predicción del precio de la vivienda, el propietario podría estar interesado en saber por qué el precio previsto fue alto, en comparación con el precio más bajo que esperaba. Si mi solicitud de préstamo es rechazada, no me importa escuchar todos los factores que generalmente hablan a favor o en contra de un rechazo. Estoy interesado en los factores en mi solicitud que tendrían que cambiar para obtener el préstamo. Quiero saber el contraste entre mi aplicación y la versión de mi solicitud que sería aceptada. El reconocimiento de que las explicaciones contrastantes importan es un hallazgo importante para el aprendizaje automático explicable. De la mayoría de los modelos interpretables, es posible extraer una explicación que contrasta implícitamente una predicción de una instancia con la predicción de una instancia de datos artificiales o un promedio de instancias. Los médicos podrían preguntar: ¿Por qué el medicamento no funcionó para mi paciente?. Y podrían querer una explicación que contraste a su paciente con un paciente para quien el medicamento funcionó y que sea similar al paciente que no responde. Las explicaciones contrastantes son más fáciles de entender que explicaciones completas. Una explicación completa de la pregunta del médico de por qué el medicamento no funciona puede incluir: El paciente ha tenido la enfermedad durante 10 años, 11 genes se sobreexpresan, el cuerpo del paciente es muy rápido en descomponer el medicamento en químicos ineficaces. Una explicación contrastante podría ser mucho más simple: en contraste con el paciente que responde, el paciente que no responde tiene una cierta combinación de genes que hacen que el medicamento sea menos efectivo. La mejor explicación es la que destaca la mayor diferencia entre el objeto de interés y el objeto de referencia. Lo que significa para el aprendizaje automático interpretable: los humanos no queremos una explicación completa para una predicción, sino comparar las diferencias con la predicción de otra observación (que puede ser artificial). La creación de explicaciones contrastantes depende de la aplicación, porque requiere un punto de referencia para la comparación. Y esto puede depender del punto de datos a explicar, pero también del usuario que recibe la explicación. Un usuario de un sitio web de predicción del precio de la vivienda puede querer tener una explicación de una predicción del precio de la vivienda en contraste con su propia casa o tal vez con otra casa en el sitio web o tal vez con una casa promedio en el vecindario. La solución para la creación automatizada de explicaciones contrastantes también podría implicar la búsqueda de prototipos o arquetipos en los datos. Las explicaciones se seleccionan. La gente no espera explicaciones que cubran la lista real y completa de causas de un evento. Estamos acostumbrados a seleccionar una o dos causas de una variedad de causas posibles como LA explicación. Como prueba, encienda las noticias de TV: El descenso en los precios de las acciones se atribuye a una creciente reacción contra el producto de la compañía debido a problemas con la última actualización de software. Tsubasa y su equipo perdieron el partido debido a una defensa débil: dieron a sus oponentes demasiado espacio para desarrollar su estrategia. La creciente desconfianza de las instituciones establecidas y nuestro gobierno son los principales factores que han reducido la participación electoral. El hecho de que un evento puede explicarse por varias causas se llama Efecto Rashomon. Rashomon es una película japonesa que cuenta historias alternativas y contradictorias (explicaciones) sobre la muerte de un samurai. Para los modelos de aprendizaje automático, es ventajoso si se puede hacer una buena predicción a partir de diferentes características. Los métodos de conjunto que combinan múltiples modelos con diferentes características (diferentes explicaciones) generalmente funcionan bien porque promediar esas historias hace que las predicciones sean más sólidas y precisas. Pero también significa que hay más de una explicación selectiva de por qué se hizo una determinada predicción. Lo que significa para el aprendizaje automático interpretable: Haz la explicación muy breve, dé solo 1 a 3 razones, incluso si el mundo es más complejo. El método LIME hace un buen trabajo con esto. Las explicaciones son sociales. Son parte de una conversación o interacción entre el explicador y el receptor de la explicación. El contexto social determina el contenido y la naturaleza de las explicaciones. Si quisiera explicarle a una persona técnica por qué las criptomonedas digitales valen tanto, diría cosas como: La contabilidad descentralizada, distribuida y basada en blockchain, que no puede ser controlado por una entidad central, resuena con las personas que desean asegurarse su riqueza, lo que explica la alta demanda y el precio. Pero a mi abuela le diría: Mira, abuela: las criptomonedas son un poco como el oro de la computadora. A la gente le gusta y paga mucho por el oro, y a los jóvenes les gusta y pagan mucho por el oro de la computadora. Lo que significa para el aprendizaje automático interpretable: Presta atención al entorno social de tu aplicación de aprendizaje automático y al público objetivo. Obtener la parte social del modelo de aprendizaje automático correcto depende completamente de su aplicación específica. Encuentra expertos de las humanidades (por ejemplo, psicólogos y sociólogos) para que te ayuden. Las explicaciones se centran en lo anormal. Las personas se enfocan más en causas anormales para explicar los eventos (Kahnemann y Tversky, 198110). Estas son causas que tenían una pequeña probabilidad pero que, sin embargo, ocurrieron. La eliminación de estas causas anormales habría cambiado mucho el resultado (explicación contrafáctica). Los humanos consideramos este tipo de causas anormales como buenas explicaciones. Un ejemplo de trumbelj y Kononenko (2011)11 es: supongamos que tenemos un conjunto de datos de situaciones de prueba entre profesores y alumnos. Los estudiantes asisten a un curso y lo aprueban directamente después de una presentación exitosa. El maestro tiene la opción de hacer preguntas adicionales al alumno para evaluar su conocimiento. Los estudiantes que no puedan responder estas preguntas reprobarán el curso. Los estudiantes pueden tener diferentes niveles de preparación, lo que se traduce en diferentes probabilidades de responder correctamente las preguntas del maestro (si deciden evaluar al estudiante). Queremos predecir si un alumno aprobará el curso y explicar nuestra predicción. La posibilidad de aprobar es del 100% si el maestro no hace preguntas adicionales; de lo contrario, la probabilidad de aprobar depende del nivel de preparación del alumno y la probabilidad resultante de responder las preguntas correctamente. Escenario 1: el maestro generalmente hace preguntas adicionales a los estudiantes (por ejemplo, 95 de cada 100 veces). Un estudiante que no estudió (10% de posibilidades de aprobar la parte de la pregunta) no fue uno de los afortunados y recibe preguntas adicionales que no responde correctamente. ¿Por qué el alumno reprobó el curso? Yo diría que fue culpa del estudiante por no estudiar. Escenario 2: el profesor rara vez hace preguntas adicionales (por ejemplo, 2 de cada 100 veces). Para un estudiante que no ha estudiado las preguntas, predeciríamos una alta probabilidad de aprobar el curso porque las preguntas son poco probables. Por supuesto, uno de los estudiantes no se preparó para las preguntas, lo que le da un 10% de posibilidades de aprobar las preguntas. No tiene suerte y el profesor hace preguntas adicionales que el alumno no puede responder y no aprueba el curso. ¿Cuál es la razón del fracaso? Yo diría que ahora, la mejor explicación es porque el profesor evaluó al alumno. Era poco probable que el maestro hiciera la prueba, por lo que se comportó de manera anormal. Lo que significa para el aprendizaje automático interpretable: Si una de las características de entrada para una predicción fue anormal en algún sentido (como una categoría rara de una característica categórica) y la característica influyó en la predicción, debe incluirse en una explicación, incluso si otras características normales tienen la misma influencia en la predicción que la anormal. Una característica anormal en nuestro ejemplo de predicción del precio de la vivienda podría ser que una vivienda bastante cara tiene dos balcones. Incluso si algún método de atribución determina que los dos balcones contribuyen tanto a la diferencia de precio como el tamaño promedio de la casa, el vecindario bueno o la reciente renovación, la característica anormal dos balcones podría ser la mejor explicación de por qué la casa es tan costosa. Las explicaciones son verdaderas. Las buenas explicaciones demuestran ser ciertas en la realidad (es decir, en otras situaciones). Pero inquietantemente, este no es el factor más importante para una buena explicación. Por ejemplo, la selectividad parece ser más importante que la veracidad. Una explicación que selecciona solo una o dos causas posibles rara vez cubre la lista completa de causas relevantes. La selectividad omite parte de la verdad. No es cierto que solo uno o dos factores, por ejemplo, hayan causado un colapso del mercado de valores: la verdad es que hay millones de causas que influyen en millones de personas para que actúen de tal manera que al final se causó un colapso. Lo que significa para el aprendizaje automático interpretable: La explicación debe predecir el evento con la mayor veracidad posible, que en el aprendizaje automático a veces se llama fidelidad. Entonces, si decimos que un segundo balcón aumenta el precio de una casa, eso también debería aplicarse a otras casas (o al menos a casas similares). Para los humanos, la fidelidad de una explicación no es tan importante como su selectividad, su contraste y su aspecto social. Las buenas explicaciones son consistentes con las creencias previas del explicado. Los humanos tendemos a ignorar la información que es inconsistente con sus creencias anteriores. Este efecto se llama sesgo de confirmación (Nickerson 1998 12). Las explicaciones no se salvan de este tipo de sesgo. La gente tenderá a devaluar o ignorar explicaciones que no concuerden con sus creencias. El conjunto de creencias varía de persona a persona, pero también hay creencias previas basadas en grupos, como las cosmovisiones políticas. Lo que significa para el aprendizaje automático interpretable: Las buenas explicaciones son consistentes con las creencias anteriores. Esto es difícil de integrar en el aprendizaje automático y probablemente comprometería drásticamente el rendimiento predictivo. Nuestra creencia previa sobre el efecto del tamaño de la casa en el precio previsto es que cuanto más grande sea la casa, mayor será el precio. Supongamos que un modelo también muestra un efecto negativo del tamaño de la casa en el precio previsto para algunas casas. El modelo ha aprendido esto porque mejora el rendimiento predictivo (debido a algunas interacciones complejas), pero este comportamiento contradice fuertemente nuestras creencias anteriores. Puede aplicar restricciones de monotonicidad (una característica solo puede afectar la predicción en una dirección) o usar algo como un modelo lineal que tenga esta propiedad. Las buenas explicaciones son generales y probables. Una causa que puede explicar muchos eventos es muy general y podría considerarse una buena explicación. Ten en cuenta que esto contradice la afirmación de que las causas anormales son buenas explicaciones. A mi entender, las causas anormales superan a las causas generales. Las causas anormales son, por definición, raras en el escenario dado. En ausencia de un evento anormal, una explicación general se considera una buena explicación. También recuerda que las personas tienden a juzgar mal las probabilidades de eventos conjuntos. (Joe es bibliotecario. ¿Es más probable que sea una persona tímida o una persona tímida a la que le gusta leer libros?) Un buen ejemplo es La casa es cara porque es grande, lo cual es una buena explicación de por qué las casas son caras o baratas. Lo que significa para el aprendizaje automático interpretable: La generalidad se puede medir fácilmente con el soporte de la función, que es el número de instancias a las que se aplica la explicación dividido por el número total de instancias. Lipton, Peter. Contrastive explanation. Royal Institute of Philosophy Supplements 27 (1990): 247-266. Kahneman, Daniel, and Amos Tversky. The Simulation Heuristic. Stanford Univ CA Dept of Psychology. (1981). trumbelj, Erik, and Igor Kononenko. A general method for visualizing and explaining black-box regression models. In International Conference on Adaptive and Natural Computing Algorithms, 2130. Springer. (2011). Nickerson, Raymond S. Confirmation Bias: A ubiquitous phenomenon in many guises. Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998). "],["conjuntosdedatos.html", "Capítulo 3 Conjuntos de datos", " Capítulo 3 Conjuntos de datos A lo largo del libro, todos los modelos y técnicas se aplican a conjuntos de datos reales que están disponibles gratuitamente en línea. Utilizaremos diferentes conjuntos de datos para diferentes tareas: Clasificación, regresión y clasificación de textos. "],["bike-data.html", "3.1 Alquiler de bicicletas (Regresión)", " 3.1 Alquiler de bicicletas (Regresión) Este conjunto de datos contiene recuentos diarios de bicicletas alquiladas de la empresa de alquiler de bicicletas Capital-Bikeshare en Washington D.C., junto con información meteorológica y estacional. Los datos fueron puestos a disposición por Capital-Bikeshare. Fanaee-T y Gama (2013)13 agregaron datos meteorológicos e información sobre la temporada. El objetivo es predecir cuántas bicicletas se alquilarán dependiendo del clima y el día. Los datos se pueden descargar del Repositorio de aprendizaje automático de UCI. Se agregaron nuevas características al conjunto de datos y no se usaron todas las características originales para los ejemplos de este libro. Aquí está la lista de características que se utilizaron: Recuento de bicicletas, incluidos usuarios ocasionales y registrados. El recuento se utiliza como objetivo en la tarea de regresión. La temporada, ya sea primavera, verano, otoño o invierno. Indicador de si el día fue feriado o no. El año, 2011 o 2012. Número de días desde el 01.01.2011 (el primer día en el conjunto de datos). Esta característica se introdujo para tener en cuenta la tendencia a lo largo del tiempo. Indicador de si el día fue un día laboral o un fin de semana. La situación climática de ese día. Uno de: despejado, pocas nubes, parcialmente nublado, nublado niebla + muchas nubes, niebla + pocas nubes, niebla nieve ligera, lluvia ligera + tormenta eléctrica + nubes dispersas, lluvia ligera + nubes dispersas fuertes lluvias + granizo + tormenta eléctrica + niebla, nieve + niebla Temperatura en grados Celsius. Humedad relativa en porcentaje (0 a 100). Velocidad del viento en km por hora. Para los ejemplos en este libro, los datos han sido ligeramente procesados. Puedes encontrar el script R de procesamiento en el repositorio de Github junto con el archivo RData final. Fanaee-T, Hadi, and Joao Gama. Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence. Springer Berlin Heidelberg, 115. doi:10.1007/s13748-013-0040-3. (2013). "],["spam-data.html", "3.2 Comentarios de spam de YouTube (clasificación de texto)", " 3.2 Comentarios de spam de YouTube (clasificación de texto) Como ejemplo de clasificación de texto, trabajamos con 1956 comentarios en inglés, de 5 videos de YouTube diferentes. Afortunadamente, los autores que utilizaron este conjunto de datos en un artículo sobre clasificación de spam hicieron que los datos estén disponibles gratuitamente (Alberto, Lochter y Almeida (2015)14). Los comentarios se recopilaron a través de la API de YouTube de cinco de los diez videos más vistos en YouTube en el primer semestre de 2015. Los 5 son videos musicales. Uno de ellos es Gangnam Style del artista coreano Psy. Los otros artistas fueron Katy Perry, LMFAO, Eminem y Shakira. Revisa algunos de los comentarios. Los comentarios fueron etiquetados manualmente como spam o legítimos. El spam se codificó con un 1 y los comentarios legítimos con un 0. CONTENT CLASS Huh, anyway check out this you[tube] channel: kobyoshi02 1 Hey guys check out my new channel and our first vid THIS IS US THE MONKEYS!!! Im the monkey in the white shirt,please leave a like comment and please subscribe!!!! 1 just for test I have to say murdev.com 1 me shaking my sexy ass on my channel enjoy ^_^ 1 watch?v=vtaRGgvGtWQ Check this out . 1 Hey, check out my new website!! This site is about kids stuff. kidsmediausa . com 1 Subscribe to my channel 1 i turned it on mute as soon is i came on i just wanted to check the views 0 You should check my channel for Funny VIDEOS!! 1 and u should.d check my channel and tell me what I should do next! 1 También puedes ir a YouTube y echar un vistazo a la sección de comentarios. Pero no te dejes atrapar en el infierno de YouTube, y por favor no termines viendo videos de monos robando y bebiendo cócteles de turistas en la playa. El detector de spam de Google también ha cambiado mucho desde 2015. Mira el rompe-records Gangam Style aquí. Si deseas jugar con los datos, puedes encontrar el archivo RData junto con el R-script con algunas funciones convenientes en el repositorio de Github del libro. Alberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. Tubespam: comment spam filtering on YouTube. In Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 13843. IEEE. (2015). "],["cervical.html", "3.3 Factores de riesgo para el cáncer de cuello uterino (Clasificación)", " 3.3 Factores de riesgo para el cáncer de cuello uterino (Clasificación) El conjunto de datos sobre el cáncer cervical contiene indicadores y factores de riesgo para predecir si una mujer tendrá cáncer cervical. Las características incluyen datos demográficos (como edad), estilo de vida e historial médico. Los datos se pueden descargar desde el repositorio de UCI Machine Learning y Fernandes, Cardoso y Fernandes lo describen. (2017)15. El subconjunto de características utilizadas en los ejemplos del libro son: Edad en años Número de parejas sexuales Primera relación sexual (edad en años) Número de embarazos Fumar o no Años fumando Anticonceptivos hormonales si o no Anticonceptivos hormonales (en años) Dispositivo intrauterino sí o no (DIU) Número de años con un dispositivo intrauterino (DIU) ¿Ha tenido alguna vez una enfermedad de transmisión sexual (ETS) sí o no? Número de diagnósticos de ETS Tiempo desde el primer diagnóstico de ETS Tiempo desde el último diagnóstico de ETS La biopsia resulta Saludable o Cáncer. Objetivo de clasificación. La biopsia sirve como estándar para diagnosticar el cáncer cervical. Para los ejemplos en este libro, el resultado de la biopsia se utilizó como objetivo. Los valores faltantes para cada columna fueron imputados por la moda (valor más frecuente), que probablemente sea una mala solución, ya que la respuesta verdadera podría estar correlacionada con la probabilidad de que falte un valor. Probablemente hay un sesgo porque las preguntas son de naturaleza muy privada. Pero este no es un libro sobre la imputación de datos faltantes, por lo que la imputación por la moda tendrá que ser suficiente para los ejemplos. Para reproducir los ejemplos de este libro con este conjunto de datos, busque el script R de preprocesamiento y el archivo RData final en el repositorio de Github del libro. Fernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. Transfer learning with partial observability applied to cervical cancer screening. In Iberian Conference on Pattern Recognition and Image Analysis, 24350. Springer. (2017). "],["simple.html", "Capítulo 4 Modelos interpretables", " Capítulo 4 Modelos interpretables La forma más fácil de lograr la interpretabilidad es usar aquellos algoritmos que crean modelos interpretables. La regresión lineal, la regresión logística y el árbol de decisión son modelos interpretables comúnmente utilizados. En los siguientes capítulos hablaremos sobre estos modelos. No en detalle, solo lo básico, porque ya hay una tonelada de libros, videos, tutoriales, documentos y más material disponible. Nos centraremos en cómo interpretar los modelos. El libro trata sobre regresión lineal, regresión logística, otras extensiones de regresión lineal, árboles de decisión, reglas de decisión y el algoritmo RuleFit con más detalle. También mostraremos otros modelos interpretables. Todos los modelos interpretables explicados en este libro son interpretables en un nivel modular, con la excepción del método de k-vecinos más cercanos. La siguiente tabla ofrece una descripción general de los tipos de modelos interpretables y sus propiedades. Un modelo es lineal si la asociación entre la entrada y la salida se modela linealmente. Un modelo con restricciones de monotonicidad asegura que la relación entre una característica y el resultado objetivo siempre va en la misma dirección en todo el rango de la característica: Un aumento en el valor de la característica siempre conduce a un aumento o siempre a una disminución en el resultado objetivo. La monotonicidad es útil para la interpretación de un modelo porque facilita la comprensión de una relación. Algunos modelos pueden incluir automáticamente interacciones entre características para predecir el resultado objetivo. Puedes incluir interacciones en cualquier tipo de modelo creando manualmente esas características de interacción. Las interacciones pueden mejorar el rendimiento predictivo, pero si son demasiadas o demasiado complejas pueden dañar la capacidad de interpretación. Algunos modelos manejan solo la regresión, algunos solo la clasificación y otros ambos. Desde esta tabla, puedes seleccionar un modelo interpretable adecuado para tu tarea, ya sea regresión (regr) o clasificación (clas): Algoritmo Lineal Monótono Interacción Tarea Regresión lineal Sí Sí No regr Regresión logística No Sí No clas Árboles de decisión No Algunos Sí clas,regr RuleFit Sí No Sí clas,regr Bayes ingenuo No Sí No clas k-vecinos más cercanos No No No clas,regr "],["lineal.html", "4.1 Regresión lineal", " 4.1 Regresión lineal Un modelo de regresión lineal predice el objetivo como una suma ponderada de las entradas de características. La linealidad de la relación facilita la interpretación. Los modelos de regresión lineal son utilizados por estadísticos, informáticos y otras personas que abordan problemas cuantitativos. Los modelos lineales se pueden usar para modelar la dependencia de un objetivo de regresión y de algunas características x. Las relaciones aprendidas son lineales y se pueden escribir para una sola instancia i de la siguiente manera: \\[y=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}+\\epsilon\\] El resultado previsto de una instancia es una suma ponderada de sus características p. Los betas (\\(\\beta_{j}\\)) representan los pesos o coeficientes de las características aprendidas. El primer peso en la suma (\\(\\beta_0\\)) se denomina intercepto y no se multiplica con un valor de característica. El épsilon (\\(\\epsilon\\)) es el error que cometemos, es decir, la diferencia entre la predicción y el resultado real. Se supone que estos errores siguen una distribución gaussiana, lo que significa que cometemos errores en direcciones negativas y positivas y cometemos muchos errores pequeños y pocos errores grandes. Se pueden usar varios métodos para estimar el peso óptimo. El método de mínimos cuadrados ordinarios generalmente se usa para encontrar los pesos que minimizan las diferencias al cuadrado entre los resultados reales y los estimados: \\[\\hat{\\boldsymbol{\\beta}}=\\arg\\!\\min_{\\beta_0,\\ldots,\\beta_p}\\sum_{i=1}^n\\left(y^{(i)}-\\left(\\beta_0+\\sum_{j=1}^p\\beta_jx^{(i)}_{j}\\right)\\right)^{2}\\] No discutiremos en detalle cómo se pueden encontrar los pesos óptimos, pero si estás interesado, puedes leer el capítulo 3.2 del libro Los elementos del aprendizaje estadístico (Friedman, Hastie y Tibshirani 2009)16 o uno de los otros recursos en línea sobre modelos de regresión lineal. La mayor ventaja de los modelos de regresión lineal es la linealidad: Simplifica el procedimiento de estimación y, lo que es más importante, estas ecuaciones lineales tienen una interpretación fácil de entender a nivel modular (es decir, los pesos). Esta es una de las principales razones por las que el modelo lineal y todos los modelos similares están tan extendidos en campos académicos como la medicina, la sociología, la psicología y muchos otros campos de investigación cuantitativa. Por ejemplo, en el campo de la medicina, no solo es importante predecir el resultado clínico de un paciente, sino también cuantificar la influencia del fármaco y al mismo tiempo tener en cuenta el sexo, la edad y otras características de manera interpretable. Los pesos estimados vienen con intervalos de confianza. Un intervalo de confianza es un rango para la estimación de peso que cubre el peso verdadero con una cierta confianza. Por ejemplo, un intervalo de confianza del 95% para un peso de 2 podría variar de 1 a 3. La interpretación de este intervalo sería: Si repetimos la estimación 100 veces con datos recién muestreados, el intervalo de confianza incluiría el peso verdadero en 95 de los 100 casos, dado que el modelo de regresión lineal es el modelo correcto para los datos. Si el modelo es el modelo correcto depende de si las relaciones en los datos cumplen ciertos supuestos, que son linealidad, normalidad, homocedasticidad, independencia, características fijas y ausencia de multicolinealidad. Linealidad El modelo de regresión lineal obliga a la predicción a ser una combinación lineal de características, que es tanto su mayor fortaleza como su mayor limitación. La linealidad conduce a modelos interpretables. Los efectos lineales son fáciles de cuantificar y describir. Son aditivos, por lo que es fácil separar los efectos. Si sospechas interacciones de entidades o una asociación no lineal de una entidad con el valor objetivo, puedes agregar términos de interacción o utilizar splines de regresión. Normalidad Se supone que el resultado objetivo dadas las características sigue una distribución normal. Si se viola esta suposición, los intervalos de confianza estimados de los pesos de las características no son válidos. Homocedasticidad (varianza constante) Se supone que la varianza de los términos de error es constante en todo el espacio de características. Supón que deseas predecir el valor de una casa dada la superficie habitable en metros cuadrados. Estimas un modelo lineal que asume que, independientemente del tamaño de la casa, el error en torno a la respuesta pronosticada tiene la misma variación. Esta suposición a menudo se viola en la realidad. En el ejemplo de la casa, es plausible que la variación de los términos de error alrededor del precio predicho sea mayor para las casas más grandes, ya que los precios son más altos y hay más margen para las fluctuaciones de precios. Supón que el error promedio (diferencia entre el precio predicho y el real) en su modelo de regresión lineal es de 50,000 euros. Si asumes la homocedasticidad, asumes que el error promedio de 50,000 es el mismo para casas que cuestan 1 millón y para casas que cuestan solo 40,000. Esto no es razonable, porque significaría que podemos esperar precios de vivienda negativos. Independencia Se supone que cada observación es independiente de cualquier otra observación. Si realizas mediciones repetidas, como múltiples análisis de sangre por paciente, las observaciones no son independientes. Para datos dependientes necesitas modelos especiales de regresión lineal, como modelos de efectos mixtos, GLMM o GEE. Si usas el modelo de regresión lineal normal, puedes sacar conclusiones incorrectas del modelo. Características fijas Las características de entrada se consideran fijas. Fijo significa que se tratan como constantes dadas y no como variables estadísticas. Esto implica que están libres de errores de medición. Esta es una suposición poco realista. Sin ese supuesto, sin embargo, tendrías que adaptarte a modelos de error de medición muy complejos que tengan en cuenta los errores de medición de sus características de entrada. Y generalmente no quieres hacer eso. Ausencia de multicolinealidad No deseas características fuertemente correlacionadas, porque esto arruina la estimación de los pesos. En una situación en la que dos características están fuertemente correlacionadas, se vuelve problemático estimar los pesos porque los efectos de la característica son aditivos y se vuelve indeterminable a cuál de las características correlacionadas atribuir los efectos. 4.1.1 Interpretación La interpretación de una ponderación en el modelo de regresión lineal depende del tipo de la característica correspondiente. Característica numérica: aumentar la característica numérica en una unidad cambia el resultado estimado por su peso. Un ejemplo de una característica numérica es el tamaño de una casa. Característica binaria: una característica que toma uno de los dos valores posibles para cada instancia. Un ejemplo es la característica La casa viene con un jardín. Uno de los valores cuenta como la categoría basal (en algunos lenguajes de programación codificados con 0), como Sin jardín. Cambiar la característica de la categoría basal a la otra categoría cambia el resultado estimado por el peso de la característica. Característica categórica con múltiples categorías: Una característica con un número fijo de valores posibles. Un ejemplo es la característica tipo de piso, con posibles categorías alfombra, laminado y parquet. Una solución para tratar con muchas categorías es la codificación en caliente -hot encoding-, lo que significa que cada categoría tiene su propia columna binaria. Para una característica categórica con categorías L, solo necesitas L-1 columnas, porque la columna restante tendría información redundante (por ejemplo, cuando las columnas 1 a L-1 tienen valor 0 para una instancia, sabemos que el valor de esa variable es el de la categoría restante). La interpretación para cada categoría es entonces la misma que la interpretación para las características binarias. Algunos lenguajes, como R, permiten codificar características categóricas de varias maneras, como se describe más adelante en este capítulo. Intercepto \\(\\beta_0\\): El intercepto es el peso de la característica para la característica constante, que siempre es 1 para todas las instancias. La mayoría de los paquetes de software agregan automáticamente esta característica 1 para estimar la intercepción. La interpretación es: Para una instancia con todos los valores de características numéricas en cero y los valores de características categóricas en las categorías de referencia, la predicción del modelo es el peso del intercepto. La interpretación del intercepto generalmente no es relevante porque las instancias con todos los valores de características en cero a menudo no tienen sentido. La interpretación solo es significativa cuando las características se han estandarizado (media de cero, desviación estándar de uno). En estos casos, el intercepto refleja el resultado previsto de una instancia en la que todas las características tienen su valor medio. La interpretación de las características en el modelo de regresión lineal se puede automatizar mediante el uso de las siguientes plantillas de texto. Interpretación de una característica numérica Un aumento de la característica \\(x_ {k}\\) en una unidad aumenta la predicción para y en \\(\\beta_k\\) unidades cuando todos los demás valores de la característica permanecen fijos. Interpretación de una característica categórica Cambiar la característica \\(x_{k}\\) de la categoría de referencia a la otra categoría aumenta la predicción para y en \\(\\beta_{k}\\) cuando todas las demás funciones permanecen fijas. Otra medida importante para interpretar modelos lineales es la medición de R cuadrado. Esta medida te indica qué cantidad de la varianza total de tu resultado objetivo es explicada por el modelo. Cuanto mayor sea el R cuadrado, mejor explicará tu modelo los datos. La fórmula para calcular el R-cuadrado es: \\[R^2=1-SSE/SST\\] SSE es la suma al cuadrado de los términos de error: \\[SSE=\\sum_{i=1}^n(y^{(i)}-\\hat{y}^{(i)})^2\\] SST es la suma al cuadrado de la varianza de datos: \\[SST=\\sum_{i=1}^n(y^{(i)}-\\bar{y})^2\\] El SSE te indica cuánta varianza queda después de ajustar el modelo lineal, que se mide por las diferencias al cuadrado entre los valores objetivo predichos y reales. SST es la varianza total del resultado objetivo. El R-cuadrado te indica cuánta de tu varianza puede explicarse por el modelo lineal. El R-cuadrado varía entre 0 para modelos donde el modelo no explica los datos en absoluto y 1 para los modelos que explican toda la varianza en sus datos. Hay un problema, porque el R-cuadrado aumenta con el número de características en el modelo, incluso si no contienen ninguna información sobre el valor objetivo en absoluto. Por lo tanto, es mejor usar el R cuadrado ajustado, que representa la cantidad de características utilizadas en el modelo. Su cálculo es: \\[\\bar{R}^2=R^2-(1-R^2)\\frac{n-1}{n-p-1}\\] donde p es el número de características y n el número de instancias. No tiene sentido interpretar un modelo con un R cuadrado muy bajo (ajustado), porque dicho modelo básicamente no explica gran parte de la varianza. Cualquier interpretación de los pesos no sería significativa. Importancia de la característica La importancia de una característica en un modelo de regresión lineal se puede medir por el valor absoluto de su estadístico t. El estadístico t es el peso estimado escalado con su error estándar. \\[t_{\\hat{\\beta}_j}=\\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\] Examinemos lo que esta fórmula nos dice: La importancia de una característica aumenta con el aumento de peso. Esto tiene sentido. Mientras más varianza tenga el peso estimado (= menos seguro estamos del valor correcto), menos importante es la característica. Esto también tiene sentido. 4.1.2 Ejemplo En este ejemplo, utilizamos el modelo de regresión lineal para predecir el número de bicicletas alquiladas en un día en particular, dada la información del clima y el calendario. Para la interpretación, examinamos los pesos de regresión estimados. Las características se reparten entre numéricas y categóricas. Para cada característica, la tabla muestra el peso estimado, el error estándar de la estimación (SE) y el valor absoluto del estadístico t (|t|). Weight SE |t| (Intercept) 2399.4 238.3 10.1 seasonSUMMER 899.3 122.3 7.4 seasonFALL 138.2 161.7 0.9 seasonWINTER 425.6 110.8 3.8 holidayHOLIDAY -686.1 203.3 3.4 workingdayWORKING DAY 124.9 73.3 1.7 weathersitMISTY -379.4 87.6 4.3 weathersitRAIN/SNOW/STORM -1901.5 223.6 8.5 temp 110.7 7.0 15.7 hum -17.4 3.2 5.5 windspeed -42.5 6.9 6.2 days_since_2011 4.9 0.2 28.5 Interpretación de una característica numérica (temperatura): Un aumento de la temperatura en 1 grado Celsius aumenta el número previsto de bicicletas en 110.7, cuando todas las demás características permanecen fijas. Interpretación de una característica categórica (weathersit): El número estimado de bicicletas es -1901.5 más bajo cuando está lloviendo, nevando o tormentoso, en comparación con el buen tiempo - suponiendo nuevamente que todas las demás características no cambian. Cuando el clima es brumoso, el número previsto de bicicletas es -379.4 menor en comparación con el buen clima, dado que todas las demás características siguen siendo las mismas. Todas las interpretaciones siempre vienen con la nota al pie de página de que todas las demás características siguen siendo las mismas. Esto se debe a la naturaleza de los modelos de regresión lineal. El objetivo previsto es una combinación lineal de las características ponderadas. La ecuación lineal estimada es un hiperplano en el espacio característica / objetivo (una línea simple en el caso de una característica única). Los pesos especifican la pendiente (gradiente) del hiperplano en cada dirección. El lado bueno es que la aditividad aísla la interpretación de un efecto de característica individual de todas las demás características. Eso es posible porque todos los efectos de la característica (= peso por valor de la característica) en la ecuación se combinan con un signo más. En el lado negativo de las cosas, la interpretación ignora la distribución conjunta de las características. Aumentar una característica, pero no cambiar otra, puede conducir a puntos de datos poco realistas o al menos improbables. Por ejemplo, aumentar el número de habitaciones puede ser poco realista sin aumentar también el tamaño de una casa. 4.1.3 Interpretación visual Varias visualizaciones hacen que el modelo de regresión lineal sea fácil y rápido de comprender para los humanos. 4.1.3.1 Gráfica de peso La información de la tabla de peso (estimaciones de peso y varianza) se puede visualizar en un gráfico de peso. La siguiente gráfica muestra los resultados del modelo de regresión lineal anterior. FIGURA 4.1: Los pesos estimados se muestran en los puntos, y los intervalos del 95% de confianza se muestran en las líneas. La gráfica de peso muestra que el clima lluvioso / nevoso / tormentoso tiene un fuerte efecto negativo en el número previsto de bicicletas. El peso de la característica de día laborable es cercano a cero y se incluye cero en el intervalo del 95%, lo que significa que el efecto no es estadísticamente significativo. Algunos intervalos de confianza son muy cortos y las estimaciones son cercanas a cero, aunque los efectos característicos fueron estadísticamente significativos. La temperatura es uno de esos candidatos. El problema con la gráfica de peso es que las características se miden en diferentes escalas. Mientras que para el clima el peso estimado refleja la diferencia entre el clima bueno y lluvioso / tormentoso / nevado, para la temperatura solo refleja un aumento de 1 grado Celsius. Puedes hacer los pesos más comparables al escalar las características (media cero y desviación estándar de uno) antes de ajustar el modelo lineal. 4.1.3.2 Gráfico de efectos Los pesos del modelo de regresión lineal pueden analizarse de manera más significativa cuando se multiplican por los valores reales de la característica. Los pesos dependen de la escala de las funciones y serán distintos, si tienes una variable que mida la altura de una persona y cambias de metro a centímetro. El peso cambiará, pero los efectos reales en sus datos no lo harán. También es importante conocer la distribución de su característica en los datos, porque si tiene una variación muy baja, significa que casi todas las instancias tienen una contribución similar de esta característica. La gráfica de efectos puede ayudarte a comprender cuánto contribuye la combinación de peso y función a las predicciones en sus datos. Comienza calculando los efectos, que es el peso por característica multiplicado por el valor de característica de una instancia: \\[\\text{effect}_{j}^{(i)}=w_{j}x_{j}^{(i)}\\] Los efectos se pueden visualizar con diagramas de caja. Una caja contiene el rango de efectos para la mitad de sus datos (cuantiles de efectos del 25% al 75%). La línea vertical en el cuadro es el efecto medio, es decir, el 50% de las instancias tienen un efecto más bajo y la otra mitad más alto en la predicción. Las líneas horizontales se extienden a \\(\\pm1.5\\text{IQR}/\\sqrt{n}\\), siendo IQR el rango intercuartil (Q3 menos Q1). Los puntos son valores atípicos. Los efectos de características categóricas se pueden resumir en una sola gráfica de caja, en comparación con la gráfica de peso, donde cada categoría tiene su propia fila. FIGURA 4.2: El gráfico de efectos de características muestra la distribución de los efectos (=valor de característica veces peso de característica) en los datos, por característica. Las mayores contribuciones al número esperado de bicicletas alquiladas provienen de la variable de temperatura y la variable de días, que captura la tendencia del alquiler de bicicletas con el tiempo. La temperatura tiene un amplio rango de cuánto contribuye a la predicción. La característica de tendencia del día va de cero a grandes contribuciones positivas, porque el primer día en el conjunto de datos (01.01.2011) tiene un efecto de tendencia muy pequeño y el peso estimado para esta característica es positivo (4.93). Esto significa que el efecto aumenta con cada día y es más alto para el último día en el conjunto de datos (31.12.2012). Ten en cuenta que para los efectos con un peso negativo, las observaciones con un efecto positivo son aquellas que tienen un valor de característica negativo. Por ejemplo, los días con un alto efecto negativo de la velocidad del viento son los que tienen altas velocidades del viento. 4.1.4 Explicación de predicciones individuales ¿Cuánto ha contribuido cada característica de una instancia a la predicción? Esto puede responderse calculando los efectos para esta instancia. Una interpretación de los efectos específicos de la observación solo tiene sentido en comparación con la distribución del efecto para cada característica. Queremos explicar la predicción del modelo lineal para la 6 observación del conjunto de datos de la bicicleta. La instancia tiene los siguientes valores de características. Feature Value season SPRING yr 2011 mnth JAN holiday NO HOLIDAY weekday THU workingday WORKING DAY weathersit GOOD temp 1.604356 hum 51.8261 windspeed 6.000868 cnt 1606 days_since_2011 5 Para obtener los efectos de característica de esta instancia, tenemos que multiplicar sus valores de característica por los pesos correspondientes del modelo de regresión lineal. Para el valor WORKING DAY de la característica workingday, el efecto es, 124.9. Para una temperatura de 1.6 grados Celsius, el efecto es 177.6. Agregamos estos efectos individuales como cruces al gráfico de efectos, que nos muestra la distribución de los efectos en los datos. Esto nos permite comparar los efectos individuales con la distribución de efectos en los datos. FIGURA 4.3: El gráfico de efectos para una observación muestra la distribución del efecto y marca con una cruz la instancia de interés. Si promediamos las predicciones para las instancias de datos de entrenamiento, obtenemos un promedio de 4504. En comparación, la predicción de la instancia 6-ésima es pequeña, ya que solo se predicen 1571 alquileres de bicicletas. El gráfico de efectos revela la razón por la cual. Los diagramas de caja muestran las distribuciones de los efectos para todas las instancias del conjunto de datos, los cruces muestran los efectos para la instancia 6. La instancia 6 tiene un efecto de baja temperatura porque en este día la temperatura era de 2 grados, que es baja en comparación con la mayoría de los otros días (y recuerde que el peso de la característica de temperatura es positivo). Además, el efecto de la característica de tendencia days_since_2011 es pequeño en comparación con otras instancias de datos porque esta instancia es de principios de 2011 (5 days) y la característica de tendencia también tiene un peso positivo. 4.1.5 Codificación de características categóricas Hay varias formas de codificar una característica categórica, y la elección influye en la interpretación de los pesos. El estándar en los modelos de regresión lineal es la codificación del tratamiento, que es suficiente en la mayoría de los casos. Usando diferentes codificaciones se reduce a crear diferentes matrices (de diseño) desde una sola columna con la variable categórica. Esta sección presenta tres codificaciones diferentes, pero hay muchas más. El ejemplo utilizado tiene seis instancias y una característica categórica con tres categorías. Para las dos primeras instancias, la característica toma la categoría A; para los casos tres y cuatro, categoría B; y para las dos últimas instancias, categoría C. Codificación de tratamiento En la codificación del tratamiento, el peso por categoría es la diferencia estimada en la predicción entre la categoría correspondiente y la categoría basal, o de referencia. El intercepto intersección del modelo lineal es la media de la categoría de referencia (cuando todas las demás características siguen siendo las mismas). La primera columna de la matriz de diseño es el intercepto, que siempre es 1. La columna dos indica si la instancia i está en la categoría B, la columna tres indica si está en la categoría C. No hay necesidad de una columna para la categoría A, porque entonces la ecuación lineal estaría sobreespecificada y no se puede encontrar una solución única para los pesos. Es suficiente saber que una instancia no está en la categoría B o C. Matriz de características: \\[\\begin{pmatrix}1&amp;0&amp;0\\\\1&amp;0&amp;0\\\\1&amp;1&amp;0\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\1&amp;0&amp;1\\\\\\end{pmatrix}\\] Codificación de efecto En la codificación de efecto, el peso por categoría es la diferencia en la y-estimada en la categoría correspondiente a la media general (dado que todas las demás características son cero o la categoría basal). La primera columna se usa para estimar la intersección. El peso \\(\\beta_{0}\\) asociado con el intercepto representa la media general y \\(\\beta_{1}\\), el peso de la columna dos, es la diferencia entre la media general y la categoría B. El efecto total de la categoría B es \\(\\beta_{0}+\\beta_{1}\\). La interpretación para la categoría C es equivalente. Para la categoría de referencia A, \\(-(\\beta_{1}+\\beta_{2})\\) es la diferencia con la media general y \\(\\beta_{0}-(\\beta_{1}+\\beta_{2})\\) el efecto general. Matriz de características: \\[\\begin{pmatrix}1&amp;-1&amp;-1\\\\1&amp;-1&amp;-1\\\\1&amp;1&amp;0\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\1&amp;0&amp;1\\\\\\end{pmatrix}\\] Codificación dummy En la codificación dummy, el \\(\\beta\\) por categoría es el valor medio estimado de y para cada categoría (dado que todos los demás valores de características son cero o la categoría basal). Ten en cuenta que la intersección se ha omitido aquí para que se pueda encontrar una solución única para los pesos del modelo lineal. Matriz de características: \\[\\begin{pmatrix}1&amp;0&amp;0\\\\1&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;1\\\\0&amp;0&amp;1\\\\\\end{pmatrix}\\] Si deseas profundizar un poco más en las diferentes codificaciones de características categóricas, consulta esta página web de resumen y esta publicación de blog. 4.1.6 ¿Los modelos lineales crean buenas explicaciones? A juzgar por los atributos que constituyen una buena explicación, como se presenta en el capítulo Explicaciones amigables para los humanos, los modelos lineales no crean las mejores explicaciones. Son contrastantes, pero la referencia es una observación donde todas las características numéricas son cero y las características categóricas están en sus categorías basales. Esta suele ser una instancia artificial sin sentido que es poco probable que ocurra en sus datos o realidad. Hay una excepción: Si todas las características numéricas están centradas en la media (característica menos la media de la característica) y todas las características categóricas están codificadas por efecto, la instancia de referencia es el punto de datos donde todas las características toman el valor medio de la característica. Esto también podría ser un punto de datos inexistente, pero al menos podría ser más probable o más significativo. En este caso, los pesos multiplicados por los valores de la característica (efectos de la característica) explican la contribución al resultado predicho en contraste con la instancia media. Otro aspecto de una buena explicación es la selectividad, que se puede lograr en modelos lineales usando menos características o entrenando modelos lineales dispersos. Pero por defecto, los modelos lineales no crean explicaciones selectivas. Los modelos lineales crean explicaciones verdaderas, siempre que la ecuación lineal sea un modelo apropiado para la relación entre características y resultados. Cuantas más no linealidades e interacciones haya, menos preciso será el modelo lineal y menos sinceras sus explicaciones. La linealidad hace que las explicaciones sean más generales y más simples. La naturaleza lineal del modelo, creo, es el factor principal por el cual las personas usan modelos lineales para explicar las relaciones. 4.1.7 Modelos lineales dispersos Los ejemplos de los modelos lineales que he elegido se ven bien y ordenados, ¿no es así? Pero en realidad, es posible que no tengas solo un puñado de características, sino cientos o miles. En esos casos, la interpretabilidad va cuesta abajo. Incluso puedes encontrarte en una situación en la que hay más características que instancias, y no puede ajustarse a un modelo lineal estándar en absoluto. La buena noticia es que hay formas de introducir la reducción (= pocas características) en los modelos lineales. 4.1.7.1 Lasso Lasso es una forma automática y conveniente de introducir la reducción en el modelo de regresión lineal. Lasso significa operador de reducción y selección, menor absoluto y, cuando se aplica en un modelo de regresión lineal, realiza la selección de características y la regularización de los pesos de las características seleccionadas. Consideremos el problema de minimización que optimizan los pesos: \\[min_{\\boldsymbol{\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-x_i^T\\boldsymbol{\\beta})^2\\right)\\] Lasso agrega un término a este problema de optimización. \\[min_{\\boldsymbol{\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-x_{i}^T\\boldsymbol{\\beta})^2+\\lambda||\\boldsymbol{\\beta}||_1\\right)\\] El término \\(||\\boldsymbol{\\beta}||_1\\), la norma L1 del vector de características, genera una penalización de los pesos grandes. Cuando se usa la norma L1, muchos de los pesos reciben una estimación de 0 y los otros se reducen. El parámetro lambda (\\(\\lambda\\)) controla la fuerza del efecto de regularización y generalmente se ajusta mediante validación cruzada. Especialmente cuando lambda es grande, muchos pesos se convierten en 0. Los pesos de las características se pueden visualizar en función del término de penalización lambda. El peso de cada característica se representa mediante una curva en la siguiente figura. FIGURA 4.4: Incrementando la penalidad de los pesos, cada vez menos características tienen un estimador distinto a cero. A estas curvas también se las llama caminos de regularización. ¿Qué valor deberíamos elegir para lambda? Si ves el término de penalización como un parámetro de ajuste, puedes encontrar la lambda que minimiza el error del modelo con validación cruzada. También puedes considerar lambda como un parámetro para controlar la interpretabilidad del modelo. Cuanto mayor es la penalización, menos características están presentes en el modelo (porque sus pesos son cero) y mejor se puede interpretar el modelo. Ejemplo con lasso Vamos a predecir el alquiler de bicicletas con Lasso. Establecemos de antemano la cantidad de características que queremos tener en el modelo. Primero establezcamos el número en 2 características: Weight seasonSPRING 0.00 seasonSUMMER 0.00 seasonFALL 0.00 seasonWINTER 0.00 holidayHOLIDAY 0.00 workingdayWORKING DAY 0.00 weathersitMISTY 0.00 weathersitRAIN/SNOW/STORM 0.00 temp 52.33 hum 0.00 windspeed 0.00 days_since_2011 2.15 Las dos primeras características con pesos distintos de según Lasso son la temperatura (temp) y la tendencia temporal (days_since_2011). Ahora, seleccionemos 5 características: Weight seasonSPRING -389.99 seasonSUMMER 0.00 seasonFALL 0.00 seasonWINTER 0.00 holidayHOLIDAY 0.00 workingdayWORKING DAY 0.00 weathersitMISTY 0.00 weathersitRAIN/SNOW/STORM -862.27 temp 85.58 hum -3.04 windspeed 0.00 days_since_2011 3.82 Ten en cuenta que los pesos para temp y days_since_2011 difieren del modelo con dos características. La razón de esto es que al disminuir lambda, incluso las características que ya están en el modelo se penalizan menos y pueden obtener un peso absoluto mayor. La interpretación de los pesos de lasso corresponde a la interpretación de los pesos en el modelo de regresión lineal. Solo necesitas prestar atención a si las características están estandarizadas o no, porque esto afecta los pesos. En este ejemplo, el software estandarizó las funciones, pero los pesos se transformaron automáticamente para que coincidan con las escalas de funciones originales. Otros métodos para la dispersión en modelos lineales Se puede utilizar un amplio espectro de métodos para reducir el número de características en un modelo lineal. Métodos de preprocesamiento: Funciones seleccionadas manualmente: Siempre puede utilizar el conocimiento experto para seleccionar o descartar algunas funciones. El gran inconveniente es que no se puede automatizar y debes tener acceso a alguien que comprenda los datos. Selección univariante: Un ejemplo es el coeficiente de correlación. Solo tiene en cuenta las características que exceden un cierto umbral de correlación entre la característica y el objetivo. La desventaja es que solo considera las características individualmente. Es posible que algunas características no muestren una correlación con el objetivo hasta que el modelo lineal haya tenido en cuenta algunas otras características. Estas no se advertirán con métodos de selección univariantes. Métodos paso a paso: Selección hacia adelante: Ajusta el modelo lineal con una característica. Haz esto con cada característica. Selecciona el modelo que funcione mejor (por ejemplo, el R cuadrado más alto). Ahora, de nuevo, para las características restantes, ajusta diferentes versiones de su modelo agregando cada característica a su mejor modelo actual. Selecciona el que mejor funcione. Continúa hasta que se alcance algún criterio, como el número máximo de características en el modelo. Selección hacia atrás: Similar a la selección hacia adelante. Pero en lugar de agregar funciones, comienza con el modelo que contiene todas las funciones y prueba qué variable debes eliminar para obtener el mayor aumento de rendimiento. Repite esto hasta que se alcance algún criterio de detención. Recomiendo usar Lasso, porque puede ser automatizado, considera todas las características simultáneamente y puede controlarse mediante lambda. También funciona para el modelo de regresión logística para la clasificación. 4.1.8 Ventajas El modelado de las predicciones como una suma ponderada hace que la forma en la que se produce la predicción sea transparente. Con Lasso podemos asegurarnos de que la cantidad de funciones utilizadas sea pequeña. Muchas personas usan modelos de regresión lineal. Esto significa que en muchos lugares es aceptado para el modelado predictivo y hacer inferencia. Existe un alto nivel de expertiz y experiencia colectiva, que incluye materiales didácticos sobre modelos de regresión lineal e implementaciones de software. La regresión lineal se puede encontrar en R, Python, Java, Julia, Scala, Javascript,  Matemáticamente, es sencillo estimar los pesos y tiene una garantía para encontrar pesos óptimos (dado que los datos cumplen todos los supuestos del modelo de regresión lineal). Junto con los pesos, obtienes intervalos de confianza, pruebas y una sólida teoría estadística. También hay muchas extensiones del modelo de regresión lineal (ver capítulo sobre GLM, GAM y más). 4.1.9 Desventajas Los modelos de regresión lineal solo pueden representar relaciones lineales, es decir, una suma ponderada de las características de entrada. Cada no linealidad o interacción tiene que ser hecha a mano y entregada explícitamente al modelo como una característica de entrada. Los modelos lineales a menudo también no son tan buenos con respecto al rendimiento predictivo, porque las relaciones que se pueden aprender son muy restringidas y generalmente simplifican demasiado la realidad, que suele ser más compleja. La interpretación de un peso puede ser poco intuitiva porque depende de todas las demás características. Una característica con alta correlación positiva con el resultado Y y también con alta correlación positiva con otra característica podría tener un peso negativo en el modelo lineal, porque, dada la otra característica correlacionada, se correlaciona negativamente con Y en el espacio de alta dimensión. Las características completamente correlacionadas hacen que sea incluso imposible encontrar una solución única para la ecuación lineal. Un ejemplo: Tienes un modelo para predecir el valor de una casa, y tienes características como el número de habitaciones y el tamaño de la casa. El tamaño de la casa y el número de habitaciones están altamente correlacionados: cuanto más grande es una casa, más habitaciones tiene. Si tomas ambas características en un modelo lineal, puede suceder que el tamaño de la casa sea el mejor predictor y obtenga un gran peso positivo. El número de habitaciones podría terminar teniendo un peso negativo, ya que, dado que una casa tiene el mismo tamaño, aumentar el número de habitaciones podría hacerla menos valiosa. La ecuación lineal se vuelve menos estable cuando la correlación es demasiado fuerte. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009). "],["logística.html", "4.2 Regresión logística", " 4.2 Regresión logística La regresión logística modela las probabilidades en problemas de clasificación con dos resultados posibles. Es una extensión del modelo de regresión lineal para problemas de clasificación. 4.2.1 ¿Qué tiene de malo la regresión lineal para la clasificación? El modelo de regresión lineal puede funcionar bien para la regresión, pero falla en la clasificación. ¿Por qué? En el caso de dos clases, puedes etiquetar una de las clases con 0 y la otra con 1 y usar regresión lineal. Técnicamente funciona y la mayoría de los programas de modelos lineales devolverán pesos. Pero hay algunos problemas con este enfoque: Un modelo lineal no genera probabilidades, si no que trata las clases como números (0 y 1) y se ajusta al mejor hiperplano (para una sola característica, es una línea) que minimiza las distancias entre los puntos y el hiperplano. Por lo tanto, simplemente se interpola entre los puntos y no se puede interpretar como probabilidades. Un modelo lineal también extrapola, por lo que puede dar valores por debajo de cero y por encima de uno. Esta es una señal de que podría haber un enfoque más inteligente para la clasificación. Dado que el resultado previsto no es una probabilidad, sino una interpolación lineal entre puntos, no existe un umbral significativo en el que pueda distinguir una clase de la otra. Se ha dado una buena ilustración de este problema en Stackoverflow. Los modelos lineales no se extienden a problemas de clasificación con múltiples clases. Tendrías que comenzar a etiquetar la siguiente clase con 2, luego 3, y así sucesivamente. Aunque es posible que las clases no tengan un orden significativo, el modelo lineal forzaría una estructura extraña en la relación entre las características y las predicciones de su clase. De esta forma, cuanto mayor sea el valor de una característica con un peso positivo, más contribuiria a la predicción de una clase con un número más alto, incluso si las clases que obtienen un número similar no están más cerca que otras clases. FIGURA 4.5: Un modelo lineal clasifica los tumores como malignos (1) o benignos (0) dado su tamaño. Las líneas muestran la predicción del modelo lineal. Para los datos de la izquierda, podemos usar 0.5 como umbral de clasificación. Después de introducir algunos casos más de tumores malignos, un umbral de 0.5 ya no separa las clases. Los puntos se alteran ligeramente para reducir la sobreimpresión. 4.2.2 Teoría Una solución para la clasificación es la regresión logística. En lugar de ajustar una línea recta o un hiperplano, el modelo de regresión logística utiliza la función logística para forzar a que la salida de una ecuación lineal esté entre 0 y 1. La función logística se define como: \\[\\text{logistic}(\\eta)=\\frac{1}{1+exp(-\\eta)}\\] Y se ve así: FIGURA 4.6: La función logística. Produce números entre 0 y 1. En la entrada 0, genera 0.5. El paso de la regresión lineal a la regresión logística es algo sencillo. En el modelo de regresión lineal, hemos modelado la relación entre el resultado y las características con una ecuación lineal: \\[\\hat{y}^{(i)}=\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{(i)}_{p}\\] Para la clasificación, preferimos probabilidades entre 0 y 1, por lo que ajustamos el lado derecho de la ecuación a la función logística. Esto obliga a la salida a asumir solo valores entre 0 y 1. \\[P(y^{(i)}=1)=\\frac{1}{1+exp(-(\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{(i)}_{p}))}\\] Volvamos al ejemplo del tamaño del tumor nuevamente. Pero en lugar del modelo de regresión lineal, usamos el modelo de regresión logística: FIGURA 4.7: El modelo de regresión logística encuentra el límite de decisión correcto entre maligno y benigno dependiendo del tamaño del tumor. La línea es la función logística desplazada y exprimida para ajustarse a los datos. La clasificación funciona mejor con la regresión logística y podemos usar 0.5 como umbral en ambos casos. La inclusión de puntos adicionales no afecta sustancialmente la curva estimada. 4.2.3 Interpretación La interpretación de los pesos en la regresión logística difiere de la interpretación de los pesos en la regresión lineal, ya que el resultado en la regresión logística es una probabilidad entre 0 y 1. Los pesos ya no influyen en la probabilidad linealmente. La suma ponderada se transforma mediante la función logística en una probabilidad. Por lo tanto, necesitamos reformular la ecuación para la interpretación, de modo que solo el término lineal esté en el lado derecho de la fórmula. \\[log\\left(\\frac{P(y=1)}{1-P(y=1)}\\right)=log\\left(\\frac{P(y=1)}{P(y=0)}\\right)=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\] Llamamos al término en la función log() odds (chances, probabilidad del evento dividido por probabilidad del no evento), y envuelto en el logaritmo se llama log-odds. Esta fórmula muestra que el modelo de regresión logística es un modelo lineal para las log-odds. ¡Excelente! ¡Eso no suena útil! Con una pequeña combinación de los términos, puedes descubrir cómo cambia la predicción cuando una de las características \\(x_j\\) cambia en una unidad. Para hacer esto, primero podemos aplicar la función exp() a ambos lados de la ecuación: \\[\\frac{P(y=1)}{1-P(y=1)}=odds=exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\right)\\] Luego comparamos lo que sucede cuando aumentamos uno de los valores de la característica en 1. Pero en lugar de mirar la diferencia, miramos el ratio entre las dos predicciones: \\[\\frac{odds_{x_j+1}}{odds}=\\frac{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}(x_{j}+1)+\\ldots+\\beta_{p}x_{p}\\right)}{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}x_{j}+\\ldots+\\beta_{p}x_{p}\\right)}\\] Aplicamos la siguiente regla: \\[\\frac{exp(a)}{exp(b)}=exp(a-b)\\] Y eliminamos muchos términos: \\[\\frac{odds_{x_j+1}}{odds}=exp\\left(\\beta_{j}(x_{j}+1)-\\beta_{j}x_{j}\\right)=exp\\left(\\beta_j\\right)\\] Al final, tenemos algo tan simple como la exponencial del peso de una característica. Un cambio de una característica en una unidad cambia la razón entre las posibilidades (multiplicativa) por un factor de \\(\\exp(\\beta_j)\\). También podríamos interpretarlo de esta manera: Un cambio de \\(x_j\\) en una unidad aumenta la relación de log-odds en el valor del peso correspondiente. La mayoría de las personas interpretan el ratio de odds, porque se sabe que pensar en el logaritmo de algo es duro para el cerebro. Interpretar el ratio de odds ya requiere acostumbrarse. Por ejemplo, si tienes odds de 2, significa que la probabilidad de y=1 es el doble de y=0. Si tenés un peso (= ratio log-odds) de 0.7, al aumentar la característica respectiva en una unidad multiplica las probabilidades por exp (0.7) (aproximadamente 2) y las odds cambian a 4. Pero, por lo general, no manejas las probabilidades e interpretas los pesos solo como las razones de probabilidades. Porque para calcular realmente las probabilidades, necesitarías establecer un valor para cada característica, lo que solo tiene sentido si deseas ver una instancia específica de su conjunto de datos. Estas son las interpretaciones para el modelo de regresión logística con diferentes tipos de características: Característica numérica: Si aumenta el valor de la variable \\(x_{j}\\) en una unidad, las probabilidades estimadas cambian en un factor de \\(\\exp(\\beta_{j})\\) Característica categórica binaria: Uno de los dos valores de la variable es la categoría de referencia o basal (en algunos idiomas, el codificado en 0). Cambiar la variable \\(x_{j}\\) de la categoría de referencia a la otra categoría cambia las probabilidades estimadas por un factor de \\(\\exp(\\beta_{j})\\). Característica categórica con más de dos categorías: Una solución para lidiar con múltiples categorías es una hot encoding, lo que significa que cada categoría tiene su propia columna. Solo necesita L-1 columnas para una característica categórica con L categorías, de lo contrario está sobre-parametrizada. La categoría L-ésima es entonces la categoría de referencia. Puedes usar cualquier otra codificación que puedas usar en regresión lineal. La interpretación para cada categoría es equivalente a la interpretación de características binarias. Intercepto \\(\\beta_{0}\\): Cuando todas las características numéricas son cero y las características categóricas están en la categoría de referencia, las probabilidades estimadas son \\(\\exp(\\beta_{0})\\). La interpretación del peso del intercepto generalmente no es relevante. 4.2.4 Ejemplo Utilizamos el modelo de regresión logística para predecir cáncer cervical en función de algunos factores de riesgo. La siguiente tabla muestra los pesos estimados, las razones de odds asociadas y el error estándar de las estimaciones. TABLA 4.1: Los resultados de ajustar un modelo de regresión logística en el conjunto de datos de cáncer cervical. características utilizadas en el modelo, sus pesos estimados y sus correspondientes odds ratios, y los errores estándar de los pesos estimados. Weight Odds ratio Std. Error Intercept 2.91 18.36 0.32 Hormonal contraceptives y/n 0.12 1.12 0.30 Smokes y/n -0.26 0.77 0.37 Num. of pregnancies -0.04 0.96 0.10 Num. of diagnosed STDs -0.82 0.44 0.33 Intrauterine device y/n -0.62 0.54 0.40 Interpretación de una característica numérica (Num. of diagnosed STDs): Un aumento en el número de ETS (enfermedades de transmisión sexual) diagnosticadas cambia (aumenta) las probabilidades de cáncer frente a ausencia de cáncer por un factor de 0.44, cuando todas las demás características siguen siendo las mismas. Ten en cuenta que la correlación no implica causalidad. Interpretación de una característica categórica (Anticonceptivos hormonales si/no): Para las mujeres que usan anticonceptivos hormonales, las probabilidades de cáncer versus no cáncer son por un factor 1.12 menor, en comparación con las mujeres sin anticonceptivos hormonales, dado que todas las demás características permanecen igual. Al igual que en el modelo lineal, las interpretaciones siempre vienen con la cláusula de que todas las demás características permanecen igual. 4.2.5 Ventajas y desventajas Muchos de los pros y los contras del modelo de regresión lineal también se aplican al modelo de regresión logística. La regresión logística ha sido ampliamente utilizada por muchas personas diferentes, pero lucha con su expresividad restrictiva (por ejemplo, las interacciones deben agregarse manualmente). Otros modelos pueden tener un mejor rendimiento predictivo. Otra desventaja del modelo de regresión logística es que la interpretación es más difícil porque la interpretación de los pesos es multiplicativa y no aditiva. La regresión logística puede sufrir de separación completa. Si hay una característica que separe perfectamente las dos clases, el modelo de regresión logística ya no puede ser entrenado. Esto se debe a que el peso de esa característica no convergería, porque el peso óptimo sería infinito. Esto parece ser un poco desafortunado, porque tal característica es realmente útil. Sin embargo, no necesitas aprendizaje automático si tienes una regla simple que separa ambas clases. El problema de la separación completa se puede resolver introduciendo la penalización de los pesos o definiendo una distribución de probabilidad previa de los pesos. En el lado bueno, el modelo de regresión logística no es solo un modelo de clasificación, sino que también brinda probabilidades. Esta es una gran ventaja sobre los modelos que solo pueden proporcionar la clasificación final. Saber que una observación tiene una probabilidad del 99% para una clase en comparación con el 51% hace una gran diferencia. La regresión logística también puede extenderse de la clasificación binaria a la clasificación multiclase. Entonces se llama Regresión Multinomial. 4.2.6 Software Usé la función glm en R para todos los ejemplos. Puedes encontrar regresión logística en cualquier lenguaje de programación que pueda usarse para realizar análisis de datos, como Python, Java, Stata, Matlab,  "],["extend-lm.html", "4.3 GLM, GAM y más", " 4.3 GLM, GAM y más La mayor fortaleza, pero también la mayor debilidad del modelo de regresión lineal es que la predicción se modela como una suma ponderada de las características. Además, el modelo lineal trae muchos otros supuestos. La mala noticia (bueno, en realidad no es noticia) es que todos esos supuestos a menudo se violan en la realidad: El objetivo condicionado a las características podría tener una distribución no gaussiana, las características podrían interactuar y la relación entre las características y el resultado podría no ser lineal. La buena noticia es que la comunidad de estadísticos ha desarrollado una variedad de modificaciones que transforman el modelo de regresión lineal de una hoja simple a una navaja suiza. Este capítulo no es tu guía definitiva para extender modelos lineales. Por el contrario, sirve como una descripción general de las extensiones como los Modelos lineales generalizados (GLM) y los Modelos aditivos generalizados (GAM), y da una pequeña intuición. Después de leer, debes tener una descripción general sólida de cómo extender modelos lineales. Si deseas obtener más información sobre el modelo de regresión lineal primero, te sugiero que leas el capítulo sobre modelos de regresión lineal, si aún no lo has hecho. Recordemos la fórmula de un modelo de regresión lineal: \\[y=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}+\\epsilon\\] El modelo de regresión lineal supone que el resultado Y de una instancia puede expresarse mediante una suma ponderada de sus características p con un error individual \\(\\epsilon\\) que sigue una distribución gaussiana. Al forzar los datos en este corset de una fórmula, obtenemos mucha interpretación del modelo. Los efectos de las características son aditivos, lo que significa que no hay interacciones, y la relación es lineal, lo que significa que un aumento de una característica en una unidad debe traducirse directamente en un aumento / disminución del resultado previsto. El modelo lineal nos permite comprimir la relación entre una característica y el resultado esperado en un solo número, es decir, el peso estimado. Pero una suma ponderada simple es demasiado restrictiva para muchos problemas de predicción del mundo real. En este capítulo aprenderemos sobre tres problemas del modelo clásico de regresión lineal y cómo resolverlos. Hay muchos más problemas con supuestos posiblemente violados, pero nos centraremos en los tres que se muestran en la siguiente figura: FIGURA 4.8: Tres supuestos del modelo lineal (lado izquierdo): distribución gaussiana del resultado dadas las características, la aditividad (= sin interacciones) y la relación lineal. La realidad generalmente no se adhiere a esos supuestos (lado derecho): los resultados pueden tener distribuciones no gaussianas, las características pueden interactuar y la relación puede ser no lineal. Hay una solución para todos estos problemas: Problema: El resultado objetivo y dadas las características no sigue una distribución gaussiana. Ejemplo: supongamos que quiero predecir cuántos minutos voy a andar en bicicleta en un día determinado. Como características tengo el tipo de día, el clima, etc. Si uso un modelo lineal, podría predecir minutos negativos porque supone una distribución gaussiana que no se detiene en 0 minutos. Además, si quiero predecir probabilidades con un modelo lineal, puedo obtener probabilidades que son negativas o mayores que 1. Solución: Modelos lineales generalizados (GLM). Problema: las características interactúan. Ejemplo: En promedio, la lluvia ligera tiene un ligero efecto negativo en mi deseo de ir en bicicleta. Pero en verano, durante las horas pico, doy la bienvenida a la lluvia, ¡porque entonces todos los ciclistas de buen tiempo se quedan en casa y tengo los carriles bici para mí! Esta es una interacción entre el tiempo y el clima que no puede ser capturada por un modelo puramente aditivo. Solución: Agregar interacciones manualmente. Problema: La verdadera relación entre las características e Y no es lineal. Ejemplo: Entre 0 y 25 grados Celsius, la influencia de la temperatura en mi deseo de andar en bicicleta podría ser lineal, lo que significa que un aumento de 0 a 1 grado causa el mismo aumento en el deseo de ciclismo que un aumento de 20 a 21. Pero a temperaturas más altas, mi motivación para completar el ciclo se nivela e incluso disminuye: no me gusta andar en bicicleta cuando hace demasiado calor. Soluciones: Modelos aditivos generalizados (GAM); transformación de características. Las soluciones a estos tres problemas se presentan en este capítulo. Se omiten muchas extensiones adicionales del modelo lineal. Si intentara cubrir todo aquí, el capítulo se convertiría rápidamente en un libro dentro de un libro, sobre un tema que ya está cubierto en muchos otros libros. Pero como ya estás aquí, he hecho un pequeño problema+solución para las extensiones de modelo lineal, que puedes encontrar al final del capítulo. El nombre de la solución está destinado a servir como punto de partida para una búsqueda. 4.3.1 Resultados no gaussianos: GLM El modelo de regresión lineal supone que el resultado, dadas las características de entrada, sigue una distribución gaussiana. Este supuesto excluye muchos casos: El resultado también puede ser una categoría (cáncer versus salud), un recuento (número de niños), el tiempo hasta la ocurrencia de un evento (tiempo hasta el fallo de una máquina) o un resultado muy sesgado con unos valores muy altos (como el ingreso familiar). El modelo de regresión lineal se puede extender para modelar todos estos tipos de resultados. Esta extensión se llama Modelos lineales generalizados o GLM para abreviar. A lo largo de este capítulo, usaré el nombre GLM tanto para el marco general como para modelos particulares de ese marco. El concepto central de cualquier GLM es: Mantener la suma ponderada de las características, pero permitiendo distribuciones de resultados no gaussianas, y conectando la media esperada de esta distribución y la suma ponderada a través de una función posiblemente no lineal. Por ejemplo, el modelo de regresión logística supone una distribución de Bernoulli para el resultado y vincula la media esperada y la suma ponderada utilizando la función logística. El GLM vincula matemáticamente la suma ponderada de las características con el valor medio de la distribución asumida utilizando la función de enlace g, que se puede elegir de manera flexible dependiendo del tipo de resultado. \\[g(E_Y(y|x))=\\beta_0+\\beta_1{}x_{1}+\\ldots{}\\beta_p{}x_{p}\\] Los GLM consisten en tres componentes: La función de enlace g, la suma ponderada \\(X^T\\beta\\) (a veces llamada predictor lineal) y una distribución de probabilidad de la familia exponencial que define \\(E_Y\\). La familia exponencial es un conjunto de distribuciones que se pueden escribir con la misma fórmula (parametrizada) que incluye un exponente, la media y varianza de la distribución y algunos otros parámetros. No entraré en los detalles matemáticos porque este es un universo muy grande. Wikipedia tiene una lista de distribuciones ordenada de la familia exponencial. Puedes elegir cualquier distribución de esta lista para tu GLM. Según el tipo de resultado que deseas predecir, elige una distribución adecuada. ¿El resultado es un conteo de algo (por ejemplo, número de niños que viven en un hogar)? Entonces la distribución de Poisson podría ser una buena opción. ¿El resultado es siempre positivo (por ejemplo, tiempo entre dos eventos)? Entonces la distribución exponencial podría ser una buena opción. Consideremos el modelo lineal clásico como un caso especial de un GLM. La función de enlace para la distribución gaussiana en el modelo lineal clásico es simplemente la función de identidad. La distribución gaussiana se parametriza por la media y los parámetros de varianza. La media describe el valor que esperamos en promedio y la varianza describe cuánto varían los valores en torno a esta media. En el modelo lineal, la función de enlace vincula la suma ponderada de las características con la media de la distribución gaussiana. Bajo el marco GLM, este concepto se generaliza a cualquier distribución (de la familia exponencial), con funciones de enlace arbitrarias. Si Y es un conteo de algo, como la cantidad de cafés que alguien bebe en un día determinado, podríamos modelarlo con un GLM con una distribución de Poisson y el logaritmo natural como función de enlace: \\[ln(E_Y(y|x))=x^{T}\\beta\\] El modelo de regresión logística también es un GLM que asume una distribución de Bernoulli y utiliza la función logit como función de enlace. La media de la distribución binomial utilizada en la regresión logística es la probabilidad de que Y sea 1. \\[x^{T}\\beta=ln\\left(\\frac{E_Y(y|x)}{1-E_Y(y|x)}\\right)=ln\\left(\\frac{P(y=1|x)}{1-P(y=1|x)}\\right)\\] Y si resolvemos esta ecuación para tener P(y=1) en un lado, obtenemos la fórmula de regresión logística: \\[P(y=1)=\\frac{1}{1+exp(-x^{T}\\beta)}\\] Cada distribución de la familia exponencial tiene una función de enlace canónica, que puede derivarse matemáticamente de la distribución. El marco GLM permite elegir la función de enlace independientemente de la distribución. ¿Cómo elegir la función de enlace correcta? No hay una receta perfecta. Hay que tener en cuenta el conocimiento sobre la distribución del objetivo, pero también las consideraciones teóricas y qué tan bien el modelo se ajusta a tus datos reales. Para algunas distribuciones, la función de enlace canónica puede conducir a valores que no son válidos para esa distribución. En el caso de la distribución exponencial, la función de enlace canónica es el inverso negativo, que puede conducir a predicciones negativas que están fuera del dominio de la distribución exponencial. Como puedes elegir cualquier función de enlace, la solución simple es elegir otra función que respete el dominio de la distribución. Ejemplos He simulado un conjunto de datos sobre el comportamiento del consumo de café para resaltar la necesidad de GLM. Supongamos que has recopilado datos sobre tu comportamiento diario de consumo de café. Si no te gusta el café, finge que se trata de té. Junto con la cantidad de tazas, registras tu nivel de estrés actual en una escala del 1 al 10, qué tan bien dormías la noche anterior en una escala del 1 al 10 y si tuviste que trabajar ese día. El objetivo es predecir la cantidad de cafés, dadas las características de estrés, sueño y trabajo. Simulé datos durante 200 días. El estrés y el sueño se dibujaron uniformemente entre 1 y 10 y el trabajo sí/no se dibujó con una probabilidad de 50/50 (¡qué vida!). Para cada día, el número de cafés se extrajo de una distribución de Poisson, modelando la intensidad \\(\\lambda\\) (que también es el valor esperado de la distribución de Poisson) en función de las características sueño, estrés y trabajo. Puedes adivinar a dónde llevará esta historia: Oye, modelemos estos datos con un modelo lineal  Oh, no funciona  Probemos un GLM con distribución de Poisson  ¡SORPRESA! ¡Ahora funciona!. Espero no haberte arruinado demasiado la historia. Veamos la distribución de la variable objetivo, la cantidad de cafés en un día determinado: FIGURA 4.9: Distribución simulada del número de cafés diarios durante 200 días. En 76 de los días 200 no tomaste café y en el día más extremo tomaste 7. Usemos ingenuamente un modelo lineal para predecir la cantidad de cafés usando el nivel de sueño, el nivel de estrés y el trabajo sí/no como características. ¿Qué puede salir mal cuando asumimos falsamente una distribución gaussiana? Un supuesto erróneo puede invalidar las estimaciones, especialmente los intervalos de confianza de los pesos. Un problema más obvio es que las predicciones no coinciden con el dominio permitido del resultado real, como muestra la siguiente figura. FIGURA 4.10: Número previsto de cafés dependientes del estrés, el sueño y el trabajo. El modelo lineal predice valores negativos. El modelo lineal no tiene sentido, porque predice un número negativo de cafés. Este problema se puede resolver con los modelos lineales generalizados (GLM). Podemos cambiar la función de enlace y la distribución asumida. Una posibilidad es mantener la distribución gaussiana y usar una función de enlace que siempre conduzca a predicciones positivas como el log-link (inversa de la función exp) en lugar de la función de identidad. Aun mejor: Elegimos una distribución que corresponde al proceso de generación de datos y una función de enlace apropiada. Como el resultado es un recuento, la distribución de Poisson es una elección natural, junto con el logaritmo como función de enlace. En este caso, los datos incluso se generaron con la distribución de Poisson, por lo que Poisson GLM es la elección perfecta. El Poisson GLM ajustado conduce a la siguiente distribución de valores pronosticados: FIGURA 4.11: Número previsto de cafés que dependen del estrés, el sueño y el trabajo. El GLM con el supuesto de Poisson y el log-link es un modelo apropiado para este conjunto de datos. No hay cantidades negativas de cafés, se ve mucho mejor ahora. Interpretación de los pesos GLM La distribución asumida junto con la función de enlace determina cómo se interpretan los pesos estimados de las características. En el ejemplo del recuento de café, utilicé un GLM con distribución de Poisson y enlace de registro, lo que implica la siguiente relación entre las características y el resultado esperado. \\[ln(E(\\text{coffees}|\\text{stress},\\text{sleep},\\text{workYES}))=\\beta_0+\\beta_{\\text{stress}}x_{\\text{stress}}+\\beta_{\\text{sleep}}x_{\\text{sleep}}+\\beta_{\\text{workYES}}x_{\\text{workYES}}\\] Para interpretar los pesos, invertimos la función de enlace para poder interpretar el efecto de las características en el resultado esperado y no en el logaritmo del resultado esperado. \\[E(\\text{coffees}|\\text{stress},\\text{sleep},\\text{workYES})=exp(\\beta_0+\\beta_{\\text{stress}}x_{\\text{stress}}+\\beta_{\\text{sleep}}x_{\\text{sleep}}+\\beta_{\\text{workYES}}x_{\\text{workYES}})\\] Como todos los pesos están en la función exponencial, la interpretación del efecto no es aditiva, sino multiplicativa, porque exp(a + b) es exp (a) por exp (b). El último ingrediente para la interpretación son los pesos reales del ejemplo del juguete. La siguiente tabla enumera los pesos estimados y exp(pesos) junto con el intervalo de confianza del 95%: peso exp(peso) [2.5%, 97.5%] (Intercept) -0.16 0.85 [0.54, 1.32] stress 0.12 1.12 [1.07, 1.18] sleep -0.15 0.86 [0.82, 0.90] workYES 0.80 2.23 [1.72, 2.93] Aumentar el nivel de estrés en un punto multiplica el número esperado de cafés por el factor 1.12. Aumentar la calidad del sueño en un punto multiplica el número esperado de cafés por el factor 0.86. El número previsto de cafés en un día de trabajo es en promedio 2.23 veces el número de cafés en un día libre. En resumen, cuanto más estrés, menos sueño y más trabajo, más café se consume. En esta sección, aprendiste un poco sobre los modelos lineales generalizados que son útiles cuando el objetivo no sigue una distribución gaussiana. A continuación, veremos cómo integrar las interacciones entre dos características en el modelo de regresión lineal. 4.3.2 Interacciones El modelo de regresión lineal supone que el efecto de una característica es el mismo independientemente de los valores de las otras características (= sin interacciones). Pero a menudo hay interacciones en los datos. Para predecir el número de bicicletas alquilados, puede haber una interacción entre la temperatura y si es un día hábil o no. Quizás, cuando las personas tienen que trabajar, la temperatura no influye mucho en el número de bicicletas alquiladas, porque las personas andarán en la bicicleta alquilada para trabajar, pase lo que pase. En los días libres, muchas personas viajan por placer, pero solo cuando hace suficiente calor. Cuando se trata de alquilar bicicletas, se puede esperar una interacción entre la temperatura y la jornada laboral. ¿Cómo podemos lograr que el modelo lineal incluya interacciones? Antes de ajustar el modelo lineal, agrega una columna a la matriz de características que represente la interacción entre las características y ajuste el modelo como de costumbre. La solución es elegante en cierto modo, ya que no requiere ningún cambio del modelo lineal, solo columnas adicionales en los datos. En el ejemplo de la jornada laboral y la temperatura, agregaríamos una nueva característica que tiene ceros para los días sin trabajo, de lo contrario tiene el valor de la característica de temperatura, suponiendo que la jornada laboral sea la categoría de referencia. Supongamos que nuestros datos se ven así: trabajo temp Y 25 N 12 N 30 Y 5 La matriz de datos utilizada por el modelo lineal se ve ligeramente diferente. La siguiente tabla muestra el aspecto de los datos preparados para el modelo si no especificamos ninguna interacción. Normalmente, esta transformación se realiza automáticamente por cualquier software estadístico. Intercepto trabajoY temp 1 1 25 1 0 12 1 0 30 1 1 5 La primera columna es el intercepto. La segunda columna codifica la característica categórica, con 0 para la categoría de referencia y 1 para la otra. La tercera columna contiene la temperatura. Si queremos que el modelo lineal considere la interacción entre la temperatura y la característica de día laborable, debemos agregar una columna para la interacción: Intercepto trabajoY temp trabajoY.temp 1 1 25 25 1 0 12 0 1 0 30 0 1 1 5 5 La nueva columna workY.temp captura la interacción entre las características día laborable (trabajo) y temperatura (temperatura). Esta nueva columna de características es cero para una instancia si la característica de trabajo está en la categoría de referencia (N), de lo contrario, asume los valores de la característica de temperatura de instancias. Con este tipo de codificación, el modelo lineal puede aprender un efecto lineal diferente de la temperatura para ambos tipos de días. Este es el efecto de interacción entre las dos características. Sin un término de interacción, el efecto combinado de una característica categórica y numérica se puede describir mediante una línea que se desplaza verticalmente para las diferentes categorías. Si incluimos la interacción, permitimos que el efecto de las características numéricas (la pendiente) tenga un valor diferente en cada categoría. La interacción de dos características categóricas funciona de manera similar. Creamos características adicionales que representan combinaciones de categorías. Aquí hay algunos datos artificiales que contienen el día laboral (trabajo) y una característica meteorológica categórica (wthr): trabajo wthr Y 2 N 0 N 1 Y 2 A continuación, incluimos términos de interacción: Intercepto trabajoY wthr1 wthr2 trabajoY.wthr1 trabajoY.wthr2 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 La primera columna sirve para estimar el intercepto. La segunda columna es la característica de trabajo codificada. Las columnas tres y cuatro son para la característica del clima, que requiere dos columnas porque necesita dos pesos para capturar el efecto para tres categorías, una de las cuales es la categoría de referencia. El resto de las columnas capturan las interacciones. Para cada categoría de ambas características (excepto las categorías de referencia), creamos una nueva columna de características que es 1 si ambas características tienen una determinada categoría, de lo contrario 0. Para dos características numéricas, la columna de interacción es aún más fácil de construir: Simplemente multiplicamos ambas características numéricas. Existen enfoques para detectar y agregar automáticamente términos de interacción. Uno de ellos se puede encontrar en el Capítulo de RuleFit. El algoritmo RuleFit primero mina los términos de interacción y luego estima un modelo de regresión lineal que incluye interacciones. Nota del traductor: Para un ejemplo de términos de interacción consultar la versión original en inglés. 4.3.3 Efectos no lineales - GAM El mundo no es lineal. La linealidad en los modelos lineales significa que no importa qué valor tenga una instancia en una característica particular, aumentar el valor en una unidad siempre tiene el mismo efecto en el resultado previsto. ¿Es razonable suponer que aumentar la temperatura en un grado a 10 grados centígrados tiene el mismo efecto en el número de bicicletas de alquiler que aumentar la temperatura cuando ya tiene 40 grados? Intuitivamente, uno espera que el aumento de la temperatura de 10 a 11 grados centígrados tenga un efecto positivo en el alquiler de bicicletas y de 40 a 41 un efecto negativo, que también es el caso, como verás, en muchos ejemplos a lo largo del libro. La característica de la temperatura tiene un efecto lineal y positivo en el número de bicicletas de alquiler, pero en algún momento se aplana e incluso tiene un efecto negativo a altas temperaturas. Al modelo lineal no le importa, obedientemente encontrará el mejor plano lineal (minimizando la distancia euclidiana). Puede modelar relaciones no lineales utilizando una de las siguientes técnicas: Transformación simple de la característica (por ejemplo, logaritmo) Categorización de la función Modelos aditivos generalizados (GAM) Antes de entrar en los detalles de cada método, comencemos con un ejemplo que ilustra los tres. Tomé el conjunto de datos de alquiler de bicicletas y entrené un modelo lineal con solo la función de temperatura para predecir el número de bicicletas de alquiler. La siguiente figura muestra la pendiente estimada con: el modelo lineal estándar, un modelo lineal con temperatura transformada (logaritmo), un modelo lineal con temperatura tratada como característica categórica y utilizando splines de regresión (GAM). FIGURA 4.12: Predicción del número de bicicletas alquiladas utilizando solo la variable de temperatura. Un modelo lineal (arriba a la izquierda) no se ajusta bien a los datos. Una solución es transformar la función con, por ejemplo, logaritmo (arriba a la derecha), clasificarlo (abajo a la izquierda), que generalmente es una mala decisión, o usar modelos de aditivos generalizados que puedan ajustarse automáticamente a una curva suave de temperatura (abajo a la derecha). Transformación de características A menudo, el logaritmo de la característica se usa como una transformación. El uso del logaritmo indica que cada aumento de temperatura de 10 veces tiene el mismo efecto lineal en el número de bicicletas, por lo que cambiar de 1 grado Celsius a 10 grados Celsius tiene el mismo efecto que cambiando de 0.1 a 1 (suena mal). Otros ejemplos de transformaciones de características son la raíz cuadrada, la función cuadrada y la función exponencial. El uso de una transformación de característica significa que reemplaza la columna de esta característica en los datos con una función de la característica, como el logaritmo, y ajusta el modelo lineal como de costumbre. Algunos programas estadísticos también te permiten especificar transformaciones en la llamada del modelo lineal. Puedes ser creativo cuando transformas la función. La interpretación de la característica cambia según la transformación seleccionada. Si usas una transformación de logaritmo, la interpretación en un modelo lineal se convierte en: Si el logaritmo de la característica aumenta en uno, la predicción aumenta en el peso correspondiente. Cuando usas un GLM con una función de enlace que no es la función de identidad, la interpretación se vuelve más complicada, porque tienes que incorporar ambas transformaciones en la interpretación (excepto cuando se cancelan entre sí, como log y exp, donde la interpretación se hace más fácil). Categorización de características Otra posibilidad para lograr un efecto no lineal es discretizar la característica; conviértalo en una característica categórica. Por ejemplo, puede cortar la función de temperatura en 20 intervalos con los niveles [-10, -5), [-5, 0),  y así sucesivamente. Cuando utilizas la temperatura categorizada en lugar de la temperatura continua, el modelo lineal estimaría una función de paso porque cada nivel obtiene su propia estimación. El problema con este enfoque es que necesita más datos, es más probable que se sobreajuste y no está claro cómo discretizar la característica de manera significativa (intervalos equidistantes o cuantiles, ¿cuántos intervalos?). Solo usaría la discretización si hay un caso muy sólido para ello. Por ejemplo, para hacer que el modelo sea comparable a otro estudio. Modelos aditivos generalizados (GAM) ¿Por qué no simplemente permitir que el modelo lineal (generalizado) aprenda relaciones no lineales? Esa es la motivación detrás de los GAM. Los GAM relajan la restricción de que la relación debe ser una simple suma ponderada y, en cambio, suponen que el resultado puede ser modelado por una suma de funciones arbitrarias de cada característica. Matemáticamente, la relación en un GAM se ve así: \\[g(E_Y(y|x))=\\beta_0+f_1(x_{1})+f_2(x_{2})+\\ldots+f_p(x_{p})\\] La fórmula es similar a la fórmula GLM con la diferencia de que el término lineal \\(\\beta_j{}x_{j}\\) se reemplaza por una función más flexible \\(f_j(x_{j})\\). El núcleo de un GAM sigue siendo una suma de efectos de características, pero tiene la opción de permitir relaciones no lineales entre algunas características y la salida. Los efectos lineales también están cubiertos por el marco, porque para que las características se manejen linealmente, puede limitar sus \\(f_j(x_{j})\\) solo para tomar la forma de \\(x_{j}\\beta_j\\). La gran pregunta es cómo aprender funciones no lineales. La respuesta se llama splines o funciones de spline. Las splines son funciones que se pueden combinar para aproximar funciones arbitrarias. Un poco como apilar ladrillos de Lego para construir algo más complejo. Hay una cantidad confusa de formas de definir estas funciones de spline. Si estás interesado en aprender más sobre todas las formas de definir splines, te deseo buena suerte en tu viaje. No voy a entrar en detalles aquí, solo voy a construir una intuición. Lo que más me ayudó personalmente para comprender las splines fue visualizar las funciones individuales de splines y analizar cómo se modifica la matriz de datos. Por ejemplo, para modelar la temperatura con splines, eliminamos la característica de temperatura de los datos y la reemplazamos con, por ejemplo, 4 columnas, cada una de las cuales representa una función de spline. En general tendría más funciones de spline, solo reduje el número con fines ilustrativos. El valor para cada instancia de estas nuevas características de spline depende de los valores de temperatura de las instancias. Junto con todos los efectos lineales, el GAM también estima estos pesos spline. Los GAM también introducen un término de penalización para los pesos para mantenerlos cerca de cero. Esto reduce efectivamente la flexibilidad de las splines y reduce el sobreajuste. Un parámetro de suavidad que se usa comúnmente para controlar la flexibilidad de la curva se ajusta mediante validación cruzada. Ignorando el término de penalización, el modelado no lineal con splines es una ingeniería sofisticada. En el ejemplo en el que estamos prediciendo el número de bicicletas con un GAM utilizando solo la temperatura, la siguiente figura muestra cómo se ven estas funciones de spline: FIGURA 4.13: Para modelar suavemente el efecto de temperatura, utilizamos 4 funciones de spline. Cada valor de temperatura se asigna a (aquí) 4 valores de spline. Si una instancia tiene una temperatura de 30 ° C, la el valor para la primera característica de spline es -1, para el segundo 0.7, para el tercero -0.8 y para el cuarto 1.7. La curva real, que resulta de la suma de las funciones de spline ponderadas con los pesos estimados, se ve así: FIGURA 4.14: Efecto de la función GAM de la temperatura para predecir el número de bicicletas alquiladas (la temperatura se usa como la única característica). La interpretación de los efectos suaves requiere una verificación visual de la curva ajustada. Las splines generalmente se centran alrededor de la predicción media, por lo que un punto en la curva es la diferencia con la predicción media. Por ejemplo, a 0 grados centígrados, el número previsto de bicicletas es 3000 menor que la predicción promedio. 4.3.4 Ventajas Todas estas extensiones del modelo lineal son un poco un universo en sí mismas. Cualesquiera que sean los problemas que enfrentas con los modelos lineales, probablemente encontrarás una extensión que lo corrige. La mayoría de los métodos se han utilizado durante décadas. Por ejemplo, los GAM tienen casi 30 años. Muchos investigadores y profesionales de la industria tienen mucha experiencia con modelos lineales y los métodos son aceptados en muchas comunidades como status quo para el modelado. Además de hacer predicciones, puedes usar los modelos para hacer inferencia, sacar conclusiones sobre los datos, dado que los supuestos del modelo no se violan. Obtiene intervalos de confianza para pesos, pruebas de significación, intervalos de predicción y mucho más. El software estadístico generalmente tiene interfaces realmente buenas para adaptarse a GLM, GAM y modelos lineales más especiales. La opacidad de muchos modelos de aprendizaje automático proviene de 1) una falta de dispersión, lo que significa que se utilizan muchas características, 2) características que se tratan de manera no lineal, lo que significa que necesita más de un peso para describir el efecto, y 3) el modelado de interacciones entre las características. Suponiendo que los modelos lineales son altamente interpretables pero a menudo no se ajustan a la realidad, las extensiones descritas en este capítulo ofrecen una buena manera de lograr una transición suave hacia modelos más flexibles, al tiempo que conservan algo de la capacidad de interpretación. 4.3.5 Desventajas Como ventaja, he dicho que los modelos lineales viven en su propio universo. La gran cantidad de formas en que puede extender el modelo lineal simple es abrumadora, no solo para principiantes. En realidad, hay múltiples universos paralelos, porque muchas comunidades de investigadores y profesionales tienen sus propios nombres para los métodos que hacen más o menos lo mismo, lo que puede ser muy confuso. La mayoría de las modificaciones del modelo lineal hacen que el modelo sea menos interpretable. Cualquier función de enlace (en un GLM) que no sea la función de identidad complica la interpretación; las interacciones también complican la interpretación; Los efectos de características no lineales son menos intuitivos (como la transformación logarítmica) o ya no se pueden resumir en un solo número (por ejemplo, funciones de spline). Los GLM, GAM, etc. se basan en suposiciones sobre el proceso de generación de datos. Si se violan, la interpretación de los pesos ya no es válida. El rendimiento de los conjuntos basados en árboles, árbol de gradiente -gradient tree-, es en muchos casos mejor que los modelos lineales más sofisticados. Esto es en parte mi propia experiencia y en parte las observaciones de los modelos ganadores en plataformas como kaggle.com. 4.3.6 Software Todos los ejemplos en este capítulo fueron creados usando el lenguaje R. Para los GAM, se utilizó el paquete gam, pero hay muchos otros. R tiene una increíble cantidad de paquetes para extender los modelos de regresión lineal. R alberga más extensiones que cualquier otro lenguaje analítico, abarcando todas las extensiones imaginables del modelo de regresión lineal. Encontrarás implementaciones de p. GAM en Python (como pyGAM), pero estas implementaciones no son tan maduras. 4.3.7 Extensiones adicionales Como se prometió, aquí hay una lista de problemas que puedes encontrar con los modelos lineales, junto con el nombre de una solución para este problema que puedes copiar y pegar en tu motor de búsqueda favorito. Mis datos violan el supuesto de ser independientes e idénticamente distribuidos (iid). Por ejemplo, mediciones repetidas en el mismo paciente. Busca modelos mixtos o ecuaciones de estimación generalizadas. Mi modelo tiene errores heterocedasticos. Por ejemplo, al predecir el valor de una casa, los errores del modelo suelen ser mayores en las casas caras, lo que viola la homocedasticidad del modelo lineal. Busca regresión robusta. Tengo valores atípicos que influyen fuertemente en mi modelo. Busca regresión robusta. Quiero predecir el tiempo hasta que ocurra un evento. Los datos de tiempo hasta el evento generalmente vienen con mediciones censuradas, lo que significa que en algunos casos no hubo suficiente tiempo para observar el evento. Por ejemplo, una empresa quiere predecir el fallo de sus máquinas de hielo, pero solo tiene datos durante dos años. Algunas máquinas siguen intactas después de dos años, pero podrían fallar más tarde. Busca modelos de supervivencia paramétricos, regresión de Cox, análisis de supervivencia. Mi resultado para predecir es una categoría. Si el resultado tiene dos categorías, usa un modelo de regresión logística, que modela la probabilidad de las categorías. Si tienes más categorías, busca regresión multinomial. La regresión logística y la regresión multinomial son dos GLM. Quiero predecir categorías ordenadas. Por ejemplo, calificaciones escolares. Busca modelo de probabilidades proporcionales. Mi resultado es un recuento (como el número de hijos en una familia). Busca regresión de Poisson. El modelo de Poisson también es un GLM. También puedes tener el problema de que el valor de recuento de 0 es muy frecuente. Busca regresión de Poisson inflada a cero, modelo de obstáculo. No estoy seguro de qué características deben incluirse en el modelo para sacar conclusiones causales correctas. Por ejemplo, quiero saber el efecto de un medicamento sobre la presión arterial. El medicamento tiene un efecto directo sobre algún valor sanguíneo y este valor sanguíneo afecta el resultado. ¿Debo incluir el valor sanguíneo en el modelo de regresión? Busca inferencia causal, análisis de mediación. Me faltan datos. Busca imputación múltiple. Quiero integrar el conocimiento previo en mis modelos. Busca inferencia bayesiana. Me siento un poco deprimido últimamente. Busca Amazon Alexa Gone Wild !!! Versión completa de principio a fin. "],["arbol.html", "4.4 Árbol de decisión", " 4.4 Árbol de decisión La regresión lineal y los modelos de regresión logística fallan en situaciones donde la relación entre las características y el resultado es no lineal o donde las características interactúan entre sí. ¡Es hora de brillar para el árbol de decisión! Los modelos basados en árboles dividen los datos varias veces de acuerdo con ciertos valores de corte en las características. A través de la división, se crean diferentes subconjuntos del conjunto de datos, y cada observación pertenece a un subconjunto. Los subconjuntos finales se denominan nodos terminales o de hoja y los subconjuntos intermedios se denominan nodos internos. Para predecir el resultado en cada nodo hoja, se utiliza el resultado promedio de los datos de entrenamiento en este nodo. Los árboles se pueden usar para clasificación y regresión. Hay varios algoritmos que pueden hacer crecer un árbol. Difieren en la posible estructura del árbol (por ejemplo, número de divisiones por nodo), los criterios de cómo encontrar las divisiones, cuándo detener la división y cómo estimar los modelos simples dentro de los nodos de las hojas. El algoritmo de clasificación y regresión de árboles (CART) es probablemente el algoritmo más popular para la inducción de árboles. Nos centraremos en CART, pero la interpretación es similar para la mayoría de los otros tipos de árboles. Recomiendo el libro Los elementos del aprendizaje estadístico (Friedman, Hastie y Tibshirani 2009)17 para una introducción más detallada a CART. FIGURA 4.15: Árbol de decisión con datos artificiales. Las instancias con un valor mayor que 3 para la característica x1 terminan en el nodo 5. Todas las demás instancias se asignan al nodo 3 o al nodo 4, dependiendo de si los valores de la característica x2 exceden 1. La siguiente fórmula describe la relación entre el resultado y y las características x. \\[\\hat{y}=\\hat{f}(x)=\\sum_{m=1}^Mc_m{}I\\{x\\in{}R_m\\}\\] Cada instancia cae exactamente en un nodo hoja (= subconjunto \\(R_m\\)). \\(I_{\\{x\\in{}R_m\\}}\\) es la función de identidad que devuelve 1 si \\(x\\) está en el subconjunto \\(R_m\\) y 0 en caso contrario. Si una instancia cae en un nodo hoja \\(R_l\\), el resultado previsto es \\(\\hat{y}=c_l\\), donde \\(c_l\\) es el promedio de todas las instancias de entrenamiento en el nodo hoja \\(R_l\\). ¿Pero de dónde vienen los subconjuntos? Esto es bastante simple: CART toma una función y determina qué punto de corte minimiza la varianza de Y para una regresión o el índice de Gini de la distribución de clase de Y para clasificación. La varianza nos dice cuánto se distribuyen los valores y en un nodo alrededor de su valor medio. El índice de Gini nos dice cuán impuro es un nodo. Si todas las clases tienen la misma frecuencia, el nodo es impuro; si solo hay una clase presente, es completamente puro. La varianza y el índice de Gini se minimizan cuando los puntos de datos en los nodos tienen valores muy similares para Y. Como consecuencia, el mejor punto de corte hace que los dos subconjuntos resultantes sean lo más diferentes posible con respecto al resultado objetivo. Para las características categóricas, el algoritmo intenta crear subconjuntos probando diferentes agrupaciones de categorías. Después de determinar el mejor límite por característica, el algoritmo selecciona la característica para dividir que resultaría en la mejor partición en términos de varianza o índice de Gini y agrega esta división al árbol. El algoritmo continúa esta búsqueda y división recursivamente en ambos nodos nuevos hasta que se alcanza un criterio de detención. Los posibles criterios son: Un número mínimo de instancias que deben estar en un nodo antes de la división, o el número mínimo de instancias que deben estar en un nodo terminal. 4.4.1 Interpretación La interpretación es simple: Comenzando desde el nodo raíz, vas a los siguientes nodos y los bordes te dicen qué subconjuntos estás mirando. Una vez que llegues al nodo hoja, el nodo te indica el resultado predicho. Todos los bordes están conectados por Y, lo que significa que las condiciones deben cumplirse simultáneamente. Plantilla: Si la característica x es [menor / mayor] que el umbral c Y además  entonces el resultado predicho es el valor medio de y de las observaciones en ese nodo. Importancia de la característica La importancia general de una característica en un árbol de decisión se puede calcular de la siguiente manera: Revisa todas las divisiones para las que se utilizó la función y mide cuánto ha reducido la varianza o el índice de Gini en comparación con el nodo principal. La suma de todas las importancias se escala a 100. Esto significa que cada importancia puede interpretarse como una parte de la importancia general del modelo. Descomposición del árbol Las predicciones individuales de un árbol de decisión pueden explicarse descomponiendo la ruta de decisión en un componente por característica. Podemos rastrear una decisión a través del árbol y explicar una predicción por las contribuciones agregadas en cada nodo de decisión. El nodo raíz en un árbol de decisión es nuestro punto de partida. Si tuviéramos que usar el nodo raíz para hacer predicciones, predeciría la media del resultado de los datos de entrenamiento. Con la siguiente división, restamos o sumamos un término a esta suma, dependiendo del siguiente nodo en la ruta. Para llegar a la predicción final, debemos seguir la ruta de la instancia de datos que queremos explicar y seguir agregando a la fórmula. \\[\\hat{f}(x)=\\bar{y}+\\sum_{d=1}^D\\text{split.contrib(d,x)}=\\bar{y}+\\sum_{j=1}^p\\text{feat.contrib(j,x)}\\] La predicción de una instancia individual es la media del resultado objetivo más la suma de todas las contribuciones de las divisiones D que ocurren entre el nodo raíz y el nodo terminal donde termina la instancia. Sin embargo, no estamos interesados en las contribuciones divididas, sino en las contribuciones de características. Una característica puede usarse para más de una división o para ninguna. Podemos agregar las contribuciones para cada una de las características p y obtener una interpretación de cuánto ha contribuido cada característica a una predicción. 4.4.2 Ejemplo Echemos otro vistazo a los datos de alquiler de bicicletas. Queremos predecir el número de bicicletas alquiladas en un día determinado con un árbol de decisión. El árbol aprendido se ve así: FIGURA 4.16: Árbol de regresión instalado en los datos de alquiler de la bicicleta. La profundidad máxima permitida para el árbol se estableció en 2. La característica de tendencia (días desde 2011) y la temperatura (temperatura) tienen seleccionado para las divisiones. Los diagramas de caja muestran la distribución de los conteos de bicicletas en el nodo terminal. La primera división y una de las segundas divisiones se realizaron con la función de tendencia, que cuenta los días desde que comenzó la recopilación de datos y cubre la tendencia de que el servicio de alquiler de bicicletas se ha vuelto más popular con el tiempo. Para los días anteriores al día 105, el número previsto de bicicletas es de alrededor de 1800, entre el día 106 y 430 es de alrededor de 3900. Para los días posteriores al día 430, la predicción es 4600 (si la temperatura es inferior a 12 grados) o 6600 (si la temperatura es superior a 12 grados). La importancia de la característica nos dice cuánto ayudó una característica a mejorar la pureza de todos los nodos. Aquí, se utilizó la regresión, ya que predecir el alquiler de bicicletas es una tarea de regresión. El árbol visualizado muestra que tanto la temperatura como la tendencia temporal se usaron para las divisiones, pero no cuantifica qué característica fue más importante. La medida de importancia de la característica muestra que la tendencia temporal es mucho más importante que la temperatura. FIGURA 4.17: Importancia de las características medidas por cuánto se mejora la pureza del nodo en promedio. 4.4.3 Ventajas La estructura de árbol es ideal para capturar interacciones entre características en los datos. Los datos terminan en grupos distintos que a menudo son más fáciles de entender que los puntos en un hiperplano multidimensional como en la regresión lineal. La interpretación es bastante simple. La estructura de árbol también tiene una visualización natural, con sus nodos y bordes. Los árboles crean buenas explicaciones como se define en el capítulo sobre Explicaciones amigables para los humanos. La estructura de árbol invita automáticamente a pensar en los valores pronosticados para instancias individuales como contrafactuales: Si una característica hubiera sido mayor / menor que el punto de división, la predicción habría sido y1 en lugar de y2. Las explicaciones del árbol son contrastantes, ya que siempre se puede comparar la predicción de una instancia con escenarios relevantes qué pasaría si (tal como los define el árbol) que son simplemente los otros nodos de hoja del árbol. Si el árbol es corto (una o tres divisiones de profundidad) las explicaciones resultantes son selectivas. Un árbol con una profundidad de tres requiere un máximo de tres características y puntos divididos para crear la explicación para la predicción de una observación individual. La veracidad de la predicción depende del rendimiento predictivo del árbol. Las explicaciones para los árboles cortos son muy simples y generales, porque para cada división la instancia cae en una u otra hoja, y las decisiones binarias son fáciles de entender. No hay necesidad de transformar características. En modelos lineales, a veces es necesario tomar el logaritmo de una entidad. Un árbol de decisión funciona igualmente bien con cualquier transformación monotónica de una característica. 4.4.4 Desventajas Los árboles no pueden lidiar con relaciones lineales. Cualquier relación lineal entre una característica de entrada y el resultado debe ser aproximada por divisiones, creando una función de paso. Esto no es eficiente. Esto va de la mano con la falta de suavidad. Los cambios leves en la función de entrada pueden tener un gran impacto en el resultado previsto, lo que generalmente no es deseable. Imagina un árbol que predice el valor de una casa y el árbol usa el tamaño de la casa como una de las características divididas. La división ocurre en 100.5 metros cuadrados. Imagina al usuario de un estimador de precios de la vivienda utilizando su modelo de árbol de decisión: Miden su casa, llegan a la conclusión de que la casa tiene 99 metros cuadrados, la ingresan en la calculadora de precios y obtienen una predicción de 200 000 euros. Los usuarios notan que se han olvidado de medir un pequeño trastero con 2 metros cuadrados. El cuarto de almacenamiento tiene una pared inclinada, por lo que no están seguros si pueden contar toda el área o solo la mitad. Entonces deciden probar tanto 100.0 como 101.0 metros cuadrados. Los resultados: la calculadora de precios genera 200.000 euros y 205.000 euros, lo cual es bastante poco intuitivo, porque no ha habido un cambio de 99 metros cuadrados a 100, pero sí de 100 a 101. Los árboles también son bastante inestables. Algunos cambios en el conjunto de datos de entrenamiento pueden crear un árbol completamente diferente. Esto se debe a que cada división depende de la división principal. Y si se selecciona una característica diferente como la primera característica dividida, la estructura de árbol completa cambia. No crea confianza en el modelo si la estructura cambia tan fácilmente. Los árboles de decisión son muy interpretables, siempre que sean cortos. El número de nodos terminales aumenta rápidamente con la profundidad. Cuantos más nodos terminales y más profundo sea el árbol, más difícil será comprender las reglas de decisión de un árbol. Una profundidad de 1 significa 2 nodos terminales. Profundidad de 2 significa máx. 4 nodos. Profundidad de 3 significa máx. 8 nodos. El número máximo de nodos terminales en un árbol es 2 a la potencia de la profundidad. 4.4.5 Software Para los ejemplos de este capítulo, utilicé el paquete rpart R que implementa CART (árboles de clasificación y regresión). CART se implementa en muchos lenguajes de programación, incluido Python. Podría decirse que CART es un algoritmo bastante antiguo y algo anticuado y hay algunos algoritmos nuevos e interesantes para ajustar árboles. Puedes encontrar una descripción general de algunos paquetes R para árboles de decisión en la Vista de tareas CRAN Aprendizaje automático y aprendizaje estadístico bajo la palabra clave Recursive Partitioning. ## Warning in install.packages : ## installation of package &#39;../pkg/sbrl_1.2.tar.gz&#39; had non-zero exit status Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009). "],["reglas.html", "4.5 Reglas de decisión", " 4.5 Reglas de decisión Una regla de decisión es una simple declaración SI-ENTONCES (IF-THEN) que consiste en una condición (también llamada antecedente) y una predicción. Por ejemplo: SI llueve Y SI es abril (condición), ENTONCES lloverá mañana (predicción). Se puede usar una sola regla de decisión o una combinación de varias reglas para hacer predicciones. Las reglas de decisión siguen una estructura general: SI se cumplen las condiciones, ENTONCES haga una cierta predicción. Las reglas de decisión son probablemente los modelos de predicción más interpretables. Su estructura SI-ENTONCES se asemeja semánticamente al lenguaje natural y a la forma en que pensamos, siempre que la condición se construya a partir de características inteligibles, la duración de la condición sea corta (pequeño número de pares característica = valor combinados con un Y) y hay no demasiadas reglas. En programación, es muy natural escribir reglas SI-ENTONCES. Lo nuevo en el aprendizaje automático es que las reglas de decisión se aprenden a través de un algoritmo. Imagina usar un algoritmo para aprender las reglas de decisión para predecir el valor de una casa (bajo, medio o alto). Una regla de decisión aprendida por este modelo podría ser: Si una casa es más grande que 100 metros cuadrados y tiene un jardín, entonces su valor es alto. Más formalmente: SI tamaño&gt;100 Y jardín=1 ENTONCES valor = alto. Analicemos la regla de decisión: tamaño&gt;100 es la primera condición en la parte SI. jardín = 1 es la segunda condición en la parte SI. Las dos condiciones están conectadas con un Y para crear una nueva condición. Ambas deben ser ciertas para que se aplique la regla. El resultado previsto (ENTONCES) es valor = alto. Una regla de decisión utiliza al menos una declaración característica = valor en la condición, sin límite superior de cuántas más se pueden agregar con un AND. Una excepción es la regla predeterminada que no tiene una parte IF explícita y que se aplica cuando no se aplica ninguna otra regla, pero veremos más sobre esto más adelante. La utilidad de una regla de decisión generalmente se resume en dos números: Soporte y precisión. Soporte o cobertura de una regla: El porcentaje de instancias a las que se aplica la condición de una regla se denomina soporte. Tomemos, por ejemplo, la regla tamaño = grande Y ubicación = buena ENTONCES valor = alto para predecir los valores de la casa. Suponga que 100 de 1000 casas son grandes y están en una buena ubicación, entonces el respaldo de la regla es del 10%. La predicción (ENTONCES) no es importante para el cálculo del soporte. Precisión o confianza de una regla: La precisión de una regla es una medida de cuán precisa es la regla para predecir la clase correcta para las instancias a las que se aplica la condición de la regla. Por ejemplo: Digamos de las 100 casas, donde la regla tamaño = grande Y ubicación = buena ENTONCES valor = alto, 85 tienen valor = alto, 14 tienen valor = medio y 1 tiene valor = bajo, entonces la precisión de la regla es del 85%. Por lo general, existe una compensación entre precisión y soporte: Al agregar más funciones a la condición, podemos lograr una mayor precisión, pero perdemos soporte. Para crear un buen clasificador para predecir el valor de una casa, es posible que necesites aprender no solo una regla, sino tal vez 10 o 20. Entonces las cosas pueden complicarse y puedes encontrarte con uno de los siguientes problemas: Las reglas pueden superponerse: ¿Qué sucede si quiero predecir el valor de una casa y se aplican dos o más reglas y me dan predicciones contradictorias? No se aplica ninguna regla: ¿Qué sucede si quiero predecir el valor de una casa y no se aplica ninguna de las reglas? Hay dos estrategias principales para combinar varias reglas: Listas de decisiones (ordenadas) y conjuntos de decisiones (sin ordenar). Ambas estrategias implican diferentes soluciones al problema de la superposición de reglas. Una lista de decisiones introduce un orden en las reglas de decisión. Si la condición de la primera regla es verdadera para una instancia, usamos la predicción de la primera regla. Si no, pasamos a la siguiente regla y verificamos si corresponde y así sucesivamente. Las listas de decisiones resuelven el problema de la superposición de reglas al devolver solo la predicción de la primera regla de la lista que se aplica. Un conjunto de decisiones se asemeja a una democracia de las reglas, excepto que algunas reglas pueden tener un mayor poder de voto. En un conjunto, las reglas son mutuamente excluyentes o hay una estrategia para resolver conflictos, como la votación por mayoría, que puede ser ponderada por la precisión de las reglas individuales u otras medidas de calidad. La interpretabilidad sufre potencialmente cuando se aplican varias reglas. Tanto las listas de decisiones como los conjuntos pueden sufrir el problema de que ninguna regla se aplica a una instancia. Esto se puede resolver mediante la introducción de una regla predeterminada. La regla predeterminada es la regla que se aplica cuando no se aplica ninguna otra regla. La predicción de la regla predeterminada suele ser la clase más frecuente de los puntos de datos que no están cubiertos por otras reglas. Si un conjunto o una lista de reglas cubre todo el espacio de características, lo llamamos exhaustivo. Al agregar una regla predeterminada, un conjunto o lista se vuelve exhaustivo automáticamente. Hay muchas maneras de aprender reglas de los datos y este libro está lejos de abarcarlas todas. Este capítulo te muestra tres de ellos. Los algoritmos se eligen para cubrir una amplia gama de ideas generales para reglas de aprendizaje, por lo que los tres representan enfoques muy diferentes. OneR aprende las reglas de una sola característica. OneR se caracteriza por su simplicidad, interpretabilidad y su uso como punto de referencia. La cobertura secuencial es un procedimiento general que aprende de forma iterativa las reglas y elimina los puntos de datos cubiertos por la nueva regla. Este procedimiento es utilizado por muchos algoritmos de aprendizaje de reglas. Listas de reglas bayesianas combinan patrones frecuentes previamente minados en una lista de decisiones utilizando estadísticas bayesianas. El uso de patrones minados es un enfoque común utilizado por muchos algoritmos de aprendizaje de reglas. Comencemos con el enfoque más simple: usar la mejor característica para aprender reglas. 4.5.1 Aprender las reglas de una sola función (OneR) El algoritmo OneR sugerido por Holte (1993)18 es uno de los algoritmos de inducción de reglas más simples. De todas las características, OneR selecciona la que lleva más información sobre el resultado de interés y crea reglas de decisión a partir de esta característica. A pesar del nombre OneR, que significa Una regla, el algoritmo genera más de una regla: En realidad, es una regla por valor de característica única de la mejor característica seleccionada. Un mejor nombre sería OneFeatureRules. El algoritmo es simple y rápido: Discretiza las características continuas eligiendo los intervalos apropiados. Para cada característica: Crea una tabla cruzada entre los valores de la característica y el resultado (categórico). Para cada valor de la característica, crea una regla que prediga la clase más frecuente de las instancias que tienen este valor de característica particular (puede leerse en la tabla cruzada). Calcula el error total de las reglas para la función. Selecciona la función con el error total más pequeño. OneR siempre cubre todas las instancias del conjunto de datos, ya que utiliza todos los niveles de la función seleccionada. Los valores faltantes pueden tratarse como un valor de característica adicional o imputarse de antemano. Un modelo OneR es un árbol de decisión con una sola división. La división no es necesariamente binaria como en CART, sino que depende del número de valores de características únicas. Veamos un ejemplo de cómo OneR elige la mejor característica. La siguiente tabla muestra un conjunto de datos artificiales sobre casas con información sobre su valor, ubicación, tamaño y si se permiten mascotas. Estamos interesados en aprender un modelo simple para predecir el valor de una casa. ubicación tamaño mascotas valor bueno pequeño sí alto bueno grande no alto bueno grande no alto malo mediano no medio bueno mediano solo gatos medio bueno pequeño solo gatos medio malo mediano sí medio malo pequeño sí bajo malo mediano sí bajo malo pequeño no bajo OneR crea las tablas cruzadas entre cada característica y el resultado: valor=bajo valor=medio valor=alto ubicación=bueno 0 2 3 ubicación=malo 3 2 0 valor=bajo valor=medio valor=alto tamaño=grande 0 0 2 tamaño=mediano 1 3 0 tamaño=pequeño 2 1 1 valor=bajo valor=medio valor=alto mascotas=no 1 1 2 mascotas=sí 2 1 1 mascotas=solo gatos 0 2 0 Para cada característica, revisamos la tabla fila por fila: Cada valor de característica es la parte SI de una regla; La clase más común para las instancias con este valor de característica es la predicción, la parte ENTONCES de la regla. Por ejemplo, la función de tamaño con los niveles pequeño,mediano y grande da como resultado tres reglas. Para cada característica calculamos la tasa de error total de las reglas generadas, que es la suma de los errores. La función de ubicación tiene los valores posibles malo ybueno. El valor más frecuente para las casas en ubicaciones malas es bajo y cuando usamosbajo como predicción, cometemos dos errores, porque dos casas tienen un valor medio. El valor predicho de las casas en buenas ubicaciones es alto y nuevamente cometemos dos errores, porque dos casas tienen un valormedio. El error que cometemos al usar la función de ubicación es 4/10, para la función de tamaño es 3/10 y para la función de mascota es 4/10. La función de tamaño produce las reglas con el error más bajo y se utilizará para el modelo final de OneR: SI tamaño = chico ENTONCES valor = bajo SI tamaño = medio ENTONCES value = medio SItamaño = grandeENTONCESvalue = grande` OneR prefiere características con muchos niveles posibles, porque esas características pueden sobreajustar el objetivo más fácilmente. Imagina un conjunto de datos que contiene solo ruido y ninguna señal, lo que significa que todas las características toman valores aleatorios y no tienen un valor predictivo para el objetivo. Algunas características tienen más niveles que otras. Las características con más niveles ahora pueden adaptarse más fácilmente. Una característica que tiene un nivel separado para cada instancia de los datos predeciría perfectamente todo el conjunto de datos de entrenamiento. Una solución sería dividir los datos en conjuntos de entrenamiento y validación, aprender las reglas sobre los datos de entrenamiento y evaluar el error total para elegir la función en el conjunto de validación. Los lazos son otro problema, es decir, cuando dos características dan como resultado el mismo error total. OneR resuelve los lazos al tomar la primera característica con el error más bajo o la que tiene el valor p más bajo de una prueba de chi-cuadrado. Ejemplo Probemos OneR con datos reales. Utilizamos la tarea de clasificación de cáncer cervical para probar el algoritmo OneR. Todas las características de entrada continua se discretizaron en sus 5 cuantiles. Se crean las siguientes reglas: Age prediction (12.9,27.2] Healthy (27.2,41.4] Healthy (41.4,55.6] Healthy (55.6,69.8] Healthy (69.8,84.1] Healthy OneR eligió la función de edad como la mejor función predictiva. Dado que el cáncer es raro, para cada regla la clase mayoritaria y, por lo tanto, la etiqueta predicha es siempre Saludable, lo cual es bastante inútil. No tiene sentido usar la predicción de etiqueta en este caso desequilibrado. La tabla cruzada entre los intervalos de Edad y Cáncer/Saludable junto con el porcentaje de mujeres con cáncer es más informativa: # Cancer # Healthy P(Cancer) Age=(12.9,27.2] 26 477 0.05 Age=(27.2,41.4] 25 290 0.08 Age=(41.4,55.6] 4 31 0.11 Age=(55.6,69.8] 0 1 0.00 Age=(69.8,84.1] 0 4 0.00 Pero antes de comenzar a interpretar: Dado que la predicción para cada característica y cada valor es Saludable, la tasa de error total es la misma para todas las características. Los vínculos en el error total se resuelven, de manera predeterminada, utilizando la primera función de las que tienen las tasas de error más bajas (aquí, todas las funciones tienen 55/858), que resulta ser la característica Edad. OneR no admite tareas de regresión. Pero podemos convertir una tarea de regresión en una tarea de clasificación cortando el resultado continuo en intervalos. Utilizamos este truco para predecir el número de bicicletas alquiladas con OneR cortando el número de bicicletas en sus cuatro cuartiles (0-25%, 25-50%, 50-75% y 75-100%) La siguiente tabla muestra la función seleccionada después de ajustar el modelo OneR: mnth prediction JAN [22,3152] FEB [22,3152] MAR [22,3152] APR (3152,4548] MAY (5956,8714] JUN (4548,5956] JUL (5956,8714] AUG (5956,8714] SEP (5956,8714] OKT (5956,8714] NOV (3152,4548] DEZ [22,3152] La función seleccionada es el mes. La función de mes tiene (¡sorpresa!) 12 niveles de funciones, que es más que la mayoría de las otras funciones. Por lo tanto, existe el peligro de sobreajustar. En el lado más optimista: la función del mes puede manejar la tendencia estacional (por ejemplo, bicicletas menos alquiladas en invierno) y las predicciones parecen ser sensatas. Ahora pasamos del simple algoritmo OneR a un procedimiento más complejo usando reglas con condiciones más complejas que consisten en varias características: Cobertura secuencial. 4.5.2 Cobertura secuencial La cobertura secuencial es un procedimiento general que aprende repetidamente una sola regla para crear una lista de decisiones (o conjunto) que cubre todo el conjunto de datos regla por regla. Muchos algoritmos de aprendizaje de reglas son variantes del algoritmo de cobertura secuencial. Este capítulo presenta la receta principal y utiliza RIPPER, una variante del algoritmo de cobertura secuencial para los ejemplos. La idea es simple: Primero, encuentra una buena regla que se aplique a algunos de los puntos de datos. Eliminatodos los puntos de datos cubiertos por la regla. Se cubre un punto de datos cuando se aplican las condiciones, independientemente de si los puntos se clasifican correctamente o no. Repite el aprendizaje de reglas y la eliminación de los puntos cubiertos con los puntos restantes hasta que no queden más puntos o se cumpla otra condición de detención. El resultado es una lista de decisiones. Este enfoque de aprendizaje de reglas repetido y eliminación de puntos de datos cubiertos se denomina dividir y conquistar. Supongamos que ya tenemos un algoritmo que puede crear una sola regla que cubra parte de los datos. El algoritmo de cobertura secuencial para dos clases (una positiva, una negativa) funciona así: Comienza con una lista vacía de reglas (rlist). Aprende una regla r. Si bien la lista de reglas está por debajo de cierto umbral de calidad (o los ejemplos positivos aún no están cubiertos): Agrega la regla r a rlist. Elimina todos los puntos de datos cubiertos por la regla r. Aprende otra regla sobre los datos restantes. Devuelve la lista de decisiones. FIGURA 4.18: El algoritmo de cobertura funciona cubriendo secuencialmente el espacio de características con reglas individuales y eliminando los puntos de datos que ya están cubiertos por esas reglas. Para fines de visualización, las características x1 y x2 son continuas, pero la mayoría de los algoritmos de aprendizaje de reglas requieren caracteristicas categóricas. Por ejemplo: Tenemos una tarea y un conjunto de datos para predecir los valores de las casas por tamaño, ubicación y si se permiten mascotas. Aprendemos la primera regla, que resulta ser: Si tamaño = grande y ubicación = buena, entonces valor = alto. Luego eliminamos todas las casas grandes en buenas ubicaciones del conjunto de datos. Con los datos restantes aprendemos la siguiente regla. Quizás: si ubicación = buena, entonces valor = medio. Ten en cuenta que esta regla se aprende en datos sin grandes casas en buenas ubicaciones, dejando solo casas medianas y pequeñas en buenas ubicaciones. Para configuraciones de varias clases, el enfoque debe modificarse. Primero, las clases se ordenan aumentando la prevalencia. El algoritmo de cobertura secuencial comienza con la clase menos común, aprende una regla para ello, elimina todas las instancias cubiertas, luego pasa a la segunda clase menos común y así sucesivamente. La clase actual siempre se trata como la clase positiva y todas las clases con una prevalencia más alta se combinan en la clase negativa. La última clase es la regla predeterminada. Esto también se conoce como estrategia de uno contra todos en la clasificación. ¿Cómo aprendemos una sola regla? El algoritmo OneR sería inútil aquí, ya que siempre cubriría todo el espacio de características. Pero hay muchas otras posibilidades. Una posibilidad es aprender una sola regla de un árbol de decisión con beam search (búsqueda de haz): Aprende un árbol de decisión (con CART u otro algoritmo de aprendizaje de árbol). Comienza en el nodo raíz y selecciona recursivamente el nodo más puro (por ejemplo, con la tasa de clasificación errónea más baja). La clase mayoritaria del nodo terminal se usa como la predicción de la regla; la ruta que conduce a ese nodo se usa como condición de regla. La siguiente figura ilustra la búsqueda del haz en un árbol: FIGURA 4.19: Aprender una regla buscando una ruta a través de un árbol de decisión. Comienza en el nodo raíz, con avidez e iteración sigue la ruta que produce localmente el subconjunto más puro (por ejemplo, la precisión más alta) y agrega todos los valores divididos a la condición de la regla. Termina con: Si ubicación = buena y tamaño = grande, entonces valor = alto. Aprender una sola regla es un problema de búsqueda, donde el espacio de búsqueda es el espacio de todas las reglas posibles. El objetivo de la búsqueda es encontrar la mejor regla de acuerdo con algunos criterios. Existen muchas estrategias de búsqueda diferentes: hill-climbing, beam search, exhaustive search, búsqueda de primer orden, búsqueda ordenada, búsqueda estocástica, búsqueda de arriba hacia abajo, búsqueda de abajo hacia arriba,  RIPPER (poda incremental repetida para producir reducción de errores) por Cohen (1995)19 es una variante del algoritmo de cobertura secuencial. RIPPER es un poco más sofisticado y utiliza una fase de posprocesamiento (poda de reglas) para optimizar la lista de decisiones (o conjunto). RIPPER puede ejecutarse en modo ordenado o no ordenado y generar una lista de decisiones o un conjunto de decisiones. Ejemplos Usaremos RIPPER para los ejemplos. El algoritmo RIPPER no encuentra ninguna regla en la tarea de clasificación para cáncer cervical. Cuando usamos RIPPER en la tarea de regresión para predecir recuento de bicicletas se encuentran algunas reglas. Dado que RIPPER solo funciona para la clasificación, el conteo de bicicletas debe convertirse en un resultado categórico. Lo logré cortando los recuentos de bicicletas en los cuartiles. Por ejemplo (4548, 5956) es el intervalo que cubre el recuento previsto de bicicletas entre 4548 y 5956. La siguiente tabla muestra la lista de decisiones de las reglas aprendidas. rules (days_since_2011 &gt;= 438) and (temp &gt;= 17) and (temp &lt;= 27) and (hum &lt;= 67) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 443) and (temp &gt;= 12) and (weathersit = GOOD) and (hum &gt;= 59) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 441) and (windspeed &lt;= 10) and (temp &gt;= 13) =&gt; cnt=(5956,8714] (temp &gt;= 12) and (hum &lt;= 68) and (days_since_2011 &gt;= 551) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 100) and (days_since_2011 &lt;= 434) and (hum &lt;= 72) and (workingday = WORKING DAY) =&gt; cnt=(3152,4548] (days_since_2011 &gt;= 106) and (days_since_2011 &lt;= 323) =&gt; cnt=(3152,4548] =&gt; cnt=[22,3152] La interpretación es simple: Si se aplican las condiciones, predecimos el intervalo en el lado derecho para el número de bicicletas. La última regla es la regla predeterminada que se aplica cuando ninguna de las otras reglas se aplica a una instancia. Para predecir una nueva instancia, comienza en la parte superior de la lista y verifica si se aplica una regla. Cuando una condición coincide, el lado derecho de la regla es la predicción para esta instancia. La regla predeterminada asegura que siempre haya una predicción. 4.5.3 Listas de reglas bayesianas En esta sección, se mostrará otro enfoque para aprender una lista de decisiones, que sigue esta receta aproximada: Pre-mina los patrones frecuentes de los datos que pueden usarse como condiciones para las reglas de decisión. Aprende una lista de decisiones de una selección de las reglas previamente minadas. Un enfoque específico que utiliza esta receta se llama Listas de Reglas Bayesianas (Letham et. Al., 2015)20 o BRL para abreviar. BRL utiliza estadísticas bayesianas para aprender listas de decisiones de patrones frecuentes que se extraen previamente con el algoritmo FP-tree (Borgelt 2005)21 Pero comencemos lentamente con el primer paso de BRL. Pre-minería de patrones frecuentes Un patrón frecuente es la aparición frecuente y conjunta de valores de características. Como un paso de preprocesamiento para el algoritmo BRL, utilizamos las características (no necesitamos el resultado objetivo en este paso) y extraemos patrones que ocurren con frecuencia de ellos. Un patrón puede ser un valor de entidad único como peso = medio o una combinación de valores de entidad como peso = medio Y ubicación = mala. La frecuencia de un patrón se mide con su soporte en el conjunto de datos: \\[Support(x_j=A)=\\frac{1}n{}\\sum_{i=1}^nI(x^{(i)}_{j}=A)\\] donde A es el valor de la característica, n el número de puntos de datos en el conjunto de datos e I la función de identidad que devuelve 1 si la función \\(x_j\\) de la instancia i tiene el nivel A, de lo contrario 0. En un conjunto de datos de valores de casas, si el 20% de las casas no tiene balcón y el 80% tiene uno o más, entonces el soporte para el patrón balcón = 0 es del 20%. El soporte también se puede medir para combinaciones de valores de características, por ejemplo para balcón = 0 Y mascotas = permitido. Existen muchos algoritmos para encontrar patrones tan frecuentes, por ejemplo, Apriori o FP-Growth. Lo que usa no importa mucho, solo la velocidad a la que se encuentran los patrones es diferente, pero los patrones resultantes son siempre los mismos. Daré una idea aproximada de cómo funciona el algoritmo Apriori para encontrar patrones frecuentes. En realidad, el algoritmo Apriori consta de dos partes, donde la primera parte encuentra patrones frecuentes y la segunda parte crea reglas de asociación a partir de ellos. Para el algoritmo BRL, solo estamos interesados en los patrones frecuentes que se generan en la primera parte de Apriori. En el primer paso, el algoritmo Apriori comienza con todos los valores de características que tienen un soporte mayor que el soporte mínimo definido por el usuario. Si el usuario dice que el soporte mínimo debe ser del 10% y solo el 5% de las casas tienen peso = grande, eliminaríamos ese valor de la característica y mantendríamos solo peso = medio y peso = chico como patrones. Esto no significa que las casas se eliminen de los datos, solo significa que peso = grande no se devuelve como un patrón frecuente. Basado en patrones frecuentes con un solo valor de característica, el algoritmo Apriori intenta iterativamente encontrar combinaciones de valores de característica de orden cada vez más alto. Los patrones se construyen combinando declaraciones característica = valor con un Y lógico, por ejemplo peso = medio Y ubicación = mala. Se eliminan los patrones generados con un soporte por debajo del soporte mínimo. Al final tenemos todos los patrones frecuentes. Cualquier subconjunto de un patrón frecuente es frecuente nuevamente, lo que se llama la propiedad Apriori. Tiene sentido intuitivamente: Al eliminar una condición de un patrón, el patrón reducido solo puede cubrir más o la misma cantidad de puntos de datos, pero no menos. Por ejemplo, si el 20% de las casas son peso = medio Y ubicación = buena, entonces el soporte de las casas que son solo peso = medio es 20% o más. La propiedad Apriori se usa para reducir el número de patrones que se inspeccionarán. Solo en el caso de patrones frecuentes tenemos que verificar patrones de orden superior. Ahora hemos terminado con las condiciones previas a la minería para las Listas de Reglas Bayesianas. Pero antes de pasar al segundo paso de BRL, me gustaría insinuar otra forma para el aprendizaje de reglas basado en patrones pre-minados. Otros enfoques sugieren incluir el resultado de interés en el proceso frecuente de minería de patrones y también ejecutar la segunda parte del algoritmo Apriori que construye las reglas SI-ENTONCES. Dado que el algoritmo no está supervisado, la parte ENTONCES también contiene valores de características que no nos interesan. Pero podemos filtrar por reglas que solo tienen el resultado de interés en la parte ENTONCES. Estas reglas ya forman un conjunto de decisiones, pero también sería posible organizar, podar, eliminar o recombinar las reglas. 4.5.4 Ventajas Esta sección discute los beneficios de las reglas SI-ENTONCES en general. Las reglas SI-ENTONCES son fáciles de interpretar. Son probablemente el más interpretable de los modelos interpretables. Esta declaración solo se aplica si el número de reglas es pequeño, las condiciones de las reglas son cortas (máximo 3, diría yo) y si las reglas están organizadas en una lista de decisiones o en un conjunto de decisiones que no se superponen. Las reglas de decisión pueden ser tan expresivas como los árboles de decisión, mientras que son más compactas. Los árboles de decisión a menudo también sufren de subárboles replicados, es decir, cuando las divisiones en un nodo secundario izquierdo y derecho tienen la misma estructura. La predicción con las reglas SI-ENTONCES es rápida, ya que solo se necesitan verificar unas pocas declaraciones binarias para determinar qué reglas se aplican. Las reglas de decisión son robustas contra las transformaciones monótonas de las características de entrada, sólo cambiará el umbral de las condiciones. También son robustas frente a los valores atípicos, ya que solo importa si una condición se aplica o no. Las reglas SI-ENTONCES generalmente generan modelos dispersos, lo que significa que no se incluyen muchas características. Seleccionan solo las características relevantes para el modelo. Por ejemplo, un modelo lineal asigna un peso a cada característica de entrada de forma predeterminada. Las características que son irrelevantes pueden simplemente ser ignoradas por las reglas SI-ENTONCES. Reglas simples como las de OneR pueden usarse como línea de base para algoritmos más complejos. 4.5.5 Desventajas Esta sección trata las desventajas de las reglas SI-ENTONCES en general. La investigación y la literatura para las reglas SI-ENTONCES se enfoca en la clasificación y casi descuida completamente la regresión. Si bien siempre puede dividir un objetivo continuo en intervalos y convertirlo en un problema de clasificación, siempre pierde información. En general, los enfoques son más atractivos si pueden usarse tanto para la regresión como para la clasificación. A menudo, las características también tienen que ser categóricas. Eso significa que las características numéricas deben clasificarse si desea usarlas. Hay muchas formas de cortar una característica continua en intervalos, pero esto no es trivial y viene con muchas preguntas sin respuestas claras. ¿En cuántos intervalos debe dividirse la función? ¿Cuál es el criterio de división: longitudes de intervalo fijas, cuantiles u otra cosa? La categorización de características continuas es un problema no trivial que a menudo se descuida y las personas simplemente usan el siguiente mejor método (como lo hice en los ejemplos). Muchos de los algoritmos de aprendizaje de reglas más antiguos son propensos al sobreajuste. Todos los algoritmos presentados aquí tienen al menos algunas garantías para evitar el sobreajuste: OneR es limitado porque solo puede usar una característica (solo problemático si la característica tiene demasiados niveles o si hay muchas características, lo que equivale al problema de prueba múltiple), RIPPER hace podas y las listas de reglas bayesianas imponen una distribución previa de la lista de decisión. Las reglas de decisión son malas al describir relaciones lineales entre características y resultados. Ese es un problema que comparten con los árboles de decisión. Los árboles de decisión y las reglas solo pueden producir funciones de predicción escalonadas, donde los cambios en la predicción siempre son pasos discretos y nunca curvas suaves. Esto está relacionado con el problema de que las entradas tienen que ser categóricas. En los árboles de decisión, se clasifican implícitamente dividiéndolos. 4.5.6 Software y alternativas OneR se implementa en el paquete R OneR, que se utilizó para los ejemplos en este libro. OneR también se implementa en la librería de aprendizaje automático Weka y, como tal, disponible en Java, R y Python. RIPPER también se implementa en Weka. Para los ejemplos, utilicé la implementación R de JRIP en el paquete RWeka. SBRL está disponible como paquete R (que utilicé para los ejemplos), en Python o como implementación C. Ni siquiera intentaré enumerar todas las alternativas para aprender conjuntos de reglas de decisión y listas, sino que señalaré algunos trabajos de resumen. Recomiendo el libro Fundamentos del aprendizaje de reglas de Fuernkranz et. al (2012)22. Es un trabajo extenso sobre reglas de aprendizaje, para aquellos que desean profundizar en el tema. Proporciona un marco holístico para pensar sobre las reglas de aprendizaje y presenta muchos algoritmos de aprendizaje de reglas. También recomiendo revisar los Estudiantes de reglas de Weka, que implementan RIPPER, M5Rules, OneR, PART y muchos más. Las reglas SI-ENTONCES pueden usarse en modelos lineales como se describe en este libro en el capítulo sobre el algoritmo RuleFit. Holte, Robert C. Very simple classification rules perform well on most commonly used datasets. Machine learning 11.1 (1993): 63-90. Cohen, William W. Fast effective rule induction. Machine Learning Proceedings (1995). 115-123. Letham, Benjamin, et al. Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics 9.3 (2015): 1350-1371. Borgelt, C. An implementation of the FP-growth algorithm. Proceedings of the 1st International Workshop on Open Source Data Mining Frequent Pattern Mining Implementations - OSDM 05, 15. http://doi.org/10.1145/1133905.1133907 (2005). Fürnkranz, Johannes, Dragan Gamberger, and Nada Lavra. Foundations of rule learning. Springer Science &amp; Business Media, (2012). "],["rulefit.html", "4.6 RuleFit", " 4.6 RuleFit El algoritmo RuleFit de Friedman y Popescu (2008)23 aprende modelos lineales dispersos que incluyen efectos de interacción detectados automáticamente en forma de reglas de decisión. El modelo de regresión lineal no tiene en cuenta las interacciones entre las características. ¿No sería conveniente tener un modelo que sea tan simple e interpretable como los modelos lineales, pero que también integre interacciones de características? RuleFit llena este vacío. RuleFit aprende un modelo lineal disperso con las características originales y también una serie de características nuevas que son reglas de decisión. Estas nuevas características capturan interacciones entre las características originales. RuleFit genera automáticamente estas características a partir de los árboles de decisión. Cada ruta a través de un árbol se puede transformar en una regla de decisión combinando las decisiones divididas en una regla. Las predicciones de nodo se descartan y solo las divisiones se utilizan en las reglas de decisión: FIGURA 4.20: Se pueden generar 4 reglas desde un árbol con 3 nodos terminales. ¿De dónde vienen esos árboles de decisión? Los árboles están entrenados para predecir el resultado de interés. Esto asegura que las divisiones sean significativas para la tarea de predicción. Cualquier algoritmo que genere muchos árboles se puede usar para RuleFit, por ejemplo, un random forest. Cada árbol se descompone en reglas de decisión que se utilizan como características adicionales en un modelo de regresión lineal disperso (Lasso). El artículo RuleFit utiliza los datos de vivienda de Boston para ilustrar esto: El objetivo es predecir el valor medio de la casa de un vecindario de Boston. Una de las reglas generadas por RuleFit es: SI número de habitaciones &gt; 6.64 Y concentración de óxido nítrico &lt; 0.67 ENTONCES 1 SI NO 0. RuleFit también viene con una medida de importancia de características que ayuda a identificar términos lineales y reglas que son importantes para las predicciones. La importancia de la característica se calcula a partir de los pesos del modelo de regresión. La medida de importancia se puede agregar para las características originales (que se utilizan en su forma bruta y posiblemente en muchas reglas de decisión). RuleFit también presenta gráficos de dependencia parcial para mostrar el cambio promedio en la predicción al cambiar una característica. El diagrama de dependencia parcial es un método independiente del modelo que se puede usar con cualquier modelo, y se explica en el capítulo del libro sobre diagramas de dependencia parcial. 4.6.1 Interpretación y ejemplo Dado que RuleFit estima un modelo lineal al final, la interpretación es la misma que para modelos lineales normales. La única diferencia es que el modelo tiene nuevas características derivadas de las reglas de decisión. Las reglas de decisión son características binarias: Un valor de 1 significa que se cumplen todas las condiciones de la regla; de lo contrario, el valor es 0. Para términos lineales en RuleFit, la interpretación es la misma que en los modelos de regresión lineal: Si la característica aumenta en una unidad, el resultado previsto cambia según el peso de la característica correspondiente. En este ejemplo, usamos RuleFit para predecir el número de bicicletas alquiladas en un día determinado. La tabla muestra cinco de las reglas generadas por RuleFit, junto con sus pesos e importancias en Lasso. El cálculo se explica más adelante en el capítulo. Descripción Peso Importancia days_since_2011 &gt; 111 &amp; weathersit in (GOOD, MISTY) 793 303 37.25 &lt;= hum &lt;= 90 -20 272 temp &gt; 13 &amp; days_since_2011 &gt; 554 676 239 4 &lt;= windspeed &lt;= 24 -41 202 days_since_2011 &gt; 428 &amp; temp &gt; 5 366 179 La regla más importante era: days_since_2011 &gt; 111 &amp; weathersit in (GOOD,MISTY) y el peso correspondiente es 793. La interpretación es: Si days_since_2011 &gt; 111 &amp; weathersit in (GOOD, MISTY), entonces el número predicho de bicicletas aumenta en 793, cuando todos los demás valores de características permanecen fijos. En total, 278 reglas se crearon a partir de las 8 características originales. ¡Bastante! Pero gracias a Lasso, solo 58 de esas 278 tienen un peso diferente de 0. Calcular la importancia de las características globales revela que la temperatura y la tendencia temporal son las características más importantes: FIGURA 4.21: Importancia de características en un modelo RuleFit que predice cantidad de bicicletas. Las más importantes son la temperatura y la tendencia temporal. La medición de la importancia de la característica incluye la importancia del término de característica sin procesar y todas las reglas de decisión en las que aparece la característica. Plantilla de interpretación La interpretación es análoga a los modelos lineales: El resultado previsto cambia en \\(\\beta_j\\) si la función \\(x_j\\) cambia en una unidad, siempre que todas las demás funciones permanezcan sin cambios. La interpretación del peso de una regla de decisión es un caso especial: Si se aplican todas las condiciones de una regla de decisión \\(r_k\\), el resultado previsto cambia en \\(\\alpha_k\\) (el peso aprendido de la regla \\(r_k\\) en el modelo lineal). Para la clasificación (usando regresión logística en lugar de regresión lineal): Si se aplican todas las condiciones de la regla de decisión \\(r_k\\), las probabilidades de evento frente a no evento cambian por un factor de \\(\\alpha_k\\). 4.6.2 Teoría Profundicemos en los detalles técnicos del algoritmo RuleFit. RuleFit consta de dos componentes: El primer componente crea reglas a partir de árboles de decisión y el segundo componente ajusta un modelo lineal con las características originales y las nuevas reglas como entrada (de ahí el nombre RuleFit). Paso 1: generación de reglas ¿Cómo se ve una regla? Las reglas generadas por el algoritmo tienen una forma simple. Por ejemplo: SI x2 &lt; 3 Yx5 &lt; 7 LUEGO 1 SI NO 0. Las reglas se construyen descomponiendo los árboles de decisión: Cualquier ruta a un nodo en un árbol se puede convertir en una regla de decisión. Los árboles utilizados para las reglas se ajustan para predecir el resultado objetivo. Por lo tanto, las divisiones y las reglas resultantes están optimizadas para predecir el resultado de interés. Simplemente encadena las decisiones binarias que conducen a un determinado nodo con Y, y listo, hay una regla. Es deseable generar muchas reglas diversas y significativas. El aumento de gradiente se utiliza para ajustar un conjunto de árboles de decisión al retroceder o clasificar y con sus características originales X. Cada árbol resultante se convierte en múltiples reglas. Se puede usar cualquier algoritmo de árboles para generar los de RuleFit. Un conjunto de árboles se puede describir con esta fórmula general: \\[f(x)=a_0+\\sum_{m=1}^M{}a_m{}f_m(X)\\] M es el número de árboles y \\(f_m(x)\\) es la función de predicción del m-ésimo árbol. Los \\(\\alpha\\) son los pesos. Bagged ensembles, Random forest, AdaBoost y MART producen conjuntos de árboles y se pueden usar para RuleFit. Creamos las reglas de todos los árboles del conjunto. Cada regla \\(r_m\\) toma la forma de: \\[r_m(x)=\\prod_{j\\in\\text{T}_m}I(x_j\\in{}s_{jm})\\] donde \\(\\text{T}_{m}\\) es el conjunto de características utilizadas en el m-ésimo árbol, I es la función identitaria que es 1 cuando la característica \\(x_j\\) está en el subconjunto especificado de valores s para la característica j (según lo especificado por las divisiones de árbol) y 0 de lo contrario. Para funciones numéricas, \\(s_{jm}\\) es un intervalo en el rango de valores de la función. El intervalo se parece a uno de los dos casos: \\[x_{s_{jm},\\text{mínimo}}&lt;x_j\\] \\[x_j&lt;x_{s_{jm},upper}\\] Las divisiones adicionales en esa característica posiblemente conduzcan a intervalos más complicados. Para las características categóricas, el subconjunto s contiene algunas categorías específicas de la característica. Un ejemplo inventado para el conjunto de datos de alquiler de bicicletas: \\[r_{17}(x)=I(x_{\\text{temp}}&lt;15)\\cdot{}I(x_{\\text{weather}}\\in\\{\\text{good},\\text{cloudy}\\})\\cdot{}I(10\\leq{}x_{\\text{windspeed}}&lt;20)\\] Esta regla devuelve 1 si se cumplen las tres condiciones; de lo contrario, 0. RuleFit extrae todas las reglas posibles de un árbol, no solo de los nodos hoja. Entonces, otra regla que se crearía es: \\[r_{18}(x)=I(x_{\\text{temp}}&lt;15)\\cdot{}I(x_{\\text{weather}}\\in\\{\\text{good},\\text{cloudy}\\}\\] En total, el número de reglas creadas a partir de un conjunto de árboles M con nodos terminales \\(t_m\\) cada uno es: \\[K=\\sum_{m=1}^M2(t_m-1)\\] Un truco introducido por los autores de RuleFit es entrenar árboles con profundidad aleatoria para que se generen muchas reglas diversas con diferentes longitudes. Ten en cuenta que descartamos el valor predicho en cada nodo: solo mantenemos las condiciones que nos llevan a un nodo y luego creamos una regla a partir de él. La ponderación de las reglas de decisión se realiza en el paso 2 de RuleFit. Otra forma de ver el paso 1: RuleFit genera un nuevo conjunto de características a partir de sus características originales. Estas características son binarias y pueden representar interacciones bastante complejas de sus características originales. Las reglas se eligen para maximizar la tarea de predicción. Las reglas se generan automáticamente a partir de la matriz de covariables X. Simplemente puede ver las reglas como nuevas características basadas en sus características originales. Paso 2: modelo lineal disperso Obtienes MUCHAS reglas en el paso 1. Dado que el primer paso puede verse solo como una transformación de características, aún no has terminado de ajustar un modelo. Además, deseas reducir el número de reglas. Además de las reglas, todas tus características brutas de su conjunto de datos original también se utilizarán en el modelo lineal disperso. Cada regla y cada característica original se convierte en una característica en el modelo lineal, y por lo tanto obtiene una estimación de peso. Las características originales sin procesar se agregan porque los árboles no pueden representar relaciones lineales simples entre x e y. Antes de entrenar un modelo lineal disperso, clasificamos las características originales para que sean más robustas frente a los valores atípicos: \\[l_j^*(x_j)=min(\\delta_j^+,max(\\delta_j^-,x_j))\\] donde \\(\\delta_j^-\\) y \\(\\delta_j^+\\) son los cuantiles \\(\\delta\\) de la distribución de datos de la característica \\(x_j\\). Una opción de \\(\\delta\\) = 0.05 significa que cualquier valor de la característica \\(x_j\\) que esté en los valores 5% más bajos o 5% más altos se establecerá en los cuantiles en 5% o 95% respectivamente. Como regla general, puedes elegir \\(\\delta\\) = 0.025. Además, los términos lineales deben normalizarse para que tengan la misma importancia previa que una regla de decisión típica: \\[l_j(x_j)=0.4\\cdot{}l^*_j(x_j)/std(l^*_j(x_j))\\] \\(0.4\\) es la desviación estándar promedio de las reglas con una distribución uniforme de \\(s_k\\sim{}U(0,1)\\). Combinamos ambos tipos de características para generar una nueva matriz de características y entrenar un modelo lineal disperso con Lasso, con la siguiente estructura: \\[\\hat{f}(x)=\\hat{\\beta}_0+\\sum_{k=1}^K\\hat{\\alpha}_k{}r_k(x)+\\sum_{j=1}^p\\hat{\\beta}_j{}l_j(x_j)\\] donde \\(\\hat{\\alpha}\\) es el vector de peso estimado para las características de la regla y \\(\\hat{\\beta}\\) el vector de peso para las características originales. Dado que RuleFit usa Lasso, la función de pérdida obtiene la restricción adicional que obliga a algunos de los pesos a obtener una estimación cero: \\[(\\{\\hat{\\alpha}\\}_1^K,\\{\\hat{\\beta}\\}_0^p)=argmin_{\\{\\hat{\\alpha}\\}_1^K,\\{\\hat{\\beta}\\}_0^p}\\sum_{i=1}^n{}L(y^{(i)},f(x^{(i)}))+\\lambda\\cdot\\left(\\sum_{k=1}^K|\\alpha_k|+\\sum_{j=1}^p|b_j|\\right)\\] El resultado es un modelo lineal que tiene efectos lineales para todas las características originales y para las reglas. La interpretación es la misma que para los modelos lineales, la única diferencia es que algunas características son ahora reglas binarias. Paso 3 (opcional): importancia de la característica Para los términos lineales de las características originales, la importancia de la característica se mide con el predictor estandarizado: \\[I_j=|\\hat{\\beta}_j|\\cdot{}std(l_j(x_j))\\] donde \\(\\beta_j\\) es el peso del modelo con Lasso, y \\(std(l_j(x_j))\\) es la desviación estándar del término lineal sobre los datos. Para los términos de la regla de decisión, la importancia se calcula con la siguiente fórmula: \\[I_k=|\\hat{\\alpha}_k|\\cdot\\sqrt{s_k(1-s_k)}\\] donde \\(\\hat{\\alpha}_k\\) es el peso de Lasso asociado de la regla de decisión y \\(s_k\\) es el soporte de la característica en los datos, que es el porcentaje de puntos de datos a los que se aplica la regla de decisión (donde \\(r_k(x)=0\\)): \\[s_k=\\frac{1}{n}\\sum_{i=1}^n{}r_k(x^{(i)})\\] Una característica ocurre como un término lineal y posiblemente también dentro de muchas reglas de decisión. ¿Cómo medimos la importancia total de una característica? La importancia \\(J_j(x)\\) de una característica se puede medir para cada predicción individual: \\[J_j(x)=I_l(x)+\\sum_{x_j\\in{}r_k}I_k(x)/m_k\\] donde \\(I_l\\) es la importancia del término lineal y \\(I_k\\) la importancia de las reglas de decisión en las que aparece \\(x_j\\), y \\(m_k\\) es el número de características que constituyen la regla \\(r_k\\). Agregar la importancia de la característica de todas las instancias nos da la importancia de la característica global: \\[J_j(X)=\\sum_{i=1}^n{}J_j(x^{(i)})\\] Es posible seleccionar un subconjunto de instancias y calcular la importancia de las características para este grupo. 4.6.3 Ventajas RuleFit agrega automáticamente interacciones de características a modelos lineales. Por lo tanto, resuelve el problema de los modelos lineales en los que debe agregar términos de interacción manualmente y ayuda un poco con el tema del modelado de relaciones no lineales. RuleFit puede manejar tareas de clasificación y regresión. Las reglas creadas son fáciles de interpretar, porque son reglas de decisión binarias. La regla se aplica a una instancia o no. La buena interpretación solo se garantiza si el número de condiciones dentro de una regla no es demasiado grande. Una regla con 1 a 3 condiciones me parece razonable. Esto significa una profundidad máxima de 3 para los árboles en el conjunto de árboles. Incluso si hay muchas reglas en el modelo, no se aplican a todas las instancias. Para una instancia individual, solo se aplican un puñado de reglas (= tienen pesos distintos de cero). Esto mejora la interpretabilidad local. RuleFit propone un montón de herramientas de diagnóstico útiles. Estas herramientas son independientes del modelo, por lo que puedes encontrarlas en la sección modelo-agnóstica: importancia de la característica, gráficos de dependencia parcial e interacciones de la característica 4.6.4 Desventajas A veces, RuleFit crea muchas reglas que obtienen un peso distinto de cero en el modelo Lasso. La interpretabilidad se degrada con el número creciente de características en el modelo. Una solución prometedora es forzar que los efectos de las características sean monótonos, lo que significa que un aumento de una característica tiene que conducir a un aumento de la predicción. Una anécdota personal: los artículos afirman un buen rendimiento de RuleFit, ¡a menudo cercano al rendimiento predictivo de random forests! pero en los pocos casos en que lo intenté, el rendimiento fue decepcionante. Simplemente pruébalo para tu problema y ve cómo funciona. El producto final del procedimiento RuleFit es un modelo lineal con características sofisticadas adicionales (las reglas de decisión). Pero dado que es un modelo lineal, la interpretación del peso aún no es intuitiva. Viene con la misma nota al pie que un modelo de regresión lineal habitual:  dado que todas las características son fijas. Se vuelve un poco más complicado cuando tienes reglas superpuestas. Por ejemplo, una regla de decisión (característica) para la predicción de la bicicleta podría ser: temp&gt; 10 y otra regla podría ser temp&gt; 15 &amp; weather = GOOD. Si el clima es bueno y la temperatura es superior a 15 grados, la temperatura es automáticamente mayor que 10. En los casos en que se aplica la segunda regla, también se aplica la primera regla. La interpretación del peso estimado para la segunda regla es: Suponiendo que todas las demás características permanezcan fijas, el número previsto de bicicletas aumenta en \\(\\beta_2\\) cuando el clima es bueno y la temperatura supera los 15 grados. Pero, ahora queda muy claro que todas las demás características fijas son problemáticas, porque si se aplica la regla 2, también se aplica la regla 1 y la interpretación no tiene sentido. 4.6.5 Software y alternativa El algoritmo RuleFit está implementado en R por Fokkema y Christoffersen (2017)24 y puede encontrar una versión de Python en Github. Un marco muy similar es skope-rules, un módulo de Python que también extrae reglas de conjuntos. Difiere en la forma en que aprende las reglas finales: Primero, las reglas de skope eliminan las reglas de bajo rendimiento, basadas en los umbrales de recuperación y precisión. Luego, las reglas duplicadas y similares se eliminan al realizar una selección basada en la diversidad de términos lógicos (variable + operador más grande / más pequeño) y el rendimiento (puntaje F1) de las reglas. Friedman, Jerome H, and Bogdan E Popescu. Predictive learning via rule ensembles. The Annals of Applied Statistics. JSTOR, 91654. (2008). Fokkema, Marjolein, and Benjamin Christoffersen. Pre: Prediction rule ensembles. https://CRAN.R-project.org/package=pre (2017). "],["interpretables-otros.html", "4.7 Otros modelos interpretables", " 4.7 Otros modelos interpretables La lista de modelos interpretables está en constante crecimiento y es de tamaño desconocido. Incluye modelos simples como modelos lineales, árboles de decisión y Naive Bayes, pero también modelos más complejos que combinan o modifican modelos de aprendizaje automático no interpretables para hacerlos más interpretables. Especialmente las publicaciones sobre este último tipo de modelos se están produciendo actualmente en alta frecuencia y es difícil mantenerse al día con los desarrollos. El libro solo muestra el clasificador Naive Bayes y los k vecinos más cercanos en este capítulo. 4.7.1 Clasificador Naive Bayes El clasificador Naive Bayes utiliza el teorema de Bayes de probabilidades condicionales. Para cada característica, calcula la probabilidad de una clase dependiendo del valor de la característica. El clasificador Naive Bayes calcula las probabilidades de clase para cada característica de forma independiente, lo que equivale a un supuesto fuerte (= ingenuo) de independencia de las características. Naive Bayes es un modelo de probabilidad condicional y modela la probabilidad de una clase \\(C_k\\) de la siguiente manera: \\[P(C_k|x)=\\frac{1}{Z}P(C_k)\\prod_{i=1}^n{}P(x_i|C_k)\\] El término Z es un parámetro de escala que asegura que la suma de probabilidades para todas las clases es 1 (de lo contrario, no serían probabilidades). La probabilidad condicional de una clase es la probabilidad de clase multiplicada por la probabilidad de cada característica dada la clase, normalizada por Z. Esta fórmula puede derivarse utilizando el teorema de Bayes. Naive Bayes es un modelo interpretable debido al supuesto de independencia. Se puede interpretar a nivel modular. Está muy claro para cada característica cuánto contribuye a una determinada predicción de clase, ya que podemos interpretar la probabilidad condicional. 4.7.2 K Vecinos más cercanos El método del k vecino más cercano se puede usar para regresión y clasificación, y utiliza los vecinos más cercanos de un punto de datos para la predicción. Para la clasificación, el método vecino asigna la clase más común de los vecinos más cercanos de una instancia. Para la regresión, toma el promedio del resultado de los vecinos. Las partes difíciles son encontrar la k correcta y decidir cómo medir la distancia entre instancias, lo que finalmente define el vecindario. El modelo k vecino más cercano difiere de los otros modelos interpretables presentados en este libro porque es un algoritmo de aprendizaje basado en instancias. ¿Cómo se pueden interpretar los vecinos k-más cercanos? En primer lugar, no hay parámetros para aprender, por lo que no hay interpretabilidad a nivel modular. Además, hay una falta de interpretación del modelo global porque el modelo es inherentemente local y no hay pesos o estructuras globales explícitamente aprendidas. ¿Quizás es interpretable a nivel local? Para explicar una predicción, siempre puedes recuperar los k vecinos que se usaron para la predicción. Si el modelo es interpretable depende únicamente de la pregunta de si puede interpretar una sola instancia en el conjunto de datos. Si una instancia consta de cientos o miles de características, entonces no es interpretable, argumentaría. Pero si tienes pocas características o una forma de reducir tu instancia a las características más importantes, presentar a los k vecinos más cercanos puede darte buenas explicaciones. "],["agnostico.html", "Capítulo 5 Métodos modelo-agnósticos", " Capítulo 5 Métodos modelo-agnósticos Separar las explicaciones del modelo de aprendizaje automático (= métodos de interpretación independientes del modelo) tiene algunas ventajas (Ribeiro, Singh y Guestrin 201625). La gran ventaja de los métodos de interpretación independientes del modelo sobre los específicos del modelo es su flexibilidad. Los desarrolladores de aprendizaje automático pueden usar cualquier modelo de aprendizaje automático que deseen cuando los métodos de interpretación se pueden aplicar a cualquier modelo. Todo lo que se basa en una interpretación de un modelo de aprendizaje automático, como una interfaz gráfica o de usuario, también se vuelve independiente del modelo de aprendizaje automático subyacente. Por lo general, no solo se evalúa uno, sino que se evalúan muchos tipos de modelos de aprendizaje automático para resolver una tarea, y cuando se comparan modelos en términos de interpretabilidad, es más fácil trabajar con explicaciones independientes del modelo, ya que se puede usar el mismo método para cualquier tipo de modelo. Una alternativa a los métodos de interpretación independientes del modelo es usar solo modelos interpretables, que a menudo tiene la gran desventaja de que se pierde el rendimiento predictivo en comparación con otros modelos de aprendizaje automático y se limita a un tipo de modelo. La otra alternativa es utilizar métodos de interpretación específicos del modelo. La desventaja de esto es que también lo vincula a un tipo de modelo y será difícil cambiar a otra cosa. Los aspectos deseables de un sistema de explicación modelo-agnóstico son (Ribeiro, Singh y Guestrin 2016): Flexibilidad del modelo: El método de interpretación puede funcionar con cualquier modelo de aprendizaje automático, como random forest y redes neuronales profundas. Flexibilidad de explicación: No está limitado a una cierta forma de explicación. En algunos casos, puede ser útil tener una fórmula lineal, en otros casos, un gráfico con características importantes. Flexibilidad de representación: El sistema de explicación debería poder utilizar una representación de características diferente como el modelo que se explica. Para un clasificador de texto que usa vectores abstractos de incrustación de palabras, podría ser preferible usar la presencia de palabras individuales para la explicación. La fotografía más grande Echemos un vistazo de alto nivel a la interpretabilidad agnóstica del modelo. Capturamos el mundo mediante la recopilación de datos y lo resumimos aún más al aprender a predecir los datos (para la tarea) con un modelo de aprendizaje automático. La interpretabilidad es solo otra capa en la parte superior que ayuda a los humanos a comprender. FIGURA 5.1: El panorama general del aprendizaje automático explicable. El mundo real atraviesa muchas capas antes de llegar al humano en forma de explicaciones. La capa más baja es el Mundo. Esto podría ser literalmente la naturaleza misma, como la biología del cuerpo humano y cómo reacciona a la medicación, pero también cosas más abstractas como el mercado inmobiliario. Esta capa contiene todo lo que se puede observar y es de interés. En definitiva, queremos aprender algo sobre el mundo e interactuar con él. La segunda capa es la capa Datos. Tenemos que digitalizar el mundo para que sea procesable para las computadoras y también para almacenar información. La capa de datos contiene cualquier cosa, desde imágenes, textos, datos tabulares, etc. Al ajustar modelos de aprendizaje automático basados en la capa de datos, obtenemos la capa de modelos de caja negra. Los algoritmos de aprendizaje automático aprenden con datos del mundo real para hacer predicciones o encontrar estructuras. Por encima de la capa de modelos de caja negra se encuentra la capa Métodos de interpretación, que nos ayuda a lidiar con la opacidad de los modelos de aprendizaje automático. ¿Cuáles fueron las características más importantes para un diagnóstico particular? ¿Por qué una transacción financiera se clasificó como fraude? La última capa está ocupada por un Humano. ¡Mira! ¡Te saluda porque estás leyendo este libro y estás ayudando a proporcionar mejores explicaciones para los modelos de caja negra! Los humanos son, en última instancia, los consumidores de las explicaciones. Esta abstracción de varias capas también ayuda a comprender las diferencias en los enfoques entre los estadísticos y los profesionales del aprendizaje automático. Los estadísticos se ocupan de la capa de datos, como planificar ensayos clínicos o diseñar encuestas. Se saltan la capa del Modelo de caja negra y van directamente a la capa Métodos de interpretación. Los especialistas en aprendizaje automático también se ocupan de la capa de datos, como recolectar muestras etiquetadas de imágenes de cáncer de piel o rastrear Wikipedia. Luego entrenan un modelo de aprendizaje automático de caja negra. Se omite la capa Métodos de interpretación y los humanos se ocupan directamente de las predicciones del modelo de caja negra. Es genial que el aprendizaje automático interpretable fusione el trabajo de estadísticos y especialistas en aprendizaje automático. Por supuesto, este gráfico no captura todo: Los datos pueden provenir de simulaciones. Los modelos de caja negra también generan predicciones que podrían ni siquiera llegar a los humanos, sino que solo suministran otras máquinas, etc. Pero, en general, es una abstracción útil comprender cómo la interpretabilidad se convierte en esta nueva capa además de los modelos de aprendizaje automático. Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability of machine learning. ICML Workshop on Human Interpretability in Machine Learning. (2016). "],["pdp.html", "5.1 Diagrama de dependencia parcial (PDP)", " 5.1 Diagrama de dependencia parcial (PDP) El gráfico de dependencia parcial (PDP o gráfico de PD) muestra el efecto marginal que una o dos características tienen sobre el resultado previsto de un modelo de aprendizaje automático (J. H. Friedman 200126). Un gráfico de dependencia parcial puede mostrar si la relación entre el objetivo y una característica es lineal, monótona o más compleja. Por ejemplo, cuando se aplica a un modelo de regresión lineal, los gráficos de dependencia parcial siempre muestran una relación lineal. La función de dependencia parcial para la regresión se define como: \\[\\hat{f}_{x_S}(x_S)=E_{x_C}\\left[\\hat{f}(x_S,x_C)\\right]=\\int\\hat{f}(x_S,x_C)d\\mathbb{P}(x_C)\\] \\(X_S\\) son las características para las cuales se debe trazar la función de dependencia parcial y \\(x_C\\) son las otras características utilizadas en el modelo de aprendizaje automático \\(\\hat{f}\\). Por lo general, solo hay una o dos características en el conjunto S. Las características en S son aquellas para las que queremos saber el efecto en la predicción. Los vectores de características \\(x_S\\) y \\(x_C\\) combinados forman el espacio total de características x. La dependencia parcial funciona al marginar la salida del modelo de aprendizaje automático sobre la distribución de las características en el conjunto C, de modo que la función muestra la relación entre las características en el conjunto S que nos interesan y el resultado previsto. Al marginar sobre las otras características, obtenemos una función que depende solo de las características en S, incluidas las interacciones con otras características. La función parcial \\(\\hat{f}_{x_S}\\) se calcula calculando promedios en los datos de entrenamiento, también conocido como método Monte Carlo: \\[\\hat{f}_{x_S}(x_S)=\\frac{1}{n}\\sum_{i=1}^n\\hat{f}(x_S,x^{(i)}_{C})\\] La función parcial nos dice para los valores dados de las características S cuál es el efecto marginal promedio en la predicción. En esta fórmula, \\(x^{(i)}_{C}\\) son valores de características reales del conjunto de datos para las características en las que no estamos interesados, y n es el número de instancias en el conjunto de datos. Una suposición del PDP es que las características en C no están correlacionadas con las características en S. Si se viola este supuesto, los promedios calculados para el gráfico de dependencia parcial incluirán puntos de datos que son muy improbables o incluso imposibles (ver desventajas). Para la clasificación en la que el modelo de aprendizaje automático genera probabilidades, el gráfico de dependencia parcial muestra la probabilidad de una determinada clase dados diferentes valores para las características en S. Una manera fácil de lidiar con múltiples clases es dibujar una línea o diagrama por clase. El diagrama de dependencia parcial es un método global: El método considera todas las instancias y ofrece una declaración sobre la relación global de una característica con el resultado previsto. Características categóricas Hasta ahora, solo hemos considerado las características numéricas. Para características categóricas, la dependencia parcial es muy fácil de calcular. Para cada una de las categorías, obtenemos una estimación de PDP al obligar a todas las instancias de datos a tener la misma categoría. Por ejemplo, si observamos el conjunto de datos de alquiler de bicicletas y estamos interesados en el gráfico de dependencia parcial para la temporada, obtenemos 4 números, uno para cada temporada. Para calcular el valor de verano, reemplazamos la temporada de todas las instancias de datos con verano y promediamos las predicciones. 5.1.1 Ejemplos En la práctica, el conjunto de características S generalmente solo contiene una característica o un máximo de dos, porque una característica produce gráficos 2D y dos características producen gráficos 3D. Todo lo que está más allá de eso es bastante complicado. Incluso 3D en un papel o monitor 2D ya es un desafío. Volvamos al ejemplo de regresión, en el que predecimos el número de bicicletas que se alquilarán en un día determinado. Primero ajustamos un modelo de aprendizaje automático, luego analizamos las dependencias parciales. En este caso, hemos ajustado un random forest para predecir el número de bicicletas y utilizar el diagrama de dependencia parcial para visualizar las relaciones que el modelo ha aprendido. La influencia de las características climáticas en el conteo previsto de bicicletas se visualiza en la siguiente figura. FIGURA 5.2: PDP para el modelo de predicción de conteo de bicicletas y temperatura, humedad y velocidad del viento. Las mayores diferencias se pueden ver en la temperatura. Cuanto más caliente, más bicicletas se alquilan. Esta tendencia sube a 20 grados centígrados, luego se aplana y cae ligeramente a 30. Las marcas en el eje x indican la distribución de datos. Para clima cálido pero no demasiado caluroso, el modelo predice en promedio un gran número de bicicletas alquiladas. Los ciclistas potenciales se inhiben cada vez más en el alquiler de una bicicleta cuando la humedad supera el 60%. Además, cuanto más viento, a menos personas les gusta andar en bicicleta, lo que tiene sentido. Curiosamente, el número previsto de alquileres de bicicletas no disminuye cuando la velocidad del viento aumenta de 25 a 35 km/h, pero no hay muchos datos de entrenamiento, por lo que el modelo de aprendizaje automático probablemente no pueda obtener una predicción significativa para este rango. Al menos intuitivamente, esperaría que la cantidad de bicicletas disminuya al aumentar la velocidad del viento, especialmente cuando la velocidad del viento es muy alta. Para ilustrar un diagrama de dependencia parcial con una característica categórica, examinamos el efecto de la característica de temporada en el alquiler de bicicletas previsto. FIGURA 5.3: PDP para el modelo de predicción de conteo de bicicletas y la temporada. Inesperadamente, todas las estaciones muestran un efecto similar en las predicciones del modelo, solo para la primavera, el modelo predice menos alquileres de bicicletas. También calculamos la dependencia parcial para clasificación de cáncer cervical. Esta vez, ajustamos un random forest para predecir si una mujer podría contraer cáncer cervical en función de los factores de riesgo. Calculamos y visualizamos la dependencia parcial de la probabilidad de cáncer de diferentes características para el random forest: FIGURA 5.4: PDP de probabilidad de cáncer según la edad y años con anticonceptivos hormonales. Para la edad, el PDP muestra que la probabilidad es baja hasta los 40 y aumenta después. Cuantos más años con anticonceptivos hormonales, mayor será el riesgo de cáncer previsto, especialmente después de 10 años. Para ambas características, no había muchos puntos de datos con valores grandes disponibles, por lo que las estimaciones de PD son menos confiables en esas regiones. También podemos visualizar la dependencia parcial de dos características a la vez: FIGURA 5.5: PDP de probabilidad de cáncer y la interacción de la edad y el número de embarazos. El gráfico muestra el aumento en la probabilidad de cáncer a los 45 años. Para las edades menores de 25 años, las mujeres que tuvieron 1 o 2 embarazos tienen un riesgo de cáncer más bajo previsto, en comparación con las mujeres que tuvieron 0 o más de 2 embarazos. Pero ten cuidado al sacar conclusiones: ¡Esto podría ser una correlación no causal! 5.1.2 Ventajas El cálculo de los gráficos de dependencia parcial es intuitivo: La función de dependencia parcial en un valor de característica particular representa la predicción promedio si forzamos a todos los puntos de datos a asumir ese valor de característica. En mi experiencia, los laicos generalmente entienden la idea de PDP rápidamente. Si la característica para la que calculó el PDP no está correlacionada con las otras características, entonces los PDP representan perfectamente cómo la característica influye en la predicción en promedio. En el caso no correlacionado, la interpretación es clara: El gráfico de dependencia parcial muestra cómo cambia la predicción promedio en su conjunto de datos cuando se cambia la característica j-ésima. Es más complicado cuando las características están correlacionadas, ver también desventajas. Las gráficas de dependencia parcial son fáciles de implementar. El cálculo para los gráficos de dependencia parcial tiene una interpretación causal. Intervenimos en una característica y medimos los cambios en las predicciones. Al hacerlo, analizamos la relación causal entre la característica y la predicción. 27 La relación es causal para el modelo, porque modelamos explícitamente el resultado en función de las características, ¡pero no necesariamente para el mundo real! 5.1.3 Desventajas El número máximo realista de características en una función de dependencia parcial es dos. Esto no es culpa de los PDP, sino de la representación bidimensional (papel o pantalla) y también de nuestra incapacidad para imaginar más de 3 dimensiones. Algunos gráficos de PD no muestran la distribución de características. Omitir la distribución puede ser engañoso, porque podría sobreinterpretar regiones sin casi datos. Este problema se resuelve fácilmente mostrando una alfombra (indicadores para puntos de datos en el eje x) o un histograma. La suposición de independencia es el mayor problema con los PDP. Se supone que las características para las que se calcula la dependencia parcial no están correlacionadas con otras características. Por ejemplo, supón que deseas predecir qué tan rápido camina una persona, dado el peso y la altura. Para la dependencia parcial de una de las características, por ejemplo altura, suponemos que las otras características (peso) no están correlacionadas con la altura, lo que obviamente es una suposición falsa. Para el cálculo del PDP a cierta altura (por ejemplo, 200 cm), promediamos la distribución marginal de peso, que podría incluir un peso por debajo de 50 kg, lo que no es realista para una persona de 2 metros. En otras palabras: Cuando las características están correlacionadas, creamos nuevos puntos de datos en áreas de la distribución de características donde la probabilidad real es muy baja (por ejemplo, es poco probable que alguien mida 2 metros de altura pero pese menos de 50 kg). Una solución a este problema es Gráficos de efectos locales acumulados o gráficos ALE cortos que funcionan con la distribución condicional en lugar de la marginal. Los efectos heterogéneos pueden estar ocultos porque las gráficas PD solo muestran los efectos marginales promedio. Supón que para una característica, la mitad de sus puntos de datos tienen una asociación positiva con la predicción (cuanto mayor es el valor de la característica, mayor es la predicción) y la otra mitad tiene una asociación negativa: cuanto menor es el valor de la característica, mayor es la predicción. La curva PD podría ser una línea horizontal, ya que los efectos de ambas mitades del conjunto de datos podrían cancelarse entre sí. Luego concluye que la función no tiene efecto en la predicción. Al trazar las curvas de expectativas condicionales individuales en lugar de la línea agregada, podemos descubrir efectos heterogéneos. 5.1.4 Software y alternativas Hay varios paquetes R que implementan PDP. Usé el paquete iml para los ejemplos, pero también haypdp o DALEX. En Python, las gráficas de dependencia parcial están integradas en scikit-learn y puede usarPDPBox. Las alternativas a los PDP presentados en este libro son gráficos ALE y curvas ICE. Friedman, Jerome H. Greedy function approximation: A gradient boosting machine. Annals of statistics (2001): 1189-1232. Zhao, Qingyuan, and Trevor Hastie. Causal interpretations of black-box models. Journal of Business &amp; Economic Statistics, to appear. (2017). "],["ICE.html", "5.2 Expectativa condicional individual (ICE)", " 5.2 Expectativa condicional individual (ICE) Los gráficos de Expectativas condicionales individuales (ICE) muestran una línea por instancia, que muestra cómo cambia la predicción de esa observación cuando cambia una característica. El gráfico de dependencia parcial para el efecto promedio de una característica es un método global porque no se enfoca en observaciones específicas, sino en un promedio general. El equivalente a un PDP para instancias de datos individuales se llama gráfico de expectativa condicional individual (ICE) (Goldstein et al. 2017 28). Un gráfico ICE visualiza la dependencia de la predicción en una característica para cada instancia por separado, lo que da como resultado una línea por instancia, en comparación con una línea general en los gráficos de dependencia parcial. Un PDP es el promedio de las líneas de un diagrama ICE. Los valores para una observación se pueden calcular manteniendo todas las otras características iguales, creando variantes de esta instancia reemplazando el valor de la característica con valores de una cuadrícula y haciendo predicciones con el modelo de caja negra para estas instancias recién creadas. El resultado es un conjunto de puntos para una observación con el valor de la característica de la cuadrícula y las predicciones respectivas. ¿Cuál es el punto de mirar las expectativas individuales en lugar de las dependencias parciales? Las gráficas de dependencia parcial pueden oscurecer una relación heterogénea creada por las interacciones. Los PDP pueden mostrarte cómo se ve la relación promedio entre una característica y la predicción. Esto solo funciona bien si las interacciones entre las características para las cuales se calcula el PDP y las otras características son débiles. En caso de interacciones, la trama ICE proporcionará mucha más información. Una definición más formal: En las gráficas ICE, para cada instancia en \\(\\{(x_{S}^{(i)},x_{C}^{(i)})\\}_{i=1}^N\\) la curva \\(\\hat{f}_S^{(i)}\\) se representa frente a \\(x^{(i)}_{S}\\), mientras que \\(x^{(i)}_{C}\\) permanece fijo. 5.2.1 Ejemplos Volvamos al conjunto de datos de cáncer cervical y veamos cómo la predicción de cada instancia está asociada con la función Edad. Analizaremos un random forest que predice la probabilidad de cáncer para una mujer dados los factores de riesgo. En el gráfico de dependencia parcial hemos visto que la probabilidad de cáncer aumenta alrededor de los 50 años, pero ¿es esto cierto para todas las mujeres en el conjunto de datos? El gráfico ICE revela que para la mayoría de las mujeres el efecto de la edad sigue el patrón promedio de un aumento a los 50 años, pero hay algunas excepciones: Para las pocas mujeres que tienen una alta probabilidad pronosticada a una edad temprana, la probabilidad pronosticada de cáncer no cambia mucho con la edad. FIGURA 5.6: Gráfico ICE de probabilidad de cáncer cervical por edad. Cada línea representa a una mujer. Para la mayoría de las mujeres hay un aumento en la probabilidad pronosticada de cáncer al aumentar la edad. Para algunas mujeres con un pronóstico probabilidad de cáncer por encima de 0.4, la predicción no cambia mucho a mayor edad. La siguiente figura muestra las gráficas de ICE para la predicción de alquiler de bicicletas. El modelo de predicción subyacente es un random forest. FIGURA 5.7: Gráfico ICE para el alquiler de bicicletas previsto por las condiciones climáticas. Se pueden observar los mismos efectos que en los gráficos de dependencia parcial. Todas las curvas parecen seguir el mismo curso, por lo que no hay interacciones obvias. Eso significa que el PDP es un buen resumen de las relaciones entre las características mostradas y el número previsto de bicicletas. 5.2.1.1 Gráfico ICE centrado Hay un problema con los gráficos de ICE: A veces puede ser difícil saber si las curvas ICE difieren entre los individuos porque comienzan con diferentes predicciones. Una solución simple es centrar las curvas en un cierto punto de la entidad y mostrar solo la diferencia en la predicción hasta este punto. La gráfica resultante se llama gráfica centrada de ICE (c-ICE). Anclar las curvas en el extremo inferior de la entidad es una buena opción. Las nuevas curvas se definen como: \\[\\hat{f}_{cent}^{(i)}=\\hat{f}^{(i)}-\\mathbf{1}\\hat{f}(x^{a},x^{(i)}_{C})\\] donde \\(\\mathbf{1}\\) es un vector de 1 con el número apropiado de dimensiones (generalmente una o dos), \\(\\hat{f}\\) es el modelo ajustado y xa es el punto de anclaje. 5.2.1.2 Ejemplo Por ejemplo, tome la gráfica ICE del cáncer de cuello uterino para la edad y centre las líneas en la edad más joven observada: FIGURA 5.8: Gráfico ICE centrado para la probabilidad pronosticada de cáncer por edad. Las líneas se fijan en 0 a la edad 14. En comparación con la edad 14, las predicciones para la mayoría de las mujeres permanecen sin cambios hasta la edad de 45 años, donde aumenta la probabilidad pronosticada. Las gráficas centradas de ICE facilitan la comparación de las curvas de instancias individuales. Esto puede ser útil si no queremos ver el cambio absoluto de un valor predicho, sino la diferencia en la predicción en comparación con un punto fijo del rango de características. Echemos un vistazo al gráfico centrado ICE para la predicción del alquiler de bicicletas: FIGURA 5.9: Gráfico ICE centrado ICE del número previsto de bicicletas por condición climática. Las líneas muestran la diferencia en la predicción en comparación con la predicción con el valor de la característica respectiva en su mínimo observado. 5.2.1.3 Diagrama de ICE derivado Otra forma de hacer que sea visualmente más fácil detectar la heterogeneidad es observar las derivadas individuales de la función de predicción con respecto a una característica. El diagrama resultante se llama diagrama derivado de ICE (d-ICE). Las derivadas de una función (o curva) te indican si ocurren cambios y en qué dirección ocurren. Con el diagrama ICE derivado, es fácil detectar rangos de valores de características donde las predicciones de caja negra cambian para (al menos algunas) instancias. Si no hay interacción entre la característica analizada \\(x_S\\) y las otras características \\(x_C\\), la función de predicción se puede expresar como: \\[\\hat{f}(x)=\\hat{f}(x_S,x_C)=g(x_S)+h(x_C),\\quad\\text{con}\\quad\\frac{\\delta\\hat{f}(x)}{\\delta{}x_S}=g&#39;(x_S)\\] Sin interacciones, las derivadas parciales individuales deberían ser las mismas para todas las instancias. Si difieren, se debe a interacciones y se hace visible en el diagrama d-ICE. Además de mostrar las curvas individuales para la derivada de la función de predicción con respecto a la función en S, mostrar la desviación estándar de la derivada ayuda a resaltar regiones en función en S con heterogeneidad en las derivadas estimadas. El diagrama derivado de ICE tarda mucho tiempo en calcularse y es poco práctico. 5.2.2 Ventajas Las curvas de expectativas condicionales individuales son aún más intuitivas de entender que las gráficas de dependencia parcial. Una línea representa las predicciones para una instancia si variamos la característica de interés. A diferencia de los gráficos de dependencia parcial, las curvas ICE pueden descubrir relaciones heterogéneas. 5.2.3 Desventajas Las curvas ICE solo pueden mostrar una característica de manera significativa, porque dos características requerirían el dibujo de varias superficies superpuestas y no vería nada en la gráfica. Las curvas ICE sufren el mismo problema que las PDP: Si la característica de interés está correlacionada con las otras características, entonces algunos puntos en las líneas podrían ser puntos de datos no válidos de acuerdo con la distribución conjunta de características. Si se dibujan muchas curvas ICE, la trama puede estar superpoblada y no verás nada. La solución: agrega algo de transparencia a las líneas o dibuje solo una muestra de las líneas. En los gráficos de ICE puede que no sea fácil ver el promedio. Esto tiene una solución simple: Combina las curvas de expectativa condicional individuales con la gráfica de dependencia parcial. 5.2.4 Software y alternativas Las gráficas ICE se implementan en los paquetes R iml (utilizado para estos ejemplos), ICEbox 29 y pdp. Otro paquete R que hace algo muy similar a ICE es condvis. Goldstein, Alex, et al. Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics 24.1 (2015): 44-65. Goldstein, Alex, et al. Package ICEbox. (2017). "],["ale.html", "5.3 Gráfico de efectos locales acumulados (ALE)", " 5.3 Gráfico de efectos locales acumulados (ALE) Los efectos locales acumulados 30 describen cómo las características influyen en la predicción de un modelo de aprendizaje automático en promedio. Los gráficos ALE son una alternativa más rápida e imparcial a los gráficos de dependencia parcial (PDP). Recomiendo leer primero el capítulo sobre gráficas de dependencia parcial, ya que son más fáciles de entender y ambos métodos comparten el mismo objetivo: Ambos describen cómo una característica afecta la predicción en promedio. En la siguiente sección, quiero convencerte de que los gráficos de dependencia parcial tienen un problema grave cuando las características están correlacionadas. 5.3.1 Motivación e intuición Si las características de un modelo de aprendizaje automático están correlacionadas, no se puede confiar en el diagrama de dependencia parcial. El cálculo de una gráfica de dependencia parcial para una característica que está fuertemente correlacionada con otras características implica promedios de instancias de datos artificiales que son poco probables en la realidad. Esto puede sesgar en gran medida el efecto de función estimado. Imagina calcular gráficos de dependencia parcial para un modelo de aprendizaje automático que predice el valor de una casa en función del número de habitaciones y el tamaño de la superficie habitable. Estamos interesados en el efecto del área habitable en el valor predicho. Como recordatorio, la receta para los gráficos de dependencia parcial es: 1) Seleccionar variable. 2) Definir cuadrícula. 3) Por valor de cuadrícula: a) Reemplazar la característica con el valor de cuadrícula y b) predicciones promedio. 4) Dibujar curva. Para el cálculo del primer valor de cuadrícula del PDP, digamos 30 m^2, reemplazamos el área habitable para todas instancias por 30 m^2, incluso para casas con 10 habitaciones. A mí me parece una casa muy inusual. El diagrama de dependencia parcial incluye estas casas poco realistas en la estimación del efecto característico y pretende que todo está bien. La siguiente figura ilustra dos características correlacionadas y cómo resulta que el método de diagrama de dependencia parcial promedia las predicciones de instancias poco probables. FIGURA 5.10: Características fuertemente correlacionadas x1 y x2. Para calcular el efecto de función de x1 en 0.75, el PDP reemplaza x1 de todas las instancias con 0.75, suponiendo falsamente que la distribución de x2 en x1 = 0.75 es lo mismo que la distribución marginal de x2 (línea vertical). Esto da como resultado combinaciones poco probables de x1 y x2 (p. Ej. X2 = 0.2 en x1 = 0.75), que el PDP usa para calcular el efecto promedio. ¿Qué podemos hacer para obtener una estimación del efecto de la característica que respete la correlación de las características? Podríamos promediar sobre la distribución condicional de la característica, es decir, con un valor de cuadrícula de x1, promediamos las predicciones de instancias con un valor de x1 similar. La solución para calcular los efectos de entidad usando la distribución condicional se llama Gráficos marginales o M-Plots (nombre confuso, ya que se basan en la distribución condicional, no marginal). Espera, ¿no te prometí que hablaría de los gráficos ALE? Los M-Plots no son la solución que estamos buscando. ¿Por qué los M-Plots no resuelven nuestro problema? Si promediamos las predicciones de todas las casas de aproximadamente 30 m^2, estimamos el efecto combinado del área habitable y del número de habitaciones, debido a su correlación. Suponga que la sala de estar no tiene efecto sobre el valor predicho de una casa, solo el número de habitaciones lo tiene. El diagrama M aún mostraría que el tamaño del área de vida aumenta el valor predicho, ya que el número de habitaciones aumenta con el área de vida. La siguiente gráfica muestra para dos características correlacionadas cómo funcionan los M-Plots. FIGURA 5.11: Características fuertemente correlacionadas x1 y x2. M-Plots promedio sobre la distribución condicional. Aquí la distribución condicional de x2 en x1 = 0.75. El promedio de las predicciones locales lleva a mezclar los efectos de ambas características Los Gráficos M evitan promedios de instancias de datos poco probables, pero mezclan el efecto de una característica con los efectos de todas las características correlacionadas. Las gráficas ALE resuelven este problema calculando, también en función de la distribución condicional de las características, diferencias en las predicciones en lugar de promedios. Para el efecto del área habitable a 30 m^2, el método ALE usa todas las casas con aproximadamente 30 m^2, obtiene las predicciones del modelo que fingen que estas casas fueron de 31 m^2 menos la predicción que finge que eran 29 m^2. Esto nos da el efecto puro de la sala de estar y no está mezclando el efecto con los efectos de características correlacionadas. El uso de diferencias bloquea el efecto de otras características. El siguiente gráfico proporciona una intuición de cómo se calculan los gráficos ALE. FIGURA 5.12: Cálculo de ALE para la característica x1, que se correlaciona con x2. Primero, dividimos la característica en intervalos (líneas verticales). Para las instancias de datos (puntos) en un intervalo, calculamos la diferencia en la predicción cuando reemplazamos la entidad con el límite superior e inferior del intervalo (líneas horizontales). Estas diferencias se acumulan y centran posteriormente, lo que da como resultado la curva ALE. Para resumir cómo cada tipo de gráfico (PDP, M, ALE) calcula el efecto de una característica en un determinado valor de cuadrícula v: Gráficos de dependencia parcial: Permíteme mostrarte lo que el modelo predice en promedio cuando cada instancia de datos tiene el valor v para esa característica. Ignoro si el valor v tiene sentido para todas las instancias de datos. M-Plots: Déjame mostrarte lo que el modelo predice en promedio para instancias de datos que tienen valores cercanos a v para esa característica. El efecto podría deberse a esa característica, pero también a características correlacionadas. Gráfica ALE: Permíteme mostrarte cómo cambian las predicciones del modelo en una pequeña ventana de la función alrededor de v para instancias de datos en esa ventana. 5.3.2 Teoría ¿Cómo difieren matemáticamente las gráficas PD, M y ALE? Es común a los tres métodos que reducen la compleja función de predicción f a una función que depende de solo una (o dos) características. Los tres métodos reducen la función promediando los efectos de las otras características, pero difieren en si se calculan los promedios de las predicciones o las diferencias en las predicciones y si el promedio se realiza sobre la distribución marginal o condicional. Los gráficos de dependencia parcial promedian las predicciones sobre la distribución marginal. \\[\\begin{align*}\\hat{f}_{x_S,PDP}(x_S)&amp;=E_{X_C}\\left[\\hat{f}(x_S,X_C)\\right]\\\\&amp;=\\int_{x_C}\\hat{f}(x_S,x_C)\\mathbb{P}(x_C)d{}x_C\\end{align*}\\] Este es el valor de la función de predicción f, en los valores de función \\(x_S\\), promediado sobre todas las funciones en \\(x_C\\). Promedio significa calcular la expectativa marginal E sobre las características del conjunto C, que es la integral sobre las predicciones ponderadas por la distribución de probabilidad. Suena elegante, pero para calcular el valor esperado sobre la distribución marginal, simplemente tomamos todas nuestras instancias de datos, les obligamos a tener un cierto valor de cuadrícula para las características en el conjunto S y promediamos las predicciones para este conjunto de datos manipulados. Este procedimiento asegura que promediamos la distribución marginal de las características. Las Gráficas M promedian las predicciones sobre la distribución condicional. \\[\\begin{align*}\\hat{f}_{x_S,M}(x_S)&amp;=E_{X_C|X_S}\\left[\\hat{f}(X_S,X_C)|X_S=x_s\\right]\\\\&amp;=\\int_{x_C}\\hat{f}(x_S,x_C)\\mathbb{P}(x_C|x_S)d{}x_C\\end{align*}\\] Lo único que cambia en comparación con los PDP es que promediamos las predicciones condicionales a cada valor de cuadrícula de la característica de interés, en lugar de asumir la distribución marginal en cada valor de cuadrícula. En la práctica, esto significa que tenemos que definir un vecindario, por ejemplo, para el cálculo del efecto de 30 m^2 en el valor predicho de la casa, podríamos promediar las predicciones de todas las casas entre 28 y 32 m^2. Las gráficas ALE promedian los cambios en las predicciones y las acumulan sobre la cuadrícula (más sobre el cálculo más adelante). \\[\\begin{align*}\\hat{f}_{x_S,ALE}(x_S)=&amp;\\int_{z_{0,1}}^{x_S}E_{X_C|X_S}\\left[\\hat{f}^S(X_s,X_c)|X_S=z_S\\right]dz_S-\\text{constant}\\\\=&amp;\\int_{z_{0,1}}^{x_S}\\int_{x_C}\\hat{f}^S(z_s,x_c)\\mathbb{P}(x_C|z_S)d{}x_C{}dz_S-\\text{constant}\\end{align*}\\] La fórmula revela tres diferencias con los M-Plots. Primero, promediamos los cambios de las predicciones, no las predicciones en sí. El cambio se define como el gradiente (pero más tarde, para el cálculo real, reemplazado por las diferencias en las predicciones durante un intervalo). \\[\\hat{f}^S(x_s,x_c)=\\frac{\\delta\\hat{f}(x_S,x_C)}{\\delta{}x_S}\\] La segunda diferencia es la integral adicional sobre z. Acumulamos los gradientes locales sobre el rango de características en el conjunto S, lo que nos da el efecto de la característica en la predicción. Para el cálculo real, las z se reemplazan por una cuadrícula de intervalos sobre los cuales calculamos los cambios en la predicción. En lugar de promediar directamente las predicciones, el método ALE calcula las diferencias de predicción condicionales a las características S e integra la derivada sobre las características S para estimar el efecto. Bueno, eso suena estúpido. La derivación y la integración generalmente se cancelan entre sí, como restar primero y luego sumar el mismo número. ¿Por qué tiene sentido aquí? La derivada (o diferencia de intervalo) aísla el efecto de la característica de interés y bloquea el efecto de las características correlacionadas. La tercera diferencia de los gráficos ALE con los gráficos M es que restamos una constante de los resultados. Este paso centra el gráfico ALE para que el efecto promedio sobre los datos sea cero. Queda un problema: No todos los modelos vienen con un gradiente, por ejemplo, el random forest no tiene gradiente. Pero como verás, el cálculo real funciona sin gradientes y utiliza intervalos. Profundicemos un poco más en la estimación de las gráficas ALE. 5.3.3 Estimación Primero describiré cómo se estiman las gráficas ALE para una sola característica numérica, luego para dos características numéricas y para una sola característica categórica. Para estimar los efectos locales, dividimos la característica en muchos intervalos y calculamos las diferencias en las predicciones. Este procedimiento aproxima los gradientes y también funciona para modelos sin gradientes. Primero estimamos el efecto no centrado: \\[\\hat{\\tilde{f}}_{j,ALE}(x)=\\sum_{k=1}^{k_j(x)}\\frac{1}{n_j(k)}\\sum_{i:x_{j}^{(i)}\\in{}N_j(k)}\\left[f(z_{k,j},x^{(i)}_{\\setminus{}j})-f(z_{k-1,j},x^{(i)}_{\\setminus{}j})\\right]\\] Analicemos esta fórmula, comenzando por el lado derecho. El nombre Efectos locales acumulados refleja muy bien todos los componentes individuales de esta fórmula. En esencia, el método ALE calcula las diferencias en las predicciones, por lo que reemplazamos la característica de interés con valores de cuadrícula z. La diferencia en la predicción es el Efecto que tiene la característica para una instancia individual en un intervalo determinado. La suma de la derecha suma los efectos de todas las instancias dentro de un intervalo que aparece en la fórmula como vecindario \\(N_j(k)\\). Dividimos esta suma por el número de instancias en este intervalo para obtener la diferencia promedio de las predicciones para este intervalo. Este promedio en el intervalo está cubierto por el término Local en el nombre ALE. El símbolo de suma izquierda significa que acumulamos los efectos promedio en todos los intervalos. El ALE (no centrado) de un valor de característica que se encuentra, por ejemplo, en el tercer intervalo es la suma de los efectos del primer, segundo y tercer intervalo. La palabra Acumulado en ALE refleja esto. Este efecto está centrado para que el efecto medio sea cero. \\[\\hat{\\tilde{f}}_{j,ALE}(x)=\\sum_{k=1}^{k_j(x)}\\frac{1}{n_j(k)}\\sum_{i:x_{j}^{(i)}\\in{}N_j(k)}\\left[f(z_{k,j},x^{(i)}_{\\setminus{}j})-f(z_{k-1,j},x^{(i)}_{\\setminus{}j})\\right]\\] El valor de la ALE puede interpretarse como el efecto principal de la característica en un cierto valor en comparación con la predicción promedio de los datos. Por ejemplo, una estimación de ALE de -2 en \\(x_j = 3\\) significa que cuando la característica j-ésima tiene el valor 3, entonces la predicción es menor en 2 en comparación con la predicción promedio. Los cuantiles de la distribución de la característica se utilizan como la cuadrícula que define los intervalos. El uso de los cuantiles garantiza que haya el mismo número de instancias de datos en cada uno de los intervalos. Los cuantiles tienen la desventaja de que los intervalos pueden tener longitudes muy diferentes. Esto puede conducir a algunos gráficos ALE extraños si la característica de interés está muy sesgada, por ejemplo, muchos valores bajos y solo unos pocos valores muy altos. Gráficos ALE para la interacción de dos características Las gráficas ALE también pueden mostrar el efecto de interacción de dos características. Los principios de cálculo son los mismos que para una entidad única, pero trabajamos con celdas rectangulares en lugar de intervalos, porque tenemos que acumular los efectos en dos dimensiones. Además de ajustar el efecto medio general, también ajustamos los efectos principales de ambas características. Esto significa que ALE para dos características estima el efecto de segundo orden, que no incluye los efectos principales de las características. En otras palabras, ALE para dos características solo muestra el efecto de interacción adicional de las dos características. Le ahorro las fórmulas para gráficos 2D ALE porque son largas y desagradables de leer. Si estás interesado en el cálculo, te remito al artículo, fórmulas (13) - (16). Confiaré en las visualizaciones para desarrollar la intuición sobre el cálculo de ALE de segundo orden. FIGURA 5.13: Cálculo de 2D-ALE. Colocamos una cuadrícula sobre las dos características. En cada celda de la cuadrícula calculamos las diferencias de segundo orden para todas las instancias dentro. Primero reemplazamos los valores de x1 y x2 con los valores de las esquinas de las celdas. Si a, b, c y d representan las predicciones de esquina de una instancia manipulada (como se indica en el gráfico), entonces la diferencia de segundo orden es (d - c) - (b - a). La diferencia media de segundo orden en cada celda se acumula sobre la cuadrícula y se centra. En la figura anterior, muchas celdas están vacías debido a la correlación. En el diagrama ALE, esto se puede visualizar con un cuadro atenuado u oscurecido. Alternativamente, puede reemplazar la estimación de ALE que falta de una celda vacía con la estimación de ALE de la celda no vacía más cercana. Dado que las estimaciones de ALE para dos características solo muestran el efecto de segundo orden de las características, la interpretación requiere atención especial. El efecto de segundo orden es el efecto de interacción adicional de las características después de haber contabilizado los efectos principales de las características. Supongamos que dos características no interactúan, pero cada una tiene un efecto lineal sobre el resultado predicho. En el gráfico 1D ALE para cada entidad, veríamos una línea recta como la curva ALE estimada. Pero cuando graficamos las estimaciones 2D ALE, deberían estar cerca de cero, porque el efecto de segundo orden es solo el efecto adicional de la interacción. Las gráficas ALE y PD son diferentes en este sentido: Los PDP siempre muestran el efecto total, los gráficos ALE muestran el efecto de primer o segundo orden. Estas son decisiones de diseño que no dependen de las matemáticas subyacentes. Puede restar los efectos de orden inferior en un gráfico de dependencia parcial para obtener los efectos puros principales o de segundo orden o puede obtener una estimación del total de gráficos de ALE evitando restar los efectos de orden inferior. Los efectos locales acumulados también podrían calcularse para órdenes arbitrariamente más altas (interacciones de tres o más características), pero como se argumenta en el capítulo PDP, solo tiene sentido hasta dos características, porque las interacciones más altas no se pueden visualizar o incluso interpretado de manera significativa. ALE para características categóricas El método de efectos locales acumulados necesita, por definición, los valores de las características para tener un orden, porque el método acumula efectos en una determinada dirección. Las características categóricas no tienen ningún orden natural. Para calcular un gráfico ALE para una característica categórica, tenemos que crear o encontrar un pedido de alguna manera. El orden de las categorías influye en el cálculo e interpretación de los efectos locales acumulados. Una solución es ordenar las categorías según su similitud en función de las otras características. La distancia entre dos categorías es la suma de las distancias de cada entidad. La distancia en función de las características compara la distribución acumulativa en ambas categorías, también llamada distancia de Kolmogorov-Smirnov (para características numéricas) o las tablas de frecuencias relativas (para características categóricas). Una vez que tenemos las distancias entre todas las categorías, usamos escalas multidimensionales para reducir la matriz de distancia a una medida de distancia unidimensional. Esto nos da un orden de similitud basado en las categorías. Para aclarar esto un poco, aquí hay un ejemplo: Supongamos que tenemos las dos características categóricas estación y clima y una característica numérica temperatura. Para la primera característica categórica (estación) queremos calcular los ALE. La variable tiene las categorías primavera, verano, otoño, invierno. Comenzamos a calcular la distancia entre las categorías primavera y verano. La distancia es la suma de distancias sobre la temperatura y el clima de las características. Para la temperatura, tomamos todas las instancias con la temporada primavera, calculamos la función empírica de distribución acumulativa y hacemos lo mismo para las instancias con la temporada verano y medimos su distancia con la estadística de Kolmogorov-Smirnov. Para la característica del clima, calculamos para todas las instancias de primavera las probabilidades para cada tipo de clima, hacemos lo mismo para las instancias de verano y sumamos las distancias absolutas en la distribución de probabilidad. Si primavera y verano tienen temperaturas y clima muy diferentes, la distancia total por categoría es grande. Repetimos el procedimiento con los otros pares estacionales y reducimos la matriz de distancias resultante a una sola dimensión mediante escalamiento multidimensional. 5.3.4 Ejemplos Veamos las gráficas ALE en acción. He construido un escenario en el que los PDP fallan. El escenario consiste en un modelo de predicción y dos características fuertemente correlacionadas. El modelo de predicción es principalmente un modelo de regresión lineal, pero hace algo extraño en una combinación de las dos características para las cuales nunca hemos observado casos. FIGURA 5.14: Dos características y el resultado predicho. El modelo predice la suma de las dos características (fondo sombreado), con la excepción de que si x1 es mayor que 0.7 y x2 menor que 0.3, el modelo siempre predice 2. Esta área está lejos de la distribución de datos (nube de puntos) y no afecta el rendimiento del modelo y tampoco debería afectar su interpretación. ¿Es este un escenario realista y relevante en absoluto? Cuando entrenas un modelo, el algoritmo de aprendizaje minimiza la pérdida de las instancias de datos de entrenamiento existentes. Pueden ocurrir cosas extrañas fuera de la distribución de datos de entrenamiento, porque el modelo no está penalizado por hacer cosas extrañas en estas áreas. Salir de la distribución de datos se llama extrapolación, que también se puede utilizar para engañar a los modelos de aprendizaje automático, que se describe en el capítulo sobre ejemplos adversos. Vea en nuestro pequeño ejemplo cómo se comportan las gráficas de dependencia parcial en comparación con las gráficas ALE. FIGURA 5.15: Comparación de los efectos característicos calculados con PDP (fila superior) y ALE (fila inferior). Las estimaciones de PDP están influenciadas por el comportamiento extraño del modelo externo la distribución de datos (saltos pronunciados en las gráficas). Las gráficas ALE identifican correctamente que el modelo de aprendizaje automático tiene una relación lineal entre las características y la predicción, ignorando las áreas sin datos. Pero, ¿no es interesante ver que nuestro modelo se comporta de manera extraña en x1&gt;0.7 y x2&lt;0.3? Pues sí y no. Dado que estas son instancias de datos que pueden ser físicamente imposibles o al menos extremadamente improbables, generalmente es irrelevante analizar estas instancias. Pero si sospechas que tu distribución de prueba puede ser ligeramente diferente y algunas instancias están realmente en ese rango, entonces sería interesante incluir esta área en el cálculo de los efectos de características. Pero tiene que ser una decisión consciente incluir áreas donde aún no hemos observado datos y no debe ser un efecto secundario del método de elección como PDP. Si sospechas que el modelo se usará más tarde con datos distribuidos de manera diferente, te recomiendo usar gráficos ALE y simular la distribución de datos que esperas. En cuanto a un conjunto de datos real, pronostiquemos el número de bicicletas alquiladas según el clima y el día y verifiquemos si las gráficas ALE realmente funcionan tan bien como se prometió. Entrenamos un árbol de regresión para predecir el número de bicicletas alquiladas en un día determinado y utilizamos gráficas ALE para analizar cómo la temperatura, la humedad relativa y la velocidad del viento influyen en las predicciones. Veamos lo que dicen las gráficas ALE: FIGURA 5.16: Gráficos ALE para el modelo de predicción de bicicletas por temperatura, humedad y velocidad del viento. La temperatura tiene un efecto fuerte en la predicción. La predicción promedio crece con el incremento de la temperatura, pero decrece sobre los 25 grados Celsius. La humedad tiene un efecto negativo: sobre el 60%, la más alta humedad baja la predicción. La velocidad del vientono afecta la predicción demasiado. Veamos la correlación entre temperatura, humedad y velocidad del viento y todas las demás características. Dado que los datos también contienen características categóricas, no solo podemos usar el coeficiente de correlación de Pearson, que solo funciona si ambas características son numéricas. En cambio, entreno un modelo lineal para predecir, por ejemplo, la temperatura basada en una de las otras características como entrada. Luego mido cuánta varianza explica la otra característica en el modelo lineal y tomo la raíz cuadrada. Si la otra característica era numérica, entonces el resultado es igual al valor absoluto del coeficiente de correlación de Pearson estándar. Pero este enfoque basado en modelos de explicación por la varianza (también llamado ANOVA, que significa Análisis de varianza) funciona incluso si la otra característica es categórica. La medida explicada por la varianza se encuentra siempre entre 0 (sin asociación) y 1 (la temperatura puede predecirse perfectamente a partir de la otra característica). Calculamos la varianza explicada de temperatura, humedad y velocidad del viento con todas las demás características. Cuanto mayor sea la varianza explicada (correlación), más problemas (potenciales) con los gráficos de DP. La siguiente figura visualiza cuán fuertemente se correlacionan las características climáticas con otras características. FIGURA 5.17: La fuerza de la correlación entre temperatura, humedad y velocidad del viento con todas las características, medida como la cantidad de variación explicada, cuando entrenamos un modelo lineal con, por ejemplo, temperatura para predecir y estación como característica. Para la temperatura observamos, no sorprendentemente, una alta correlación con la estación y el mes. La humedad se correlaciona con la situación climática. Este análisis de correlación revela que podemos encontrar problemas con los gráficos de dependencia parcial, especialmente para la característica de temperatura. Bueno, compruébalo tú mismo: FIGURA 5.18: PDP para temperatura, humedad y velocidad del viento. En comparación con las gráficas ALE, las PDP muestran una disminución menor en el número previsto de bicicletas para alta temperatura o alta humedad. El PDP utiliza todas las instancias de datos para calcular el efecto de las altas temperaturas, incluso si son, por ejemplo, instancias con la temporada invierno. Las gráficas ALE son más confiables. A continuación, veamos los gráficos ALE en acción para una característica categórica. El mes es una característica categórica para la que queremos analizar el efecto sobre el número previsto de bicicletas. Podría decirse que los meses ya tienen un cierto orden (enero a diciembre), pero intentemos ver qué sucede si primero reordenamos las categorías por similitud y luego calculamos los efectos. Los meses se ordenan por la similitud de días de cada mes en función de otras características, como la temperatura o si es feriado. FIGURA 5.19: Gráfico ALE para el mes de la característica categórica. Los meses se ordenan por su similitud entre sí, según las distribuciones de las otras funciones por mes. Observamos que enero, marzo y abril, pero especialmente diciembre y noviembre, tienen un efecto menor en el número previsto de bicicletas alquiladas en comparación con los otros meses. Dado que muchas de las características están relacionadas con el clima, el orden de los meses refleja fuertemente cuán similar es el clima entre los meses. Todos los meses más fríos están en el lado izquierdo (febrero a abril) y los meses más cálidos en el lado derecho (octubre a agosto). Ten en cuenta que las características no meteorológicas también se han incluido en el cálculo de similitud, por ejemplo, la frecuencia relativa de vacaciones tiene el mismo peso que la temperatura para calcular la similitud entre los meses. A continuación, consideramos el efecto de segundo orden de la humedad y la temperatura en el número previsto de bicicletas. Recuerda que el efecto de segundo orden es el efecto de interacción adicional de las dos características y no incluye los efectos principales. Esto significa que, por ejemplo, no verás el efecto principal de que la alta humedad conduce a un menor número de bicicletas predichas en promedio en el diagrama ALE de segundo orden. FIGURA 5.20: Gráfico ALE para el efecto de segundo orden de la humedad y la temperatura en el número previsto de bicicletas alquiladas. El tono más claro indica un tono por encima del promedio y el más oscuro una predicción por debajo del promedio cuando los efectos principales ya se tienen en cuenta. La trama revela una interacción entre temperatura y humedad: el clima cálido y húmedo aumenta la predicción. En climas fríos y húmedos se muestra un efecto negativo adicional en el número de bicicletas predichas. Ten en cuenta que los dos efectos principales de la humedad y la temperatura indican que el número previsto de bicicletas disminuye en climas muy cálidos y húmedos. En climas cálidos y húmedos, el efecto combinado de temperatura y humedad no es, por lo tanto, la suma de los efectos principales, sino mayor que la suma. Para enfatizar la diferencia entre el efecto de segundo orden puro (el gráfico 2D ALE que acaba de ver) y el efecto total, veamos el gráfico de dependencia parcial. El PDP muestra el efecto total, que combina la predicción media, los dos efectos principales y el efecto de segundo orden (la interacción). FIGURA 5.21: PDP del efecto total de la temperatura y la humedad en el número previsto de bicicletas. La trama combina el efecto principal de cada una de las características y su interacción efecto, a diferencia del gráfico 2D-ALE que solo muestra la interacción. Si solo estás interesado en la interacción, debes mirar los efectos de segundo orden, ya que el efecto total mezcla los efectos principales en la trama. Pero si deseas conocer el efecto combinado de las características, debes mirar el efecto total (que muestra el PDP). Por ejemplo, si deseas conocer el número esperado de bicicletas a 30 grados Celsius y 80 por ciento de humedad, puedes leerlo directamente desde el PDP 2D. Si deseas leer lo mismo de los gráficos ALE, debes mirar tres gráficos: El gráfico ALE para temperatura, humedad y temperatura + humedad y también necesitas conocer la predicción media general. En un escenario donde dos características no tienen interacción, la gráfica de efecto total de las dos características podría ser engañosa porque probablemente muestra un paisaje complejo, lo que sugiere cierta interacción, pero es simplemente el producto de los dos efectos principales. El efecto de segundo orden mostraría inmediatamente que no hay interacción. Suficientes bicicletas por ahora, pasemos a una tarea de clasificación. Entrenamos un random forest para predecir la probabilidad de cáncer cervical en función de los factores de riesgo. Visualizamos los efectos locales acumulados para dos de las características: FIGURA 5.22: Gráfica ALE del efecto de la edad y los años con anticonceptivos hormonales en la probabilidad pronosticada de cáncer cervical. Para la característica de edad, la gráfica ALE muestra que la probabilidad pronosticada de cáncer es bajo en promedio hasta los 40 años y aumenta después de eso. El número de años con anticonceptivos hormonales se asocia con un mayor riesgo de cáncer previsto después de 8 años. A continuación, observamos la interacción entre el número de embarazos y la edad. FIGURA 5.23: Gráfico ALE del efecto de segundo orden del número de embarazos y la edad. La interpretación de la trama no es concluyente, ya que muestra lo que parece un sobreajuste. Por ejemplo, la gráfica muestra un comportamiento de modelo extraño a la edad de 18-20 años y más de 3 embarazos (aumento de hasta 5 puntos porcentuales en la probabilidad de cáncer). No hay muchas mujeres en los datos con esta constelación de edad y número de embarazos (los datos reales se muestran como puntos), por lo que el modelo no se penaliza severamente durante el entrenamiento por cometer errores para esas mujeres. 5.3.5 Ventajas Las gráficas ALE son imparciales, lo que significa que aún funcionan cuando las características están correlacionadas. Las gráficas de dependencia parcial fallan en este escenario porque marginaron sobre combinaciones de valores de características improbables o incluso físicamente imposibles. Los gráficos ALE son más rápidos de calcular que los PDP y se escalan con O(n), ya que el mayor número posible de intervalos es el número de instancias con un intervalo por instancia. El PDP requiere n veces el número de estimaciones de puntos de la cuadrícula. Para 20 puntos de cuadrícula, los PDP requieren 20 veces más predicciones que el gráfico ALE del caso más desfavorable donde se utilizan tantos intervalos como instancias. La interpretación de las gráficas ALE es clara: dependiendo de un valor dado, el efecto relativo de cambiar la característica en la predicción se puede leer de la gráfica ALE. Las gráficas ALE están centradas en cero. Esto hace que su interpretación sea agradable, porque el valor en cada punto de la curva ALE es la diferencia con la predicción media. El diagrama 2D ALE solo muestra la interacción: Si dos características no interactúan, la trama no muestra nada. En general, en la mayoría de las situaciones preferiría las gráficas ALE a las PDP, porque las características generalmente están correlacionadas en cierta medida. 5.3.6 Desventajas Las gráficas ALE pueden volverse un poco inestables (muchas subidas y bajadas pequeñas) con una gran cantidad de intervalos. En este caso, reducir el número de intervalos hace que las estimaciones sean más estables, pero también suaviza y oculta parte de la verdadera complejidad del modelo de predicción. No existe una solución perfecta para establecer el número de intervalos. Si el número es demasiado pequeño, los gráficos ALE pueden no ser muy precisos. Si el número es demasiado alto, la curva puede volverse inestable. A diferencia de los PDP, los gráficos ALE no están acompañados por curvas ICE. Para los PDP, las curvas ICE son excelentes porque pueden revelar heterogeneidad en el efecto de la característica, lo que significa que el efecto de una característica se ve diferente para los subconjuntos de datos. Para los gráficos ALE solo puede verificar por intervalo si el efecto es diferente entre las instancias, pero cada intervalo tiene instancias diferentes, por lo que no es lo mismo que las curvas ICE. Las estimaciones de ALE de segundo orden tienen una estabilidad variable en todo el espacio de características, que no se visualiza de ninguna manera. La razón de esto es que cada estimación de un efecto local en una celda utiliza un número diferente de instancias de datos. Como resultado, todas las estimaciones tienen una precisión diferente (pero siguen siendo las mejores estimaciones posibles). El problema existe en una versión menos severa para los gráficos ALE de efecto principal. El número de instancias es el mismo en todos los intervalos, gracias al uso de cuantiles como cuadrícula, pero en algunas áreas habrá muchos intervalos cortos y la curva ALE consistirá en muchas más estimaciones. Pero para intervalos largos, que pueden constituir una gran parte de toda la curva, hay comparativamente menos casos. Esto sucedió en la predicción ALE de cáncer de cuello uterino para la edad avanzada, por ejemplo. Las gráficas de efectos de segundo orden pueden ser un poco molestas de interpretar, ya que siempre debes tener en cuenta los efectos principales. Es tentador leer los mapas de calor como el efecto total de las dos características, pero es solo el efecto adicional de la interacción. El efecto puro de segundo orden es interesante para descubrir y explorar interacciones, pero para interpretar cómo se ve el efecto, creo que tiene más sentido integrar los efectos principales en la trama. La implementación de los gráficos ALE es mucho más compleja y menos intuitiva en comparación con los gráficos de dependencia parcial. Aunque los gráficos ALE no están sesgados en caso de características correlacionadas, la interpretación sigue siendo difícil cuando las características están fuertemente correlacionadas. Porque si tienen una correlación muy fuerte, solo tiene sentido analizar el efecto de cambiar ambas características juntas y no de forma aislada. Esta desventaja no es específica de las gráficas ALE, sino un problema general de características fuertemente correlacionadas. Si las características no están correlacionadas y el tiempo de cálculo no es un problema, los PDP son ligeramente preferibles porque son más fáciles de entender y se pueden trazar junto con las curvas ICE. La lista de desventajas se ha vuelto bastante larga, pero no te dejes engañar por la cantidad de palabras que uso: Como regla general: usa ALE en lugar de PDP. 5.3.7 Implementación y alternativas ¿Mencioné que los gráficos de dependencia parcial y las curvas de expectativas condicionales individuales son una alternativa? =) Que yo sepa, las gráficas ALE actualmente solo se implementan en R, en el paquete ALEPlot R creado por el inventor mismo, y en el paquete iml. Apley, Daniel W. Visualizing the effects of predictor variables in black box supervised learning models. arXiv preprint arXiv:1612.08468 (2016). "],["interacción.html", "5.4 Interacción de características", " 5.4 Interacción de características Cuando las características interactúan entre sí en un modelo de predicción, la predicción no puede expresarse como la suma de los efectos de cada característica, porque el efecto de una característica depende del valor de la otra característica. El predicado de Aristóteles El todo es mayor que la suma de sus partes se aplica en presencia de interacciones. 5.4.1 Interacción de características Si un modelo de aprendizaje automático realiza una predicción basada en dos características, podemos descomponer la predicción en cuatro términos: un término constante, un término para la primera característica, un término para la segunda característica y un término para la interacción entre las dos características. La interacción entre dos características es el cambio en la predicción que ocurre al variar las características después de considerar los efectos de las características individuales. Por ejemplo, un modelo predice el valor de una casa, utilizando el tamaño de la casa (grande o pequeña) y la ubicación (buena o mala) como características, lo que arroja cuatro posibles predicciones: Ubicación Tamaño Predicción buena grande 300,000 buena pequeña 200,000 mala grande 250,000 mala pequeña 150,000 Descomponemos la predicción del modelo en las siguientes partes: Un término constante (150,000), un efecto para la característica de tamaño (+100,000 si es grande; +0 si es pequeña) y un efecto para la ubicación (+50,000 si es bueno; +0 si es malo). Esta descomposición explica completamente las predicciones del modelo. No hay efecto de interacción, porque la predicción del modelo es una suma de los efectos de características individuales para el tamaño y la ubicación. Cuando se hace grande una casa pequeña, la predicción siempre aumenta en 100,000, independientemente de la ubicación. Además, la diferencia en la predicción entre una buena y una mala ubicación es de 50,000, independientemente del tamaño. Veamos ahora un ejemplo con interacción: Ubicación Tamaño Predicción buena grande 400,000 buena pequeña 200,000 mala grande 250,000 mala pequeña 150,000 Descomponemos la tabla de predicción en las siguientes partes: Un término constante (150,000), un efecto para la característica de tamaño (+100,000 si es grande, +0 si es pequeña) y un efecto para la ubicación (+50,000 si es bueno, +0 si es malo). Para esta tabla, necesitamos un término adicional para la interacción: +100,000 si la casa es grande y está en una buena ubicación. Esta es una interacción entre el tamaño y la ubicación, porque en este caso la diferencia en la predicción entre una casa grande y una pequeña depende de la ubicación. Una forma de estimar la intensidad de la interacción es medir qué parte de la variación de la predicción depende de la interacción de las características. Esta medida se llama estadístico H, introducida por Friedman y Popescu (2008) 31. 5.4.2 Teoría: estadístico H de Friedman Vamos a tratar dos casos: Primero, una medida de interacción bidireccional que nos dice si y en qué medida dos características en el modelo interactúan entre sí; segundo, una medida de interacción total que nos dice si, y en qué medida, una característica interactúa en el modelo con todas las demás características. En teoría, se pueden medir interacciones arbitrarias entre cualquier número de características, pero estos dos son los casos más interesantes. Si dos características no interactúan, podemos descomponer la función de dependencia parcial de la siguiente manera (suponiendo que las funciones de dependencia parcial estén centradas en cero): \\[PD_{jk}(x_j,x_k)=PD_j(x_j)+PD_k(x_k)\\] donde \\(PD_{jk}(x_j,x_k)\\) es la función de dependencia parcial bidireccional de ambas características y \\(PD_j(x_j)\\) y \\(PD_k(x_k)\\) las funciones de dependencia parcial de las características individuales. Del mismo modo, si una característica no tiene interacción con ninguna de las otras características, podemos expresar la función de predicción \\(\\hat{f}(x)\\) como una suma de funciones de dependencia parcial, donde el primer sumando depende solo de j y el segundo en todas las demás funciones excepto j: \\[\\hat{f}(x)=PD_j(x_j)+PD_{-j}(x_{-j})\\] donde \\(PD_{-j}(x_{-j})\\) es la función de dependencia parcial que depende de todas las características excepto la característica j-ésima. Esta descomposición expresa la función de dependencia parcial (o predicción completa) sin interacciones (entre las características j y k, o respectivamente j y todas las demás características). En el siguiente paso, medimos la diferencia entre la función de dependencia parcial observada y la descompuesta sin interacciones. Calculamos la varianza de la salida de la dependencia parcial (para medir la interacción entre dos características) o de la función completa (para medir la interacción entre una característica y todas las demás características). La cantidad de la varianza explicada por la interacción (diferencia entre PD observada y sin interacción) se usa como estadística de fuerza de interacción. El estadístico es 0 si no hay interacción y 1 si toda la varianza de \\(PD_{jk}\\) o \\(\\hat{f}\\) se explica por la suma de las funciones de dependencia parcial. Un estadístico 1 entre dos características significa que cada función de PD es constante y el efecto en la predicción solo se produce a través de la interacción. El estadístico H también puede ser mayor que 1, lo cual es más difícil de interpretar. Esto puede suceder cuando la varianza de la interacción bidireccional es mayor que la varianza de la gráfica de dependencia parcial bidimensional. Matemáticamente, el estadístico H propuesto por Friedman y Popescu para la interacción entre la característica j y k es: \\[H^2_{jk}=\\sum_{i=1}^n\\left[PD_{jk}(x_{j}^{(i)},x_k^{(i)})-PD_j(x_j^{(i)})-PD_k(x_{k}^{(i)})\\right]^2/\\sum_{i=1}^n{PD}^2_{jk}(x_j^{(i)},x_k^{(i)})\\] Lo mismo se aplica para medir si una característica j interactúa con cualquier otra característica: \\[H^2_{j}=\\sum_{i=1}^n\\left[\\hat{f}(x^{(i)})-PD_j(x_j^{(i)})-PD_{-j}(x_{-j}^{(i)})\\right]^2/\\sum_{i=1}^n\\hat{f}^2(x^{(i)})\\] El estadístico H es costoso de evaluar, ya que itera sobre todos los puntos de datos y en cada punto se debe evaluar la dependencia parcial, que a su vez se realiza con todos los n puntos de datos. En el peor de los casos, necesitamos 2n2 llamadas a la función de predicción de modelos de aprendizaje automático para calcular el estadístico H bidireccional (j vs. k) y 3n2 para el estadístico H total (j vs. todos) Para acelerar el cálculo, podemos tomar muestras de los n puntos de datos. Esto tiene la desventaja de aumentar la varianza de las estimaciones de dependencia parcial, lo que hace que el estadístico H sea inestable. Entonces, si estás utilizando el muestreo para reducir la carga computacional, asegúrate de muestrear suficientes puntos de datos. Friedman y Popescu también proponen una estadística de prueba para evaluar si el estadístico H difiere significativamente de cero. La hipótesis nula es la ausencia de interacción. Para generar el estadístico de interacción bajo la hipótesis nula, debes poder ajustar el modelo para que no tenga interacción entre la característica j y k o todos los demás. Esto no es posible para todos los tipos de modelos. Por lo tanto, esta prueba es específica del modelo, no es independiente del modelo y, como tal, no se trata aquí. La estadística de fuerza de interacción también se puede aplicar en una configuración de clasificación si la predicción es una probabilidad. 5.4.3 Ejemplos ¡Veamos qué aspecto tienen las interacciones en la práctica! Medimos la fuerza de interacción de las características en una máquina de vectores de soporte que predice el número de bicicletas alquiladas en función del clima y las características de calendario. La siguiente gráfica muestra la característica de interacción H-estadística: FIGURA 5.24: La fuerza de interacción (estadístico H) para cada característica con todas las demás características para una máquina de vectores de soporte que predice el alquiler de bicicletas. En general, los efectos de interacción entre las características son muy débiles (menos del 10% de la varianza explicada por característica). En el siguiente ejemplo, calculamos la estadística de interacción para un problema de clasificación. Analizamos las interacciones entre características en un random forest entrenado para predecir cáncer cervical, dados algunos factores de riesgo. FIGURA 5.25: La fuerza de interacción (estadístico H) para cada característica con todas las demás características para un random forest que predice la probabilidad de cáncer cervical. Los años en los anticonceptivos hormonales tienen el mayor efecto de interacción relativa con todas las demás características, seguido del número de embarazos. Después de observar las interacciones de las características de cada característica con todas las demás características, podemos seleccionar una de las características y profundizar en todas las interacciones bidireccionales entre la característica seleccionada y las otras características. FIGURA 5.26: Las fuerzas de interacción bidimensional (Estadístico H) entre el número de embarazos y cada característica. Hay una interacción fuerte con la edad. 5.4.4 Ventajas La interacción tiene una teoría subyacente a través de la descomposición de la dependencia parcial. El estadístico H tiene una interpretación significativa: La interacción se define como la parte de varianza que se explica por la interacción. Dado que el estadístico es adimensional, es comparable entre las características e incluso entre los modelos. El estadístico detecta todo tipo de interacciones, independientemente de su forma particular. Con el estadístico H también es posible analizar interacciones superiores arbitrarias, como la fuerza de interacción entre 3 o más características. 5.4.5 Desventajas Lo primero que notarás: El estadístico H de interacción requiere mucho tiempo para computarse, porque es computacionalmente costoso. El cálculo implica estimar distribuciones marginales. Estas estimaciones también tienen una cierta variación si no usamos todos los puntos de datos. Esto significa que a medida que muestreamos puntos, las estimaciones también varían de una ejecución a otra y los resultados pueden ser inestables. Recomiendo repetir el cálculo del estadístico H varias veces para ver si tiene suficientes datos para obtener un resultado estable. No está claro si una interacción es significativamente mayor que 0. Tendríamos que realizar una prueba estadística, pero esta prueba no está (todavía) disponible en una versión independiente del modelo. Con respecto al problema de la prueba, es difícil decir cuándo el estadístico H es lo suficientemente grande como para considerar una interacción fuerte. Además, los estadísticos H pueden ser mayores que 1, lo que dificulta la interpretación. El estadístico H nos dice la fuerza de las interacciones, pero no nos dice cómo se ven las interacciones. Para eso están las gráficos de dependencia parcial. Un flujo de trabajo significativo es medir las fortalezas de interacción y luego crear diagramas de dependencia parcial 2D para las interacciones que le interesan. El estadístico H no se puede usar de manera significativa si las entradas son píxeles. Por lo tanto, la técnica no es útil para el clasificador de imágenes. El estadístico de interacción funciona bajo el supuesto de que podemos barajar características de forma independiente. Si las características se correlacionan fuertemente, el supuesto se viola e integramos sobre combinaciones de características que son muy poco probables en la realidad. Ese es el mismo problema que tienen las gráficas de dependencia parcial. No se puede decir en general si conduce a una sobreestimación o subestimación. A veces los resultados son extraños y para simulaciones pequeñas no producen los resultados esperados. Pero esto es más una observación anecdótica. 5.4.6 Implementaciones Para los ejemplos de este libro, utilicé el paquete R iml, que está disponible en CRAN y la versión de desarrollo en Github. Hay otras implementaciones, que se centran en modelos específicos: El paquete R pre implementa RuleFit y H-statistic. El paquete R gbm implementa modelos potenciados por gradiente y estadísticas H. 5.4.7 Alternativas El estadístico H no es la única forma de medir las interacciones: Redes de interacción variable (VIN) de Hooker (2004)32 es un enfoque que descompone la función de predicción en efectos principales e interacciones de características. Las interacciones entre las características se visualizan como una red. Lamentablemente, no hay software disponible todavía. Dependencia parcial basada en la interacción de características de Greenwell et. al (2018)33 mide la interacción entre dos características. Este enfoque mide la importancia de la característica (definida como la varianza de la función de dependencia parcial) de una característica condicional en diferentes puntos fijos de la otra característica. Si la varianza es alta, entonces las características interactúan entre sí, si es cero, no interactúan. El paquete R correspondiente vip está disponible en Github. El paquete también cubre gráficos de dependencia parcial e importancia de la característica de permutación. Friedman, Jerome H, and Bogdan E Popescu. Predictive learning via rule ensembles. The Annals of Applied Statistics. JSTOR, 91654. (2008). Hooker, Giles. Discovering additive structure in black box functions. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004). Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. A simple and effective model-based variable importance measure. arXiv preprint arXiv:1805.04755 (2018). "],["importanciadecaracteristicas.html", "5.5 Importancia de la característica de permutación", " 5.5 Importancia de la característica de permutación La importancia de la característica de permutación mide el aumento en el error de predicción del modelo después de permutar los valores de la característica, lo que rompe la relación entre la característica y el resultado real. 5.5.1 Teoría El concepto es realmente sencillo: Medimos la importancia de una característica calculando el aumento en el error de predicción del modelo después de permutar la característica. Una característica es importante si cambiar sus valores aumenta el error del modelo, porque en este caso el modelo se basó en la característica para la predicción. Una característica es no importante si cambiar sus valores deja el error del modelo sin cambios, porque en este caso el modelo ignoró la característica para la predicción. Breiman (2001)34 introdujo la medida de importancia de la característica de permutación para random forest. En base a esta idea, Fisher, Rudin y Dominici (2018)35 propusieron una versión independiente del modelo de la importancia de la característica y la llamaron dependencia del modelo. También introdujeron ideas más avanzadas sobre la importancia de las características, por ejemplo, una versión (específica del modelo) que tiene en cuenta que muchos modelos de predicción pueden predecir bien los datos. Vale la pena leer su artículo. El algoritmo de importancia de la característica de permutación basado en Fisher, Rudin y Dominici (2018): Entrada: Modelo entrenado f, matriz de características X, vector objetivo y, medida de error L(y,f). Estima el error del modelo original eorig = L(y, f(X)) (por ejemplo, error cuadrático medio) Para cada característica j = 1, , p: Genera la matriz de características Xperm permutando la característica j en los datos X. Esto rompe la asociación entre la característica j y el resultado verdadero y. Estima el error eperm = L(Y,f(Xperm)) en función de las predicciones de los datos permutados. Calcula la importancia de la característica de permutación FIj = eperm/eorig. Alternativamente, se puede usar la diferencia: FIj = eperm-eorig Ordena las características por FI descendente. Fisher, Rudin y Dominici (2018) sugieren en su artículo dividir el conjunto de datos a la mitad e intercambiar los valores de la característica j de las dos mitades en lugar de permutar la característica j. Esto es exactamente lo mismo que permutar la función j, si lo piensas. Si deseas una estimación más precisa, puedes estimar el error de permutar la característica j emparejando cada instancia con el valor de la característica j de cada otra instancia (excepto consigo misma). Esto te proporciona un conjunto de datos de tamaño n(n-1) para estimar el error de permutación, y requiere una gran cantidad de tiempo de cálculo. Solo puedo recomendar el uso del método n(n-1) si realmente quieres obtener estimaciones extremadamente precisas. 5.5.2 ¿Debo calcular la importancia de los datos de entrenamiento o prueba? tl;dr: no tengo una respuesta definitiva. La respuesta a la pregunta sobre los datos de entrenamiento o prueba toca la pregunta fundamental de qué importancia tiene la característica. La mejor manera de comprender la diferencia entre la importancia de la característica basada en el entrenamiento frente a los datos de la prueba es un ejemplo extremo. Entrené a una SVM para predecir un resultado objetivo aleatorio continuo con 50 características aleatorias (200 instancias). Por aleatorio quiero decir que el resultado objetivo es independiente de las 50 características. Esto es como predecir la temperatura de mañana dados los últimos números de lotería. Si el modelo aprende alguna relación, entonces se sobreajusta. Y, de hecho, el SVM superó los datos de entrenamiento. El error absoluto medio (corto: MAE) para los datos de entrenamiento es 0.29 y para los datos de prueba 0.82, que también es el error del mejor modelo posible que siempre predice el resultado medio de 0 (MAE de 0.78). En otras palabras, el modelo SVM es basura. ¿Qué valores para la importancia de la característica esperarías para las 50 características de este SVM sobreajustado? ¿Cero porque ninguna de las características contribuye a mejorar el rendimiento en datos de prueba no vistos? ¿O deberían las importancias reflejar cuánto depende el modelo de cada una de las características, independientemente de si las relaciones aprendidas se generalizan a datos no vistos? Echemos un vistazo a cómo difieren las distribuciones de las características importantes para los datos de entrenamiento y prueba. FIGURA 5.27: Distribuciones de valores de importancia de características por tipo de datos. Un SVM fue entrenado en un conjunto de datos de regresión con 50 características aleatorias y 200 instancias. en los datos de entrenamiento muestra muchas características importantes. Calculadas en datos de prueba no vistos, las importancias de las características son cercanas a una proporción de uno (= sin importancia). No me queda claro cuál de los dos resultados es más deseable. Así que intentaré presentar un caso para ambas versiones y dejarte que decidas. El caso de los datos de prueba Este es un caso simple: Las estimaciones de error del modelo basadas en datos de entrenamiento son basura -&gt; la importancia de la característica depende de las estimaciones de error del modelo -&gt; la importancia de la característica basada en datos de entrenamiento es basura. Es una de las primeras cosas que aprendes en el aprendizaje automático: Si mides el error del modelo (o el rendimiento) en los mismos datos en los que se formó el modelo, la medición suele ser demasiado optimista, lo que significa que el modelo parece funcionar mucho mejor de lo que realmente lo hace. Y dado que la importancia de la característica de permutación se basa en mediciones del error del modelo, debemos usar datos de prueba no vistos. La importancia de la característica basada en los datos de entrenamiento nos hace creer erróneamente que las características son importantes para las predicciones, cuando en realidad el modelo simplemente estaba sobreajustado y las características no eran importantes en absoluto. El caso de los datos de entrenamiento Los argumentos para usar datos de entrenamiento son algo más difíciles de formular, pero en mi humilde opinión son tan convincentes como los argumentos para usar datos de prueba. Echamos otro vistazo a nuestra basura SVM. Según los datos de entrenamiento, la característica más importante era X42. Veamos un diagrama de dependencia parcial de la característica X42. El gráfico de dependencia parcial muestra cómo cambia la salida del modelo en función de los cambios de la característica y no se basa en el error de generalización. No importa si el PDP se calcula con datos de entrenamiento o prueba. FIGURA 5.28: PDP de la característicaX42, que es la característica más importante según la importancia de la característica en función de los datos de entrenamiento. El gráfico muestra cómo el SVM depende de esto función para hacer predicciones La gráfica muestra claramente que el SVM ha aprendido a confiar en la función X42 para sus predicciones, pero de acuerdo con la importancia de la función basada en los datos de prueba (1), no es importante. Según los datos de entrenamiento, la importancia es 1.19, lo que refleja que el modelo ha aprendido a usar esta función. La importancia de la característica basada en los datos de entrenamiento nos dice qué características son importantes para el modelo en el sentido de que depende de ellas para hacer predicciones. Como parte del caso para usar datos de entrenamiento, me gustaría presentar un argumento en contra de los datos de prueba. En la práctica, deseas utilizar todos tus datos para entrenar tu modelo para obtener el mejor modelo posible al final. Esto significa que no quedan datos de prueba no utilizados para calcular la importancia de la característica. Tienes el mismo problema cuando deseas estimar el error de generalización de su modelo. Si usaras la validación cruzada (anidada) para la estimación de la importancia de la característica, tendrías el problema de que la importancia de la característica no se calcula en el modelo final con todos los datos, sino en modelos con subconjuntos de datos que podrían comportarse de manera diferente. Al final, debes decidir si deseas saber cuánto depende el modelo de cada característica para hacer predicciones (-&gt; datos de entrenamiento) o cuánto contribuye la característica al rendimiento del modelo en datos no vistos (-&gt; datos de prueba) Que yo sepa, no hay ninguna investigación que aborde la cuestión de los datos de entrenamiento versus los datos de las pruebas. Se requerirá un examen más exhaustivo que mi ejemplo basura-SVM. Necesitamos más investigación y más experiencia con estas herramientas para obtener una mejor comprensión. A continuación, veremos algunos ejemplos. Basé el cálculo de importancia en los datos de entrenamiento, porque tenía que elegir uno y usar los datos de entrenamiento necesitaba algunas líneas menos código. 5.5.3 Ejemplo e interpretación Muestro ejemplos de clasificación y regresión. Cáncer de cuello uterino (clasificación) Ajustamos un random forest para predecir cáncer cervical. Medimos el aumento de error en 1-AUC (1 menos el área bajo la curva ROC). Las características asociadas con un aumento del error del modelo por un factor de 1 (= sin cambio) no fueron importantes para predecir el cáncer cervical. FIGURA 5.29: La importancia de cada una de las características prediciendo cáncer cervical con un random forest. La más importante fue Age. Permutar Age resulta en un incremento de 1-AUC a razón de 6.49 La característica con mayor importancia fue Age asociada con un aumento de error de 6.49 después de la permutación. Bicicleta compartida (regresión) Ajustamos un modelo de SVM para predecir el número de bicicletas alquiladas, dadas las condiciones climáticas y la información del calendario. Como medida de error usamos el error absoluto medio. FIGURA 5.30: La importancia para cada una de las características en la predicción de la bicicleta cuenta con una máquina de vectores de soporte. La característica más importante fue temp, la menos importante fue holiday. 5.5.4 Ventajas Buena interpretación: la importancia de la característica es el aumento en el error del modelo cuando se destruye la información de la característica. La importancia de la característica proporciona una visión global altamente comprimida sobre el comportamiento del modelo. Un aspecto positivo del uso de la relación de error en lugar de la diferencia de error es que las mediciones de importancia de la característica son comparables entre diferentes problemas. La medida de importancia automáticamente tiene en cuenta todas las interacciones con otras características. Al permutar la función, también destruye los efectos de interacción con otras características. Esto significa que la importancia de la característica de permutación tiene en cuenta tanto el efecto de la característica principal como los efectos de interacción en el rendimiento del modelo. Esto también es una desventaja porque la importancia de la interacción entre dos características está incluida en las mediciones de importancia de ambas características. Esto significa que las características importantes no se suman a la caída total en el rendimiento, pero la suma es mayor. Solo si no hay interacción entre las características, como en un modelo lineal, las importancias se suman aproximadamente. La importancia de la característica de permutación no requiere volver a entrenar el modelo. Algunos otros métodos sugieren eliminar una función, volver a entrenar el modelo y luego comparar el error del modelo. Dado que el reciclaje de un modelo de aprendizaje automático puede llevar mucho tiempo, solo permutar una función puede ahorrar mucho tiempo. Los métodos de importancia que vuelven a entrenar el modelo con un subconjunto de características parecen intuitivos a primera vista, pero el modelo con los datos reducidos no tiene sentido para la importancia de la característica. Estamos interesados en la importancia de las características de un modelo fijo. Volver a entrenar con un conjunto de datos reducido crea un modelo diferente al que nos interesa. Supón que entrenas un modelo lineal disperso (con lasso) con un número fijo de características con un peso distinto de cero. El conjunto de datos tiene 100 características, estableces el número de pesos distintos de cero a 5. Analizas la importancia de una de las características que tienen un peso distinto de cero. Eliminas la función y vuelves a entrenar el modelo. El rendimiento del modelo sigue siendo el mismo porque otra característica igualmente buena obtiene un peso distinto de cero y tu conclusión sería que la característica no era importante. Otro ejemplo: El modelo es un árbol de decisión y analizamos la importancia de la función elegida como la primera división. Elimina la función y vuelves a entrenar el modelo. Dado que se elige otra característica como la primera división, todo el árbol puede ser muy diferente, lo que significa que comparamos las tasas de error de árboles (potencialmente) completamente diferentes para decidir qué tan importante es esa característica para uno de los árboles. 5.5.5 Desventajas No está muy claro si debe utilizar los datos de entrenamiento o prueba para calcular la importancia de la función. La importancia de la característica de permutación está vinculada al error del modelo. Esto no es inherentemente malo, pero en algunos casos no es lo que necesitas. En algunos casos, es posible que prefieras saber cuánto varía la salida del modelo para una característica sin tener en cuenta lo que significa para el rendimiento. Por ejemplo, deseas averiguar qué tan robusta es la salida de tu modelo cuando alguien manipula las características. En este caso, no le interesaría cuánto disminuye el rendimiento del modelo cuando se permuta una característica, sino cuánto explica cada característica la variación de salida del modelo. La varianza del modelo (explicada por las características) y la importancia de la característica se correlacionan fuertemente cuando el modelo se generaliza bien (es decir, no se sobreajusta). Necesitas acceso al verdadero resultado. Si alguien solo te proporciona el modelo y los datos no etiquetados, pero no el resultado real, no puedes calcular la importancia de la característica de permutación. La importancia de la característica de permutación depende de barajar la característica, lo que agrega aleatoriedad a la medición. Cuando se repite la permutación, los resultados pueden variar mucho. Repetir la permutación y promediar las medidas de importancia sobre las repeticiones estabiliza la medida, pero aumenta el tiempo de cálculo. Si las características están correlacionadas, la importancia de la característica de permutación puede estar sesgada por instancias de datos poco realistas. El problema es el mismo que con gráficos de dependencia parcial: La permutación de características produce instancias de datos poco probables cuando dos o más características están correlacionadas. Cuando se correlacionan positivamente (como la altura y el peso de una persona) y barajo una de las características, creo nuevas instancias que son poco probables o incluso físicamente imposibles (por ejemplo, una persona de 2 metros con un peso de 30 kg), pero uso estas nuevas instancias para medir la importancia. En otras palabras, por la importancia de la característica de permutación de una característica correlacionada, consideramos cuánto disminuye el rendimiento del modelo cuando intercambiamos la característica con valores que nunca observaríamos en la realidad. Comprueba si las características están fuertemente correlacionadas y ten cuidado con la interpretación de la importancia de la característica si lo están. Otra dificultad: Agregar una función correlacionada puede disminuir la importancia de la función asociada al dividir la importancia entre ambas características. Permíteme darte un ejemplo de lo que quiero decir con dividir la importancia de la característica: Queremos predecir la probabilidad de lluvia y usar la temperatura a las 8:00 a.m. del día anterior como una característica junto con otras características no correlacionadas. Entreno un random forest y resulta que la temperatura es la característica más importante y todo está bien y duermo bien la noche siguiente. Ahora imagina otro escenario en el que también incluyo la temperatura a las 9:00 a.m. como una característica que está fuertemente correlacionada con la temperatura a las 8:00 a.m. La temperatura a las 9:00 a.m. no me da mucha información adicional si ya conozco la temperatura a las 8:00 a.m. Pero tener más características siempre es bueno, ¿verdad? Entreno un random forest con las dos características de temperatura y las características no correlacionadas. Algunos de los árboles en el random forest recogen la temperatura de las 8:00 a.m., otros la temperatura de las 9:00 a.m., otra vez ambos y otra vez ninguno. Las dos características de temperatura juntas tienen un poco más importancia que la característica de temperatura única anterior, pero en lugar de estar en la parte superior de la lista de características importantes, cada temperatura ahora está en algún lugar en el medio. Al introducir una característica correlacionada, pateé la característica más importante desde la parte superior de la escala de importancia hasta la mediocridad. Por un lado, esto está bien, porque simplemente refleja el comportamiento del modelo de aprendizaje automático subyacente, aquí el random forest. La temperatura de las 8:00 a.m. simplemente se ha vuelto menos importante porque el modelo ahora puede confiar también en la medición a las 9:00 a.m. Por otro lado, hace que la interpretación de la importancia de la característica sea considerablemente más difícil. Imagina que deseas verificar las características de los errores de medición. La verificación es costosa, por lo que decides verificar solo las 3 características principales más importantes. En el primer caso verificarías la temperatura, en el segundo caso no incluirías ninguna característica de temperatura solo porque ahora comparten la importancia. Aunque los valores de importancia pueden tener sentido en el nivel de comportamiento del modelo, es confuso si hay características correlacionadas. 5.5.6 Software y alternativas El paquete iml R se utilizó para los ejemplos. Los paquetes R DALEX yvip, así como la biblioteca Python alibi, también implementan la importancia de la característica de permutación independiente del modelo. Un algoritmo llamado PIMP adapta el algoritmo de importancia de la característica para proporcionar valores p para las importancias. Breiman, Leo. Random Forests. Machine Learning 45 (1). Springer: 5-32 (2001). Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. Model Class Reliance: Variable importance measures for any machine learning model class, from the Rashomon perspective. http://arxiv.org/abs/1801.01489 (2018). "],["global.html", "5.6 Sustituto global", " 5.6 Sustituto global Un modelo sustituto global es un modelo interpretable que está entrenado para aproximar las predicciones de un modelo de caja negra. Podemos sacar conclusiones sobre el modelo de caja negra interpretando el modelo sustituto. ¡Resolvemos la interpretación del aprendizaje automático usando más aprendizaje automático! 5.6.1 Teoría Los modelos sustitutos también se utilizan en ingeniería: Si un resultado de interés es costoso, requiere mucho tiempo o es difícil de medir (por ejemplo, porque proviene de una compleja simulación por computadora), se puede utilizar un modelo sustituto barato y rápido del resultado. La diferencia entre los modelos sustitutos utilizados en ingeniería y en el aprendizaje automático interpretable es que el modelo subyacente es un modelo de aprendizaje automático (no una simulación) y que el modelo sustituto debe ser interpretable. El propósito de los modelos sustitutos (interpretables) es aproximar las predicciones del modelo subyacente con la mayor precisión posible y ser interpretables al mismo tiempo. La idea de modelos sustitutos se puede encontrar bajo diferentes nombres: Modelo de aproximación, metamodelo, modelo de superficie de respuesta, emulador,  Sobre la teoría: En realidad, no se necesita mucha teoría para comprender los modelos sustitutos. Queremos aproximar nuestra función de predicción de caja negra f lo más cerca posible con la función de predicción de modelo sustituto g, bajo la restricción de que g es interpretable. Para la función g se puede utilizar cualquier modelo interpretable, por ejemplo, del capítulo de modelos interpretables. Por ejemplo un modelo lineal: \\[g(x)=\\beta_0+\\beta_1{}x_1{}+\\ldots+\\beta_p{}x_p\\] O un árbol de decisión: \\[g(x)=\\sum_{m=1}^Mc_m{}I\\{x\\in{}R_m\\}\\] El entrenamiento de un modelo sustituto es un método independiente del modelo, ya que no requiere ninguna información sobre el funcionamiento interno del modelo de caja negra, solo es necesario el acceso a los datos y la función de predicción. Si el modelo de aprendizaje automático subyacente se reemplazó por otro, aún podrías usar el método sustituto. La elección del tipo de modelo de caja negra y del tipo de modelo sustituto está desacoplada. Realiza los siguientes pasos para obtener un modelo sustituto: Selecciona un conjunto de datos X. Este puede ser el mismo conjunto de datos que se utilizó para entrenar el modelo de caja negra o un nuevo conjunto de datos de la misma distribución. Incluso podrías seleccionar un subconjunto de datos o una cuadrícula de puntos, dependiendo de tu aplicación. Para el conjunto de datos X seleccionado, obtén las predicciones del modelo de caja negra. Selecciona un tipo de modelo interpretable (modelo lineal, árbol de decisión, ). Entrena el modelo interpretable en el conjunto de datos X y sus predicciones. ¡Felicidades! Ahora tienes un modelo sustituto. Mide qué tan bien el modelo sustituto replica las predicciones del modelo de caja negra. Interpreta el modelo sustituto. Puedes encontrar enfoques para los modelos sustitutos que tienen algunos pasos adicionales o difieren un poco, pero la idea general suele ser como se describe aquí. Una forma de medir qué tan bien el sustituto replica el modelo de caja negra es la medida de R cuadrado: \\[R^2=1-\\frac{SSE}{SST}=1-\\frac{\\sum_{i=1}^n(\\hat{y}_*^{(i)}-\\hat{y}^{(i)})^2}{\\sum_{i=1}^n(\\hat{y}^{(i)}-\\bar{\\hat{y}})^2}\\] donde \\(\\hat{y}_*^{(i)}\\) es la predicción para la i-ésima instancia del modelo sustituto, \\(\\hat{y}^{(i)}\\) la predicción del modelo de caja negra y \\(\\bar{\\hat{y}}\\) la media de las predicciones del modelo de caja negra. SSE significa error de suma de cuadrados y SST para total de suma de cuadrados. La medida de R cuadrado se puede interpretar como el porcentaje de varianza que captura el modelo sustituto. Si R-cuadrado está cerca de 1 (= SSE bajo), entonces el modelo interpretable se aproxima muy bien al comportamiento del modelo de caja negra. Si el modelo interpretable está muy cerca, es posible que desee reemplazar el modelo complejo con el modelo interpretable. Si el R cuadrado está cerca de 0 (= SSE alto), entonces el modelo interpretable no puede explicar el modelo de caja negra. Ten en cuenta que no hemos hablado sobre el rendimiento del modelo del modelo de caja negra subyacente, es decir, qué tan bueno o malo se desempeña al predecir el resultado real. El rendimiento del modelo de caja negra no juega un papel en el entrenamiento del modelo sustituto. La interpretación del modelo sustituto sigue siendo válida porque hace declaraciones sobre el modelo y no sobre el mundo real. Pero, por supuesto, la interpretación del modelo sustituto se vuelve irrelevante si el modelo de caja negra es malo, porque entonces el modelo de caja negra en sí es irrelevante. También podríamos construir un modelo sustituto basado en un subconjunto de los datos originales o volver a ponderar las instancias. De esta manera, cambiamos la distribución de la entrada del modelo sustituto, lo que cambia el enfoque de la interpretación (entonces ya no es realmente global). Si ponderamos los datos localmente por una instancia específica de los datos (cuanto más cercanas sean las instancias a la instancia seleccionada, mayor será su peso), obtendremos un modelo sustituto local que puede explicar la predicción individual de la instancia. Lee más sobre modelos locales en el siguiente capítulo. 5.6.2 Ejemplo Para demostrar los modelos sustitutos, consideramos un problema de clasificación, predecimos la probabilidad de cáncer cervical con un random forest. Nuevamente, entrenamos un árbol de decisión con el conjunto de datos original, pero con la predicción del random forest como resultado, en lugar de las clases reales (sanas versus cáncer) de los datos. FIGURA 5.31: Los nodos terminales de un árbol sustituto que se aproximan a las predicciones de un random forest entrenado en el conjunto de datos de cáncer cervical. En los nodos muestra la frecuencia de las clasificaciones de los modelos de caja negra en los nodos. El modelo sustituto tiene un R cuadrado (explica la varianza) de 0.2, lo que significa que no se aproxima bien al random forest y no debemos sobreinterpretar el árbol cuando sacar conclusiones sobre el modelo complejo. Nota del traductor: la versión original ejemplifica además con un problema de regresión que no se incluye en la traducción. 5.6.3 Ventajas El método del modelo sustituto es flexible: Se puede usar cualquier modelo del capítulo de modelos interpretables. Esto también significa que puedes intercambiar no solo el modelo interpretable, sino también el modelo de caja negra subyacente. Supón que crea un modelo complejo y lo explica a los diferentes equipos de tu empresa. Un equipo está familiarizado con los modelos lineales, el otro equipo puede entender los árboles de decisión. Puedes entrenar dos modelos sustitutos (modelo lineal y árbol de decisión) para el modelo de caja negra original y ofrecer dos tipos de explicaciones. Si encuentras un modelo de caja negra con mejor rendimiento, no tienes que cambiar tu método de interpretación, ya que puedes usar la misma clase de modelos sustitutos. Yo diría que el enfoque es muy intuitivo y directo. Esto significa que es fácil de implementar, pero también fácil de explicar a personas que no están familiarizadas con la ciencia de datos o el aprendizaje automático. Con la medida de R cuadrado, podemos medir fácilmente qué tan buenos son nuestros modelos sustitutos para aproximar las predicciones de la caja negra. 5.6.4 Desventajas Debes tener en cuenta que extraes conclusiones sobre el modelo y no sobre los datos, ya que el modelo sustituto nunca ve el resultado real. No está claro cuál es el mejor corte del R2 para estar seguro de que el modelo sustituto está lo suficientemente cerca del modelo de caja negra. ¿80% de la varianza explicada? 50%? 99%? Podemos medir qué tan cerca está el modelo sustituto del modelo de caja negra. Supongamos que no estamos muy cerca, pero lo suficientemente cerca. Podría suceder que el modelo interpretable sea muy cercano para un subconjunto del conjunto de datos, pero ampliamente divergente para otro subconjunto. En este caso, la interpretación para el modelo simple no sería igualmente buena para todos los puntos de datos. El modelo interpretable que elijas como sustituto viene con todas sus ventajas y desventajas. Algunas personas argumentan que, en general, no existen modelos intrínsecamente interpretables (incluso modelos lineales y árboles de decisión) y que incluso sería peligroso tener una ilusión de interpretabilidad. Si compartes esta opinión, entonces este método no es para tí. 5.6.5 Software Usé el paquete iml R para los ejemplos. Si puedes entrenar un modelo de aprendizaje automático, entonces deberías poder implementar modelos sustitutos usted mismo. Simplemente entrena un modelo interpretable para predecir las predicciones del modelo de caja negra. "],["lime.html", "5.7 Sustituto local (LIME)", " 5.7 Sustituto local (LIME) Los modelos sustitutos locales son modelos interpretables que se utilizan para explicar las predicciones individuales de los modelos de aprendizaje automático de caja negra. En el artículo original, los autores proponen36 una implementación concreta de modelos sustitutos locales. Los modelos sustitutos están entrenados para aproximar las predicciones del modelo de caja negra subyacente. En lugar de entrenar un modelo sustituto global, LIME se enfoca en entrenar modelos sustitutos locales para explicar las predicciones individuales. La idea es bastante intuitiva. Primero, olvida los datos de entrenamiento e imagina que solo tienes el modelo de caja negra donde puedes ingresar puntos de datos y obtener las predicciones del modelo. Puedes ingresar puntos de datos en la caja tantas veces como quieras. Tu objetivo es comprender por qué el modelo de aprendizaje automático hizo una cierta predicción. LIME prueba lo que sucede con las predicciones cuando proporcionas variaciones de tus datos al modelo de aprendizaje automático. LIME genera un nuevo conjunto de datos que consta de muestras permutadas y las correspondientes predicciones del modelo de caja negra. En este nuevo conjunto de datos, LIME luego entrena un modelo interpretable, que se pondera por la proximidad de las instancias muestreadas a la instancia de interés. El modelo interpretable puede ser cualquier modelo interpretable, por ejemplo Lasso o un árbol de decisión. El modelo aprendido debería ser una buena aproximación de las predicciones del modelo de aprendizaje automático en forma local, no necesariamente en forma global. Este tipo de precisión también se llama fidelidad local. Matemáticamente, los modelos sustitutos locales con restricción de interpretabilidad se pueden expresar de la siguiente manera: \\[\\text{explanation}(x)=\\arg\\min_{g\\in{}G}L(f,g,\\pi_x)+\\Omega(g)\\] El modelo de explicación para la observación x es el modelo g (por ejemplo, modelo de regresión lineal) que minimiza la pérdida L (por ejemplo, error cuadrático medio), que mide qué tan cerca está la explicación de la predicción del modelo original f (por ejemplo, un modelo xgboost), mientras que la complejidad del modelo \\(\\Omega(g)\\) se mantiene baja (por ejemplo, prefieres menos características). G es la familia de posibles explicaciones, por ejemplo, todos los posibles modelos de regresión lineal. La medida de proximidad \\(\\pi_x\\) define qué tan grande es el vecindario alrededor de la instancia x que consideramos para la explicación. En la práctica, LIME solo optimiza la parte de pérdida. El usuario tiene que determinar la complejidad p, seleccionando el número máximo de características que puede usar el modelo de regresión lineal. La receta para entrenar modelos locales sustitutos: Selecciona tu instancia de interés para la que deseas tener una explicación de tu predicción de caja negra. Perturba tu conjunto de datos y obtén las predicciones de caja negra para estos nuevos puntos. Pondera las nuevas muestras según su proximidad a la instancia de interés. Entrena un modelo ponderado e interpretable en el conjunto de datos con las variaciones. Explica la predicción interpretando el modelo local. En las implementaciones actuales en R y Python, por ejemplo, la regresión lineal se puede elegir como sustituto interpretable. De antemano, debes seleccionar K, la cantidad de características que deseas tener en tu modelo interpretable. Cuanto más baja es la K, más fácil es interpretar el modelo. Una K más alta potencialmente produce modelos con mayor fidelidad. Existen varios métodos para entrenar modelos con exactamente K características. Una buena opción es Lasso. Un modelo Lasso con un alto parámetro de regularización \\(\\lambda\\) produce un modelo sin ninguna característica. Al volver a entrenar los modelos Lasso con una disminución lenta de \\(\\lambda\\), una tras otra, las características obtienen estimaciones de peso que difieren de cero. Si hay K características en el modelo, has alcanzado la cantidad deseada de características. Otras estrategias son la selección de características hacia adelante o hacia atrás. Esto significa que comienza con el modelo completo (= que contiene todas las características) o con un modelo con solo el intercepto y luego prueba qué característica brindaría la mayor mejora cuando se agrega o elimina, hasta que se alcanza un modelo con K características. ¿Cómo se obtienen las variaciones de los datos? Esto depende del tipo de datos, que pueden ser texto, imagen o datos tabulares. Para texto e imágenes, la solución es activar o desactivar palabras simples o superpíxeles. En el caso de los datos tabulares, LIME crea nuevas muestras al perturbar cada característica individualmente, dibujando a partir de una distribución normal con desviación estándar y media tomada de la característica. 5.7.1 LIME para datos tabulares Los datos tabulares son datos que vienen en tablas, cada fila representa una instancia y cada columna una característica. Las muestras de LIME no se toman alrededor de la instancia de interés, sino del centro de masa de los datos de entrenamiento, lo cual es problemático. Pero aumenta la probabilidad de que el resultado para algunas de las predicciones de puntos de muestra difiera del punto de datos de interés y que LIME pueda aprender al menos alguna explicación. Es mejor explicar visualmente cómo funciona el muestreo y el entrenamiento en modelos locales: FIGURA 5.32: Algoritmo LIME para datos tabulares. A) Las predicciones aleatorias del random forest dan características x1 y x2. Clases previstas: 1 (oscuro) o 0 (claro). B) Instancia de interés (punto grande) y datos muestreados de una distribución normal (puntos pequeños). C) Asigna un mayor peso a los puntos cercanos a la instancia de interés. D) Los signos de la cuadrícula muestran las clasificaciones del modelo aprendido localmente de las muestras ponderadas. La línea blanca marca el límite de decisión (P (clase = 1) = 0.5). Como siempre, el diablo está en los detalles. Definir un vecindario significativo alrededor de un punto es difícil. LIME actualmente utiliza un smoothing kernel (núcleo exponencial de suavizado) para definir el vecindario. Es una función que toma dos instancias de datos y devuelve una medida de proximidad. El ancho del kernel determina qué tan grande es el vecindario: Un ancho de kernel pequeño significa que una instancia debe estar muy cerca para influir en el modelo local, un ancho de kernel más grande significa que las instancias que están más lejos también influyen en el modelo. Si observas la implementación de Python LIME (file lime/lime_tabular.py) verás que utiliza un kernel exponencial (en los datos normalizados) y el ancho del kernel es 0,75 veces la raíz cuadrada del número de columnas de los datos de entrenamiento. Parece una línea de código inocente, pero es como un elefante sentado en tu sala de estar al lado de la porcelana que te dieron tus abuelos. El gran problema es que no tenemos una buena manera de encontrar el mejor núcleo o ancho. ¿Y de dónde viene el 0.75? En ciertos escenarios, puedes cambiar fácilmente tu explicación cambiando el ancho del núcleo, como se muestra en la siguiente figura: FIGURA 5.33: Explicación de la predicción de la instancia x = 1.6. Las predicciones del modelo de caja negra que dependen de una sola característica se muestran como una línea gruesa y se muestra la distribución de los datos con alfombras. Se calculan tres modelos sustitutos locales con diferentes anchos de núcleo. El modelo de regresión lineal resultante depende del ancho del kernel: ¿La característica tiene un efecto negativo, positivo o nulo para x = 1.6? El ejemplo muestra solo una característica. Empeora en espacios de características de alta dimensión. Tampoco está muy claro si la medida de distancia debe tratar todas las características por igual. ¿Una unidad de distancia para la característica x1 es idéntica a una unidad para la característica x2? Las medidas de distancia son bastante arbitrarias y las distancias en diferentes dimensiones (también conocidas como características) podrían no ser comparables en absoluto. 5.7.1.1 Ejemplo Veamos un ejemplo concreto. Volvemos a los datos de alquiler de bicicletas y convertimos el problema de predicción en uno de clasificación: Después de tener en cuenta la tendencia de que el alquiler de bicicletas se ha vuelto más popular con el tiempo, queremos saber en un día determinado si el número de bicicletas alquiladas será superior o inferior a la línea de tendencia. También puedes interpretar arriba como estar por encima del número promedio de bicicletas, pero ajustado por la tendencia. Primero entrenamos un random forest con 100 árboles en la tarea de clasificación. ¿En qué día el número de bicicletas de alquiler estará por encima del promedio libre de tendencias, según la información del clima y el calendario? Las explicaciones se crean con 2 características. Los resultados de los escasos modelos lineales locales entrenados para dos instancias con diferentes clases predichas: FIGURA 5.34: Explicaciones LIME para dos instancias del conjunto de datos de alquiler de bicicletas. La temperatura más cálida y la buena situación climática tienen un efecto positivo en la predicción. El eje x muestra el efecto de la entidad: el peso multiplicado por el valor real de la entidad. De la figura queda claro que es más fácil interpretar características categóricas que características numéricas. Una solución es transformar las variables numéricas en categóricas. 5.7.2 LIME para texto LIME para texto difiere de LIME para datos tabulares. Las variaciones de los datos se generan de manera diferente: A partir del texto original, se crean nuevos textos eliminando al azar palabras del texto original. El conjunto de datos se representa con características binarias para cada palabra. Una característica es 1 si se incluye la palabra correspondiente y 0 si se ha eliminado. 5.7.2.1 Ejemplo En este ejemplo, clasificamos comentarios de YouTube como spam o normal. El modelo de caja negra es un árbol de decisión profundo entrenado en la matriz de palabras del documento. Cada comentario es un documento (= una fila) y cada columna es el número de apariciones de una palabra dada. Los árboles de decisión cortos son fáciles de entender, pero en este caso el árbol es muy profundo. También en lugar de este árbol podría haber habido una red neuronal recurrente o una SVM entrenada en incrustaciones de palabras (vectores abstractos). Veamos los dos comentarios de este conjunto de datos y las clases correspondientes (1 para spam, 0 para comentario normal): CONTENT CLASS 267 PSY is a good guy 0 173 For Christmas Song visit my channel! ;) 1 El siguiente paso es crear algunas variaciones de los conjuntos de datos utilizados en un modelo local. Por ejemplo, algunas variaciones de uno de los comentarios: For Christmas Song visit my channel! ;) prob weight 2 1 0 1 1 0 0 1 0.17 0.57 3 0 1 1 1 1 0 1 0.17 0.71 4 1 0 0 1 1 1 1 0.99 0.71 5 1 0 1 1 1 1 1 0.99 0.86 6 0 1 1 1 0 0 1 0.17 0.57 Cada columna corresponde a una palabra en la oración. Cada fila es una variación, 1 significa que la palabra es parte de esta variación y 0 significa que la palabra ha sido eliminada. La oración correspondiente para una de las variaciones es Christmas Song visit my ;). La columna prob muestra la probabilidad pronosticada de spam para cada una de las variaciones de la oración. La columna weight muestra la proximidad de la variación a la oración original, calculada como 1 menos la proporción de palabras que se eliminaron, por ejemplo, si se eliminó 1 de 7 palabras, la proximidad es 1 - 1/7 = 0.86. Aquí están las dos oraciones (una no deseada, una no deseada) con sus estimaciones de pesos locales encontradas por el algoritmo LIME: case label_prob feature feature_weight 1 0.1701170 good 0.000000 1 0.1701170 a 0.000000 1 0.1701170 is 0.000000 2 0.9939024 channel! 6.180747 2 0.9939024 For 0.000000 2 0.9939024 ;) 0.000000 La palabra channel indica una alta probabilidad de spam. Para el comentario que no es spam, no se estimó un peso distinto de cero, porque no importa qué palabra se elimine, la clase predicha sigue siendo la misma. 5.7.3 LIME para imágenes Esta sección fue escrita por Verena Haunschmid. LIME para imágenes funciona de manera diferente que LIME para datos tabulares y texto. Intuitivamente, no tendría mucho sentido perturbar píxeles individuales, ya que muchos más de un píxel contribuyen a una clase. El cambio aleatorio de píxeles individuales probablemente no cambiaría mucho las predicciones. Por lo tanto, las variaciones de las imágenes se crean segmentando la imagen en superpíxeles y activando o desactivando los superpíxeles. Los superpíxeles son píxeles interconectados con colores similares y se pueden apagar reemplazando cada píxel con un color definido por el usuario, como el gris. El usuario también puede especificar una probabilidad de apagar un superpíxel en cada permutación. 5.7.3.1 Ejemplo Como el cálculo de las explicaciones de las imágenes es bastante lento, el paquete Lime R contiene un ejemplo precalculado que también usaremos para mostrar el resultado del método. Las explicaciones se pueden mostrar directamente en las muestras de imagen. Como podemos tener varias etiquetas predichas por imagen (ordenadas por probabilidad), podemos explicar los principales n_labels. Para la siguiente imagen, las 3 predicciones principales fueron guitarra eléctrica; guitarra acustica; y labrador. FIGURA 5.35: Explicaciones LIME para las 3 clases principales de clasificación de imágenes realizadas por la red neuronal Inception de Google. El ejemplo está tomado del artículo LIME (Ribeiro et al. ., 2016). La predicción y explicación en el primer caso son muy razonables. La primera predicción de guitarra eléctrica es, por supuesto, incorrecta, pero la explicación nos muestra que la red neuronal todavía se comportó razonablemente porque la parte de la imagen identificada sugiere que podría tratarse de una guitarra eléctrica. 5.7.4 Ventajas Incluso si reemplazas el modelo de aprendizaje automático subyacente, aún puedes usar el mismo modelo local e interpretable para la explicación. Supongamos que las personas que miran las explicaciones entienden mejor los árboles de decisión. Debido a que usas modelos sustitutos locales, usas árboles de decisión como explicaciones sin tener que usar un árbol de decisión para hacer las predicciones. Por ejemplo, puedes usar una SVM. Y si resulta que un modelo xgboost funciona mejor, puedes reemplazar el SVM y aún usarlo como árbol de decisión para explicar las predicciones. Los modelos sustitutos locales se benefician de la literatura y la experiencia de entrenamiento e interpretación de modelos interpretables. Cuando se usa Lasso o árboles cortos, las explicaciones resultantes son cortas (= selectivas) y posiblemente contrastantes. Por lo tanto, hacen explicaciones amigables para los humanos. Es por eso que veo LIME más en aplicaciones donde el destinatario de la explicación es un laico o alguien con muy poco tiempo. No es suficiente para las atribuciones completas, por lo que no veo LIME en escenarios de cumplimiento en los que legalmente se le puede exigir que explique completamente una predicción. También para depurar modelos de aprendizaje automático, es útil tener todas las razones en lugar de algunas. LIME es uno de los pocos métodos que funciona para datos tabulares, texto e imágenes. La medida de fidelidad (qué tan bien el modelo interpretable se aproxima a las predicciones de caja negra) nos da una buena idea de cuán confiable es el modelo interpretable para explicar las predicciones de caja negra en la vecindad de la instancia de datos de interés. LIME se implementa en Python (biblioteca lime) y R (paquete lime y paquete iml) y es muy fácil de usar. Las explicaciones creadas con modelos sustitutos locales pueden usar otras características (interpretables) que el modelo original en el que se entrenó. Por supuesto, estas características interpretables deben derivarse de las instancias de datos. Un clasificador de texto puede confiar en la inserción de palabras abstractas como características, pero la explicación puede basarse en la presencia o ausencia de palabras en una oración. Un modelo de regresión puede basarse en una transformación no interpretable de algunos atributos, pero las explicaciones se pueden crear con los atributos originales. Por ejemplo, el modelo de regresión podría recibir entrenamiento sobre los componentes de un análisis de componentes principales (PCA) de las respuestas a una encuesta, pero LIME podría recibir entrenamiento sobre las preguntas originales de la encuesta. El uso de características interpretables para LIME puede ser una gran ventaja sobre otros métodos, especialmente cuando el modelo fue entrenado con características no interpretables. 5.7.5 Desventajas La definición correcta del vecindario es un gran problema sin resolver cuando se utiliza LIME con datos tabulares. En mi opinión, es el mayor problema con LIME y la razón por la que recomendaría usar LIME solo con mucho cuidado. Para cada aplicación, debes probar diferentes configuraciones de kernel y ver por tí mismo si las explicaciones tienen sentido. Desafortunadamente, este es el mejor consejo que puedo dar para encontrar buenos anchos de kernel. El muestreo podría mejorarse en la implementación actual de LIME. Los puntos de datos se muestrean a partir de una distribución gaussiana, ignorando la correlación entre las características. Esto puede conducir a puntos de datos poco probables que luego se pueden utilizar para aprender modelos de explicación local. La complejidad del modelo de explicación debe definirse de antemano. Esto es solo una pequeña queja, porque al final el usuario siempre tiene que definir el punto que quiere entre fidelidad y escasez. Otro gran problema es la inestabilidad de las explicaciones. En un artículo37 los autores mostraron que las explicaciones de dos puntos muy cercanos variaban mucho en un entorno simulado. Además, en mi experiencia, si repites el proceso de muestreo, las explicaciones que salen pueden ser diferentes. La inestabilidad significa que es difícil confiar en las explicaciones, por lo que debes ser muy crítico. Conclusión: Los modelos sustitutos locales, con LIME como una implementación concreta, son muy prometedores. Pero el método todavía está en fase de desarrollo y muchos problemas deben resolverse antes de que pueda aplicarse de forma segura. Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. Why should I trust you?: Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016). Alvarez-Melis, David, and Tommi S. Jaakkola. On the robustness of interpretability methods. arXiv preprint arXiv:1806.08049 (2018). "],["anchors.html", "5.8 Reglas de ámbito (Anclas)", " 5.8 Reglas de ámbito (Anclas) Autores: Tobias Goerke y Magdalena Lang Anchors (anclas) explica las predicciones individuales de cualquier modelo de clasificación de caja negra al encontrar una regla de decisión que ancla la predicción lo suficiente. Una regla ancla una predicción si los cambios en otros valores de características no afectan la predicción. Anchors utiliza técnicas de aprendizaje de refuerzo en combinación con un algoritmo de búsqueda de gráficos para reducir el número de llamadas de modelo (y, por lo tanto, el tiempo de ejecución requerido) a un mínimo mientras aún se puede recuperar de los óptimos locales. Ribeiro, Singh y Guestrin propusieron el algoritmo en 2018 38, los mismos investigadores que introdujeron el algoritmo LIME. Al igual que su predecesor, el enfoque de anclaje despliega una estrategia basada en perturbaciones para generar explicaciones locales para las predicciones de modelos de aprendizaje automático de caja negra. Sin embargo, en lugar de los modelos sustitutos utilizados por LIME, las explicaciones resultantes se expresan como reglas SI-ENTONCES fáciles de entender, llamadas anchors. Estas reglas son reutilizables ya que son de ámbito: los anclajes incluyen la noción de cobertura, indicando con precisión a qué otras instancias, posiblemente no vistas, se aplican. Encontrar anclas implica una exploración o un problema de bandido multibrazo, que se origina en la disciplina del aprendizaje por refuerzo. Con este fin, se crean y evalúan vecinos o perturbaciones para cada instancia que se está explicando. Hacerlo permite que el enfoque no tenga en cuenta la estructura de la caja negra y sus parámetros internos para que estos puedan permanecer sin ser observados ni alterados. Por lo tanto, el algoritmo es modelo-agnóstico, lo que significa que se puede aplicar a cualquier clase de modelo. En su artículo, los autores comparan sus dos algoritmos y visualizan cuán diferente estos consultan el vecindario de una instancia para obtener resultados. Para esto, la siguiente figura muestra tanto LIME como anclas localmente explicando un clasificador binario complejo (predice - o +) usando dos ejemplos de instancias. Los resultados de LIME no indican cuán fieles son, ya que LIME solo aprende un límite de decisión lineal que se aproxima mejor al modelo dado un espacio de perturbación \\(D\\). Dado el mismo espacio de perturbación, el enfoque de anclajes construye explicaciones cuya cobertura se adapta al comportamiento del modelo y expresa claramente sus límites. Por lo tanto, son fieles por diseño y establecen exactamente para qué instancias son válidas. Esta propiedad hace que las anclas sean particularmente intuitivas y fáciles de comprender. FIGURA 5.36: LIME vs. Anchors  Una visualización. Figura de Ribeiro, Singh, and Guestrin (2018). Como se mencionó anteriormente, los resultados o las explicaciones del algoritmo vienen en forma de reglas, llamadas anclas. El siguiente ejemplo simple ilustra tal ancla. Por ejemplo, supongamos que se nos da un modelo bivariado de caja negra que predice si un pasajero sobrevive o no al desastre del Titanic. Ahora nos gustaría saber por qué el modelo predice para un individuo específico que sobrevive. El algoritmo de anclaje proporciona una explicación de resultados como la que se muestra a continuación. Feature Value Edad 20 Sexo femenino Clase primera Precio del ticket 300$ Más atributos  Sobrevive TRUE Y la explicación de anclajes correspondiente es: SI SEXO = femenino Y Clase = primero ENTONCES Sobrevive = TRUE CON PRECISIÓN 97% Y COBERTURA 15% El ejemplo muestra cómo las anclas pueden proporcionar información esencial sobre la predicción de un modelo y su razonamiento subyacente. El resultado muestra qué atributos fueron tomados en cuenta por el modelo, que en este caso es el sexo femenino y la primera clase. Los seres humanos, siendo primordiales para la corrección, pueden usar esta regla para validar el comportamiento del modelo. El ancla adicionalmente nos dice que se aplica al 15% de las instancias del espacio de perturbación. En esos casos, la explicación es 97% precisa, lo que significa que los predicados mostrados son casi exclusivamente responsables del resultado predicho. Un ancla \\(A\\) se define formalmente de la siguiente manera: \\[\\mathbb{E}_{\\mathcal{D}_x(z|A)}[1_{f(x)=f(z)}]\\geq\\tau,A(x)=1\\] Donde: \\(x\\) representa la instancia que se está explicando (por ejemplo, una fila en un conjunto de datos tabular). \\(A\\) es un conjunto de predicados, es decir, la regla o ancla resultante, de modo que \\(A(x)=1\\) cuando todos los predicados de entidad definidos por \\(A\\) corresponden a los valores de característica de \\(x\\). \\(f\\) denota el modelo de clasificación que se explicará (por ejemplo, un modelo de red neuronal artificial). Se puede consultar para predecir una etiqueta para \\(x\\) y sus perturbaciones. \\(D_x(\\cdot|A)\\) indica la distribución de vecinos de \\(x\\), que coincide con \\(A\\). \\(0\\leq\\tau\\leq1\\) especifica un umbral de precisión. Solo las reglas que logran una fidelidad local de al menos \\(\\tau\\) se consideran un resultado válido. La descripción formal puede ser intimidante y puede expresarse en palabras: Dada una instancia \\(x\\) para ser explicada, se debe encontrar una regla o un ancla \\(A\\), de modo que se aplique a \\(x\\), mientras que la misma clase que para \\(x\\) se predice por una fracción de al menos \\(\\tau\\) de los vecinos de \\(x\\) donde se aplica el mismo \\(A\\). La precisión de una regla resulta de la evaluación de vecinos o perturbaciones (siguiendo \\(D_x(z|A)\\)) usando el modelo de aprendizaje automático proporcionado (indicado por la función de indicador \\(1_{f(x)=f(z)}\\)). 5.8.1 Encontrar anclas Aunque la descripción matemática de los anclajes puede parecer clara y directa, no es factible construir reglas particulares. Se requeriría evaluar \\(1_{f(x)=f(z)}\\) para todos \\(z\\in\\mathcal{D}_x(\\cdot|A)\\) que no es posible en espacios de entrada continuos o grandes. Por lo tanto, los autores proponen introducir el parámetro \\(0\\leq\\delta\\leq1\\) para crear una definición probabilística. De esta forma, se extraen muestras hasta que haya una confianza estadística con respecto a su precisión. La definición probabilística dice lo siguiente: \\[P(prec(A)\\geq\\tau)\\geq{}1-\\delta\\quad\\textrm{con}\\quad{}prec(A)=\\mathbb{E}_{\\mathcal{D}_x(z|A)}[1_{f(x)=f(z)}]\\] Las dos definiciones anteriores se combinan y amplían por la noción de cobertura. Su fundamento consiste en encontrar reglas que se apliquen a una parte preferiblemente grande del espacio de entrada del modelo. La cobertura se define formalmente como la probabilidad de un ancla de aplicar a sus vecinos, es decir, su espacio de perturbación: \\[cov(A)=\\mathbb{E}_{\\mathcal{D}_{(z)}[A(z)]}\\] La inclusión de este elemento lleva a la definición final de los anclajes teniendo en cuenta la maximización de la cobertura: \\[\\underset{A\\:\\textrm{s.t.}\\;P(prec(A)\\geq\\tau)\\geq{}1-\\delta}{\\textrm{max}}cov(A)\\] Por lo tanto, el procedimiento busca una regla que tenga la cobertura más alta entre todas las reglas elegibles (todas aquellas que satisfacen el umbral de precisión dada la definición probabilística). Se cree que estas reglas son más importantes, ya que describen una parte más grande del modelo. Ten en cuenta que las reglas con más predicados tienden a tener mayor precisión que las reglas con menos predicados. En particular, una regla que corrige cada característica de \\(x\\) reduce el vecindario evaluado a instancias idénticas. Por lo tanto, el modelo clasifica a todos los vecinos por igual, y la precisión de la regla es \\(1\\). Al mismo tiempo, una regla que corrige muchas características es demasiado específica y solo es aplicable en algunas instancias. Por lo tanto, existe una compensación entre precisión y cobertura. El enfoque de anclaje utiliza cuatro componentes principales para encontrar explicaciones, como se muestra en la figura a continuación. Generación de candidatos: genera nuevos candidatos de explicación. En la primera ronda, se crea un candidato por función de \\(x\\) y se corrige el valor respectivo de posibles perturbaciones. En cada otra ronda, los mejores candidatos de la ronda anterior se extienden por un predicado de característica que aún no está contenido en él. Identificación del mejor candidato: Las reglas de los candidatos deben compararse con respecto a qué regla explica mejor \\(x\\). Para este fin, las perturbaciones que coinciden con la regla observada actualmente se crean evaluando llamando al modelo. Sin embargo, estas llamadas deben minimizarse para limitar la sobrecarga computacional. Es por eso que, en el núcleo de este componente, existe un Multi-Armed-Bandit de exploración pura (MAB; KL-LUCB39, para ser precisos). Los MAB se utilizan para explorar y explotar de manera eficiente diferentes estrategias (llamadas brazos en una analogía con las máquinas tragamonedas) mediante la selección secuencial. En el entorno dado, cada regla candidata se debe ver como un brazo que se puede tirar. Cada vez que se tira, se evalúa a los vecinos respectivos y, por lo tanto, obtenemos más información sobre el resultado de la regla del candidato (precisión en el caso de los anclajes). La precisión así establece qué tan bien la regla describe la instancia a ser explicada. Validación de precisión del candidato: toma más muestras en caso de que aún no haya confianza estadística de que el candidato exceda el umbral de \\(\\tau\\). Búsqueda en haz modificada: Todos los componentes anteriores se ensamblan en una búsqueda de haz, que es un algoritmo de búsqueda gráfica y una variante del algoritmo de amplitud. Lleva a los \\(B\\) mejores candidatos de cada ronda a la siguiente (donde \\(B\\) se llama Beam Width). Estas mejores reglas de \\(B\\) se utilizan para crear nuevas reglas. La búsqueda de haces realiza como máximo \\(featureCount(x)\\) rondas, ya que cada característica solo se puede incluir en una regla como máximo una vez. Por lo tanto, en cada ronda \\(i\\), genera candidatos con exactamente \\(i\\) predicados y selecciona el B mejor de los mismos. Por lo tanto, al establecer \\(B\\) alto, es más probable que el algoritmo evite los óptimos locales. A su vez, esto requiere una gran cantidad de llamadas de modelo y, por lo tanto, aumenta la carga computacional. FIGURA 5.37: Los componentes del algoritmo de anclaje y sus interrelaciones (simplificadas) El enfoque es una receta aparentemente perfecta para derivar eficientemente información estadísticamente sólida sobre por qué cualquier sistema clasificó una instancia de la manera en que lo hizo. Experimenta sistemáticamente con la entrada del modelo y concluye observando las salidas respectivas. Se basa en métodos de aprendizaje automático (MAB) bien establecidos e investigados para reducir la cantidad de llamadas realizadas al modelo. Esto, a su vez, reduce drásticamente el tiempo de ejecución del algoritmo. 5.8.2 Complejidad y tiempo de ejecución Conocer el comportamiento asintótico del tiempo de ejecución del enfoque de anclaje ayuda a evaluar qué tan bien se espera que funcione en problemas específicos. Deja que \\(B\\) denote el ancho del haz y \\(p\\) el número de todas las características. Entonces el algoritmo de anclaje está sujeto a: \\[\\mathcal{O}(B\\cdot{}p^2+p^2\\cdot\\mathcal{O}_{\\textrm{MAB}\\lbrack{}B\\cdot{}p,B\\rbrack})\\] Este límite se extrae de hiperparámetros independientes del problema, como la confianza estadística \\(\\delta\\). Ignorar los hiperparámetros ayuda a reducir la complejidad del límite (consulte el documento original para obtener más información). Como el MAB extrae los \\(B\\) mejores de los candidatos \\(B\\cdotp\\) en cada ronda, la mayoría de los MAB y sus tiempos de ejecución multiplican el factor \\(p^2\\) más que cualquier otro parámetro. Por lo tanto, se hace evidente: la eficiencia del algoritmo disminuye con problemas abundantes de características. 5.8.3 Ejemplo de datos tabulares Los datos tabulares son datos estructurados representados por tablas, en donde las columnas incorporan características y las filas son instancias. Por ejemplo, utilizamos los datos de alquiler de bicicletas para demostrar el potencial del enfoque de los anclajes para explicar las predicciones de LD para las instancias seleccionadas. Para esto, convertimos la regresión en un problema de clasificación y entrenamos un random forest como nuestro modelo de caja negra. Es para clasificar si el número de bicicletas alquiladas se encuentra por encima o por debajo de la línea de tendencia. Antes de crear explicaciones de anclaje, es necesario definir una función de perturbación. Una manera fácil de hacerlo es usar un espacio de perturbación intuitivo predeterminado para casos de explicación tabular que se pueden construir mediante el muestreo, por ejemplo, de los datos de entrenamiento. Al perturbar una instancia, este enfoque predeterminado mantiene los valores de las características que están sujetos a los predicados de los anclajes, mientras reemplaza las características no fijas con valores tomados de otra instancia muestreada al azar con una probabilidad específica. Este proceso produce nuevas instancias que son similares a las explicadas pero que han adoptado algunos valores de otras instancias aleatorias. Por lo tanto, se parecen a los vecinos. FIGURA 5.38: Anclajes que explican seis instancias del conjunto de datos de alquiler de bicicletas. Cada fila representa una explicación o ancla, y cada barra representa los predicados de características que contiene. el eje muestra la precisión de una regla, y el grosor de una barra corresponde a su cobertura. La regla base no contiene predicados. Estos anclajes muestran que el modelo considera principalmente la temperatura para las predicciones. Los resultados son instintivamente interpretables y muestran para cada instancia explicada, qué características son más importantes para la predicción del modelo. Como las anclas solo tienen unos pocos predicados, además tienen una alta cobertura y, por lo tanto, se aplican a otros casos. Las reglas que se muestran arriba se generaron con \\(\\tau=0.9\\). Por lo tanto, solicitamos anclajes cuyas perturbaciones evaluadas respalden fielmente la etiqueta con una precisión de al menos \\(90\\%\\). Además, se utilizó la discretización para aumentar la expresividad y la aplicabilidad de las características numéricas. Todas las reglas anteriores se generaron para instancias en las que el modelo decide con confianza en función de algunas características. Sin embargo, otras instancias no están tan claramente clasificadas por el modelo como más características son importantes. En tales casos, los anclajes se vuelven más específicos, comprenden más características y se aplican a menos instancias. FIGURA 5.39: Explicar instancias cerca de los límites de decisión conduce a reglas específicas que comprenden un mayor número de predicados de características y menor cobertura. Además, la regla vacía, es decir, la característica base, se vuelve menos importante. Esto puede interpretarse como una señal para un límite de decisión, ya que la instancia se encuentra en un vecindario volátil. Si bien elegir el espacio de perturbación predeterminado es una opción cómoda, puede tener un gran impacto en el algoritmo y, por lo tanto, puede conducir a resultados sesgados. Por ejemplo, si el conjunto de trenes no está equilibrado (hay un número desigual de instancias de cada clase), el espacio de perturbación también lo está. Esta condición afecta aún más la búsqueda de reglas y la precisión del resultado. El conjunto de datos cáncer cervical es un excelente ejemplo de esta situación. La aplicación del algoritmo de anclaje conduce a una de las siguientes situaciones: Al explicar las instancias etiquetadas como saludable se obtienen reglas vacías ya que todos los vecinos generados se evalúan como saludables. Las explicaciones para las instancias etiquetadas como cancer son demasiado específicas, es decir, comprenden muchos predicados de características, ya que el espacio de perturbación cubre principalmente valores de instancias saludables. FIGURA 5.40: La construcción de anclajes dentro de espacios de perturbación desequilibrados conduce a resultados inexpresivos. Este resultado puede ser no deseado y puede abordarse de múltiples maneras. Por ejemplo, se puede definir un espacio de perturbación personalizado que muestree de manera diferente, por ejemplo, de un conjunto de datos no balanceados o una distribución normal. Sin embargo, esto tiene un efecto secundario: los vecinos muestreados no son representativos y cambian el alcance de la cobertura. Alternativamente, podríamos modificar la confianza del MAB \\(\\delta\\) y los valores de los parámetros de error \\(\\epsilon\\). Esto provocaría que el MAB extraiga más muestras, lo que en última instancia llevaría a la minoría a tomar muestras con mayor frecuencia en términos absolutos. Para este ejemplo, utilizamos un subconjunto del conjunto de cáncer de cuello uterino en el que la mayoría de los casos están etiquetados como cáncer. Luego tenemos el marco para crear un espacio de perturbación correspondiente a partir de él. Las perturbaciones ahora tienen más probabilidades de conducir a predicciones variables, y el algoritmo de anclaje puede identificar características importantes. Sin embargo, uno debe tener en cuenta la definición de cobertura: solo se define dentro del espacio de perturbación. En los ejemplos anteriores, utilizamos el conjunto de trenes como base del espacio de perturbación. Dado que aquí solo usamos un subconjunto, una cobertura alta no necesariamente indica una importancia de regla globalmente alta. FIGURA 5.41: Balancear el dataset antes de construir anclas muestra el razonamiento del modelo para decidir en casos de minorías. 5.8.4 Ventajas El enfoque de los anclajes ofrece múltiples ventajas sobre LIME. Primero, la salida del algoritmo es más fácil de entender, ya que las reglas son fáciles de interpretar (incluso para los laicos). Además, se pueden generar subconjuntos de anclajes e incluso establecen una medida de importancia al incluir la noción de cobertura. En segundo lugar, el enfoque de anclajes funciona cuando las predicciones del modelo son no lineales o complejas en el vecindario de una instancia. A medida que el enfoque despliega técnicas de aprendizaje por refuerzo en lugar de ajustar modelos sustitutos, es menos probable que no se ajuste al modelo. Aparte de eso, el algoritmo es modelo-agnóstico y, por lo tanto, aplicable a cualquier modelo. Además, es altamente eficiente, ya que se puede paralelizar mediante el uso de MAB que admiten el muestreo por lotes (por ejemplo, BatchSAR). 5.8.5 Desventajas El algoritmo tiene una configuración altamente configurable e impactante, al igual que la mayoría de los explicadores basados en perturbaciones. No solo los hiperparámetros, como el ancho del haz o el umbral de precisión, deben ajustarse para obtener resultados significativos, sino que también la función de perturbación debe diseñarse explícitamente para un dominio/caso de uso. Piensa en cómo se perturban los datos tabulares y piensa en cómo aplicar los mismos conceptos a los datos de imagen (Sugerencia: no se pueden aplicar). Afortunadamente, los enfoques predeterminados se pueden usar en algunos dominios (por ejemplo, tabular), lo que facilita una configuración de explicación inicial. Además, muchos escenarios requieren discretización ya que de lo contrario los resultados son demasiado específicos, tienen una cobertura baja y no contribuyen a comprender el modelo. Si bien la discretización puede ayudar, también puede difuminar los límites de decisión si se usa descuidadamente y, por lo tanto, tiene el efecto opuesto exacto. Dado que no existe la mejor técnica de discretización, los usuarios deben conocer los datos antes de decidir cómo discretizar los datos para no obtener malos resultados. La construcción de anclajes requiere muchas llamadas al modelo ML, al igual que todos los explicadores basados en perturbaciones. Si bien el algoritmo implementa MAB para minimizar el número de llamadas, su tiempo de ejecución aún depende en gran medida del rendimiento del modelo y, por lo tanto, es muy variable. Por último, la noción de cobertura no está definida en algunos dominios. Por ejemplo, no existe una definición obvia o universal de cómo los superpíxeles en una imagen se comparan con los de otras imágenes. 5.8.6 Software y alternativas Actualmente hay dos implementaciones disponibles: anchor, un paquete de Python (también integrado por Alibi) y una Implementación de Java. El primero es la referencia de los autores del algoritmo de anclaje y el segundo una implementación de alto rendimiento que viene con una interfaz R, llamada anchors, que se utilizó para los ejemplos en este capítulo. A partir de ahora, los anclajes solo admiten datos tabulares. Sin embargo, los anclajes pueden construirse teóricamente para cualquier dominio o tipo de datos. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. Anchors: High-Precision Model-Agnostic Explanations. AAAI Conference on Artificial Intelligence (AAAI), 2018 Emilie Kaufmann and Shivaram Kalyanakrishnan. Information Complexity in Bandit Subset Selection. Proceedings of Machine Learning Research (2013). "],["shapley.html", "5.9 Valores de Shapley", " 5.9 Valores de Shapley Una predicción puede explicarse suponiendo que cada valor de característica de la instancia es un jugador en un juego donde la predicción es el pago. Los valores de Shapley, un método de la teoría de juegos de coalición, nos dicen cómo distribuir equitativamente el pago entre las características. 5.9.1 Idea general Supón el siguiente escenario: Has entrenado un modelo de aprendizaje automático para predecir los precios de los apartamentos. Para un determinado apartamento predice 300,000 y necesitas explicar esta predicción. El apartamento tiene un tamaño de 50 m2, está ubicado en el segundo piso, tiene un parque cercano y los gatos están prohibidos: FIGURA 5.42: El precio previsto para un apartamento de 50 m2 en el segundo piso con un parque cercano y prohibición de gatos es de 300,000. Nuestro objetivo es explicar cómo cada uno de estos valores de características contribuyó a la predicción La predicción promedio para todos los apartamentos es de 310,000. ¿Cuánto ha contribuido cada valor de característica a la predicción en comparación con la predicción promedio? La respuesta es simple para los modelos de regresión lineal. El efecto de cada característica es el peso de la característica multiplicado por el valor de la característica. Esto solo funciona debido a la linealidad del modelo. Para modelos más complejos, necesitamos una solución diferente. Por ejemplo, LIME sugiere modelos locales para estimar los efectos. Otra solución proviene de la teoría del juego cooperativo: El valor de Shapley, acuñado por Shapley (1953)40, es un método para asignar pagos a los jugadores en función de su contribución al pago total. Los jugadores cooperan en una coalición y reciben una cierta ganancia de esta cooperación. ¿Jugadores? ¿Juego? ¿Pagar? ¿Cuál es la conexión con las predicciones e interpretabilidad del aprendizaje automático? El juego es la tarea de predicción para una sola instancia del conjunto de datos. La ganancia es la predicción real para esta instancia menos la predicción promedio para todas las instancias. Los jugadores son las características de la instancia que colaboran para recibir la ganancia (= predecir un cierto valor). En nuestro ejemplo de apartamento, los valores de la característica parque-cerca, gato-prohibido, area-50 y piso-2 trabajaron juntos para lograr la predicción de 300,000. Nuestro objetivo es explicar la diferencia entre la predicción real (300,000) y la predicción promedio (310,000): una diferencia de -10,000. La respuesta podría ser: El parque-cerca contribuyó con 30,000; area-50 contribuyó con 10.000; piso-2 contribuyó con 0; gato-prohibido contribuyó -50,000. Las contribuciones suman -10,000, la predicción final menos el precio promedio previsto del apartamento. ¿Cómo calculamos el valor de Shapley para una característica? El valor de Shapley es la contribución marginal promedio de un valor de característica en todas las coaliciones posibles. Más claro ahora? En la siguiente figura, evaluamos la contribución del valor de la característica gato-prohibido cuando se agrega a una coalición de parque-cerca y area-50. Simulamos que solo parque-cerca, gato-prohibido y area-50 están en una coalición extrayendo aleatoriamente otro departamento de los datos y usando su valor para el atributo piso. El valor piso-2 fue reemplazado por el aleatorio dibujado piso-1. Luego predecimos el precio del apartamento con esta combinación ( 310,000). En un segundo paso, eliminamos gato-prohibido de la coalición reemplazándolo con un valor aleatorio de la característica gato prohibido/permitido del departamento dibujado al azar. En el ejemplo estaba permitido, pero podría ser prohibido nuevamente. Predecimos el precio del apartamento para la coalición de parque cercano y tamaño-50 (320,000). La contribución de gato prohibido fue de 310,000 - 320,000 = -  10,000. Esta estimación depende de los valores del apartamento dibujado al azar que sirvió como donante para los valores de características de gato y piso. Obtendremos mejores estimaciones si repetimos este paso de muestreo y promediamos las contribuciones. FIGURA 5.43: Una repetición de muestra para estimar la contribución de gato-prohibido a la predicción cuando se agrega a la coalición de parque-cercayarea-50`. Repetimos este cálculo para todas las coaliciones posibles. El valor de Shapley es el promedio de todas las contribuciones marginales a todas las coaliciones posibles. El tiempo de cálculo aumenta exponencialmente con el número de características. Una solución para mantener manejable el tiempo de cálculo es calcular las contribuciones solo para unas pocas muestras de las posibles coaliciones. La siguiente figura muestra todas las coaliciones de valores de características que se necesitan para determinar el valor de Shapley para gato-prohibido. La primera fila muestra la coalición sin ningún valor de característica. En la imagen, las filas segunda, tercera y cuarta muestran diferentes coaliciones con un tamaño de coalición creciente, separadas por |. En general, son posibles las siguientes coaliciones: Sin valores de características parque-cerca area-50 piso-2 parque-cerca + area-50 parque-cerca + piso-2 area-50 + piso-2 parque-cerca + area-50 + piso-2 Para cada una de estas coaliciones, calculamos el precio predicho del apartamento con y sin el valor de característica gatos prohibidos y tomamos la diferencia para obtener la contribución marginal. El valor de Shapley es el promedio (ponderado) de las contribuciones marginales. Reemplazamos los valores de características de las características que no están en una coalición con valores de características aleatorias del conjunto de datos del departamento para obtener una predicción del modelo de aprendizaje automático. FIGURA 5.44: Las 8 coaliciones necesarias para calcular el valor exacto de Shapley del valor de la característica gatos-prohibidos. Si estimamos los valores de Shapley para todos los valores de características, obtenemos la distribución completa de la predicción (menos el promedio) entre los valores de características. 5.9.2 Ejemplos e interpretación La interpretación del valor de Shapley para el valor de característica j es: El valor de la característica j contribuyó \\(\\phi_j\\) a la predicción de esta instancia particular en comparación con la predicción promedio para el conjunto de datos. El valor de Shapley funciona tanto para la clasificación (si se trata de probabilidades) como para la regresión. Utilizamos el valor de Shapley para analizar las predicciones de un random forest aleatorio que predice cáncer cervical: FIGURA 5.45: Valores de Shapley para una mujer en el conjunto de datos de cáncer cervical. Con una predicción de 0.53, la probabilidad de cáncer de esta mujer es 0.51 por encima de la predicción promedio de 0.03. El número de ETS diagnosticadas aumentó la probabilidad más. La suma de las contribuciones produce la diferencia entre la predicción real y la media (0.51) Para el conjunto de datos de alquiler de bicicletas, también entrenamos un random forest para predecir el número de bicicletas alquiladas por un día, dada la información del clima y el calendario. Las explicaciones creadas para la predicción aleatoria del random forest de un día en particular: FIGURA 5.46: Valores de Shapley para el día 285. Con un número previsto de 2475 bicicletas alquiladas, este día está -2041 por debajo de la predicción promedio de 4516. la situación climática y la humedad tuvieron las mayores contribuciones negativas. La temperatura en este día tuvo una contribución positiva. La suma de los valores de Shapley produce la diferencia de predicción real y promedio (-2041). Ten cuidado de interpretar el valor de Shapley correctamente: El valor de Shapley es la contribución promedio de un valor de característica a la predicción en diferentes coaliciones. El valor de Shapley NO es la diferencia en la predicción cuando eliminaríamos la característica del modelo. 5.9.3 El valor de Shapley en detalle Esta sección profundiza en la definición y el cálculo del valor de Shapley para el lector curioso. Omite esta sección y ve directamente a Ventajas y desventajas si no estás interesado en los detalles técnicos. Estamos interesados en cómo cada característica afecta la predicción de un punto de datos. En un modelo lineal es fácil calcular los efectos individuales. Así es como se ve una predicción de modelo lineal para una instancia de datos: \\[\\hat{f}(x)=\\beta_0+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\] donde x es la instancia para la que queremos calcular las contribuciones. Cada \\(x_j\\) es un valor de característica, con j = 1, , p. \\(\\beta_j\\) es el peso correspondiente al atributo j. La contribución \\(\\phi_j\\) de la función j-ésima en la predicción \\(\\hat{f}(x)\\) es: \\[\\phi_j(\\hat{f})=\\beta_{j}x_j-E(\\beta_{j}X_{j})=\\beta_{j}x_j-\\beta_{j}E(X_{j})\\] donde \\(E(\\beta_jX_{j})\\) es la estimación del efecto medio para la característica j. La contribución es la diferencia entre el efecto de la característica menos el efecto promedio. ¡Agradable! Ahora sabemos cuánto contribuyó cada característica a la predicción. Si sumamos todas las contribuciones de características para una instancia, el resultado es el siguiente: \\[\\begin{align*}\\sum_{j=1}^{p}\\phi_j(\\hat{f})=&amp;\\sum_{j=1}^p(\\beta_{j}x_j-E(\\beta_{j}X_{j}))\\\\=&amp;(\\beta_0+\\sum_{j=1}^p\\beta_{j}x_j)-(\\beta_0+\\sum_{j=1}^{p}E(\\beta_{j}X_{j}))\\\\=&amp;\\hat{f}(x)-E(\\hat{f}(X))\\end{align*}\\] Este es el valor predicho para el punto de datos x menos el valor promedio predicho. Las contribuciones de funciones pueden ser negativas. ¿Podemos hacer lo mismo para cualquier tipo de modelo? Sería genial tener esto como una herramienta independiente del modelo. Como generalmente no tenemos pesos similares en otros tipos de modelos, necesitamos una solución diferente. La ayuda proviene de lugares inesperados: teoría de juegos cooperativos. El valor Shapley es una solución para calcular las contribuciones de características para predicciones individuales para cualquier modelo de aprendizaje automático. 5.9.3.1 El valor de Shapley El valor de Shapley se define mediante una función de valor val de jugadores en S. El valor de Shapley de un valor de característica es su contribución al pago, ponderado y sumado sobre todas las combinaciones posibles de valor de característica: \\[\\phi_j(val)=\\sum_{S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\setminus\\{x_j\\}}\\frac{|S|!\\left(p-|S|-1\\right)!}{p!}\\left(val\\left(S\\cup\\{x_j\\}\\right)-val(S)\\right)\\] donde S es un subconjunto de las características utilizadas en el modelo, x es el vector de valores de características de la instancia a explicar y p el número de características. \\(val_x(S)\\) es la predicción para los valores de características en el conjunto S que están marginados sobre las características que no están incluidas en el conjunto S: \\[val_{x}(S)=\\int\\hat{f}(x_{1},\\ldots,x_{p})d\\mathbb{P}_{x\\notin{}S}-E_X(\\hat{f}(X))\\] Realmente realiza múltiples integraciones para cada característica que no está contenida S. Un ejemplo concreto: El modelo de aprendizaje automático funciona con 4 características x1, x2, x3 y x4 y evaluamos la predicción para la coalición S que consta de valores de características x1 y x3: \\[val_{x}(S)=val_{x}(\\{x_{1},x_{3}\\})=\\int_{\\mathbb{R}}\\int_{\\mathbb{R}}\\hat{f}(x_{1},X_{2},x_{3},X_{4})d\\mathbb{P}_{X_2X_4}-E_X(\\hat{f}(X))\\] ¡Esto se parece a las contribuciones de características en el modelo lineal! No lo confudas con los muchos usos de la palabra valor: El valor de la característica es el valor numérico o categórico de una característica e instancia; el valor de Shapley es la contribución de la característica a la predicción; la función de valor es la función de pago para coaliciones de jugadores (valores de características). El valor de Shapley es el único método de atribución que satisface las propiedades Eficiencia, Simetría, Dummies y Aditividad, que juntas pueden considerarse una definición de pago justo. Eficiencia Las contribuciones de características deben sumarse a la diferencia de predicción para x y el promedio. \\[\\sum\\nolimits_{j=1}^p\\phi_j=\\hat{f}(x)-E_X(\\hat{f}(X))\\] Simetría Las contribuciones de dos valores de características j y k deberían ser las mismas si contribuyen igualmente a todas las coaliciones posibles. Si \\[val(S\\cup\\{x_j\\})=val(S\\cup\\{x_k\\})\\] para todos \\[S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\setminus\\{x_j,x_k\\}\\] luego \\[\\phi_j=\\phi_{k}\\] Dummies Una característica j que no cambia el valor predicho, independientemente de a qué coalición de valores de característica se agregue, debe tener un valor Shapley de 0. Si \\[val(S\\cup\\{x_j\\})=val(S)\\] para todos \\[S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\] luego \\[\\phi_j=0\\] Aditividad Para un juego con pagos combinados val+val+, los valores de Shapley respectivos son los siguientes: \\[\\phi_j+\\phi_j^{+}\\] Supongamos que entrenaste un random forest, lo que significa que la predicción es un promedio de muchos árboles de decisión. La propiedad Aditividad garantiza que para un valor de característica, puedes calcular el valor de Shapley para cada árbol individualmente, promediarlos y obtener el valor de Shapley para el valor de característica para el random forest. 5.9.3.2 Intuición Una forma intuitiva de comprender el valor de Shapley es la siguiente ilustración: Los valores de las características ingresan a una habitación en orden aleatorio. Todos los valores de características en la sala participan en el juego (= contribuyen a la predicción). El valor de Shapley de un valor de característica es el cambio promedio en la predicción que la coalición que ya está en la sala recibe cuando el valor de característica se une a ellos. 5.9.3.3 Estimación del valor de Shapley Todas las coaliciones (conjuntos) posibles de valores de características deben evaluarse con y sin la característica j-ésima para calcular el valor exacto de Shapley. Para más de unas pocas características, la solución exacta a este problema se vuelve problemática ya que el número de coaliciones posibles aumenta exponencialmente a medida que se agregan más características. Strumbelj et al. (2014)41 proponen una aproximación con el muestreo de Monte-Carlo: \\[\\hat{\\phi}_{j}=\\frac{1}{M}\\sum_{m=1}^M\\left(\\hat{f}(x^{m}_{+j})-\\hat{f}(x^{m}_{-j})\\right)\\] donde \\(\\hat{f}(x^{m}_{+j})\\) es la predicción para x, pero con un número aleatorio de valores de características reemplazados por valores de características de un punto de datos aleatorio z, excepto el valor respectivo de la característica j. El vector x \\(x^{m}_{-j}\\) es casi idéntico a \\(x^{m}_{+j}\\), pero el valor \\(x_j^{m}\\) también se toma de la muestra z. Cada una de estas M nuevas instancias es una especie de Frankenstein ensamblado a partir de dos instancias. Estimación aproximada de Shapley para el valor de una sola característica: Salida: valor de Shapley para el valor de la característica j-ésima Requerido: Número de iteraciones M, instancia de interés x, índice de características j, matriz de datos X y modelo de aprendizaje automático f - Para todos m = 1, , M: Dibuja una instancia aleatoria z de la matriz de datos X Elige una permutación aleatoria de los valores de la característica Instancia de pedido x: \\(x_o=(x_{(1)},\\ldots,x_{(j)},\\ldots,x_{(p)})\\) Instancia de pedido z: \\(z_o=(z_{(1)},\\ldots,z_{(j)},\\ldots,z_{(p)})\\) Construye dos nuevas instancias Con la función j: \\(x_{+j}=(x_{(1)},\\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\\ldots,z_{(p)})\\) Sin la característica j: \\(x_{-j}=(x_{(1)},\\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\\ldots,z_{(p)})\\) Calcular contribución marginal: \\(\\phi_j^{m}=\\hat{f}(x_{+j})-\\hat{f}(x_{-j})\\) Calcular el valor de Shapley como el promedio: \\(\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\) Primero, selecciona una instancia de interés x, una característica j y el número de iteraciones M. Para cada iteración, se selecciona una instancia aleatoria z de los datos y se genera un orden aleatorio de las características. Se crean dos nuevas instancias combinando valores de la instancia de interés x y la muestra z. La primera instancia \\(x_{+j}\\) es la instancia de interés, pero todos los valores en el orden anterior e incluido el valor de la característica j se reemplazan por los valores de la característica de la muestra z. La segunda instancia \\(x_{-j}\\) es similar, pero tiene todos los valores en el orden anterior, pero excluye la característica j reemplazada por los valores de la característica j de la muestra z. Se calcula la diferencia en la predicción de caja negra: \\[\\phi_j^{m}=\\hat{f}(x^m_{+j})-\\hat{f}(x^m_{-j})\\] Todas estas diferencias se promedian y dan como resultado: \\[\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\] El promedio pesa implícitamente las muestras por la distribución de probabilidad de X. El procedimiento debe repetirse para cada una de las características para obtener todos los valores de Shapley. 5.9.4 Ventajas La diferencia entre la predicción y la predicción promedio está distribuida de manera justa entre los valores de característica de la instancia: la propiedad de eficiencia de los valores de Shapley. Esta propiedad distingue el valor Shapley de otros métodos como LIME. LIME no garantiza que la predicción se distribuya equitativamente entre las características. El valor de Shapley podría ser el único método para entregar una explicación completa. En situaciones donde la ley requiere explicabilidad, como el derecho a explicaciones de la UE, el valor de Shapley podría ser el único método legalmente compatible, porque se basa en una teoría sólida y distribuye los efectos de manera justa. No soy abogado, así que esto refleja solo mi intuición sobre los requisitos. El valor de Shapley permite explicaciones contrastantes. En lugar de comparar una predicción con la predicción promedio de todo el conjunto de datos, puedes compararla con un subconjunto o incluso con un único punto de datos. Este contraste también es algo que los modelos locales como LIME no tienen. El valor de Shapley es el único método de explicación con una teoría sólida. Los axiomas (eficiencia, simetría, dummies, aditividad) dan a la explicación una base razonable. Métodos como LIME suponen un comportamiento lineal del modelo de aprendizaje automático a nivel local, pero no hay una teoría de por qué esto debería funcionar. Es alucinante explicar una predicción como un juego jugado por los valores de las características. 5.9.5 Desventajas El valor de Shapley requiere mucho tiempo de cómputo. En el 99.9% de los problemas del mundo real, solo la solución aproximada es factible. Un cálculo exacto del valor de Shapley es computacionalmente costoso porque hay 2k posibles coaliciones de los valores de la característica y la ausencia de una característica tiene que ser simulada dibujando instancias aleatorias, lo que aumenta la varianza para la estimación de Shapley estimación de valores. El número exponencial de las coaliciones se trata muestreando coaliciones y limitando el número de iteraciones M. La disminución de M reduce el tiempo de cálculo, pero aumenta la varianza del valor de Shapley. No hay una buena regla general para el número de iteraciones M. M debe ser lo suficientemente grande como para estimar con precisión los valores de Shapley, pero lo suficientemente pequeño como para completar el cálculo en un tiempo razonable. Debería ser posible elegir M en función de los límites de Chernoff, pero no he visto ningún documento sobre cómo hacer esto para los valores de Shapley para las predicciones de aprendizaje automático. El valor de Shapley puede malinterpretarse. El valor de Shapley de un valor de característica no es la diferencia del valor pronosticado después de eliminar la característica del entrenamiento del modelo. La interpretación del valor de Shapley es: Dado el conjunto actual de valores de características, la contribución de un valor de característica a la diferencia entre la predicción real y la predicción media es el valor estimado de Shapley. El valor de Shapley es el método de explicación incorrecto si busca explicaciones dispersas (explicaciones que contienen pocas características). Las explicaciones creadas con el método de valor Shapley siempre usan todas las características. Los humanos prefieren explicaciones selectivas, como las producidas por LIME. LIME podría ser la mejor opción para las explicaciones con las que los laicos tienen que lidiar. Otra solución es SHAP presentada por Lundberg y Lee (2016)42, que se basa en el valor de Shapley, pero también puede proporcionar explicaciones con pocas características. El valor de Shapley devuelve un valor simple por característica, pero sin modelo de predicción como LIME. Esto significa que no se puede usar para hacer declaraciones sobre cambios en la predicción de cambios en la entrada, como: Si ganara 300 euros más al año, mi puntaje de crédito aumentaría en 5 puntos. Otra desventaja es que necesitas acceso a los datos si deseas calcular el valor de Shapley para una nueva instancia de datos. No es suficiente acceder a la función de predicción porque necesitas los datos para reemplazar partes de la instancia de interés con valores de instancias de datos extraídos al azar. Esto solo se puede evitar si puedes crear instancias de datos que se vean como instancias de datos reales pero que no sean instancias reales a partir de los datos de entrenamiento. Al igual que muchos otros métodos de interpretación basados en permutación, el método del valor de Shapley sufre de inclusión de instancias de datos poco realistas cuando las características están correlacionadas. Para simular que falta un valor de característica en una coalición, marginalizamos la característica. Esto se logra mediante el muestreo de valores de la distribución marginal de la entidad. Esto está bien siempre que las características sean independientes. Cuando las características dependen, entonces podríamos probar valores de características que no tienen sentido para esta instancia. Pero los usaríamos para calcular el valor Shapley de la entidad. Que yo sepa, no hay investigación sobre lo que eso significa para los valores de Shapley, ni una sugerencia sobre cómo solucionarlo. Una solución podría ser permutar características correlacionadas juntas y obtener un valor mutuo de Shapley para ellas. O el procedimiento de muestreo podría tener que ajustarse para tener en cuenta la dependencia de las características. 5.9.6 Software y alternativas Los valores de Shapley se implementan en el paquete iml R. SHAP, un método de estimación alternativo para los valores de Shapley, se presenta en el próximo capítulo. Otro enfoque se llama breakDown, que se implementa en el paquete breakDown. BreakDown también muestra las contribuciones de cada característica a la predicción, pero las calcula paso a paso. Reutilicemos la analogía del juego: Comenzamos con un equipo vacío, agregamos el valor de la característica que contribuiría más a la predicción e iteramos hasta que se agreguen todos los valores de la característica. La contribución de cada valor de característica depende de los valores de característica respectivos que ya están en el equipo, que es el gran inconveniente del método breakDown. Es más rápido que el método de valor Shapley, y para modelos sin interacciones, los resultados son los mismos. Shapley, Lloyd S. A value for n-person games. Contributions to the Theory of Games 2.28 (1953): 307-317. trumbelj, Erik, and Igor Kononenko. Explaining prediction models and individual predictions with feature contributions. Knowledge and information systems 41.3 (2014): 647-665. Lundberg, Scott M., and Su-In Lee. A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems. 2017. "],["shap.html", "5.10 SHAP (explicaciones aditivas SHapley)", " 5.10 SHAP (explicaciones aditivas SHapley) SHAP (explicaciones aditivas SHapley) de Lundberg y Lee (2016)43 es un método para explicar las predicciones individuales. SHAP se basa en los valores de Shapley. Hay dos razones por las que SHAP tiene su propio capítulo y no es un subcapítulo de valores de Shapley. Primero, los autores de SHAP propusieron KernelSHAP, un enfoque de estimación alternativo basado en el núcleo para los valores de Shapley inspirados en modelos sustitutos locales. Y propusieron TreeSHAP, un enfoque de estimación eficiente para modelos basados en árboles. En segundo lugar, SHAP viene con muchos métodos de interpretación global basados en agregaciones de valores de Shapley. Este capítulo explica tanto los nuevos enfoques de estimación como los métodos de interpretación global. Recomiendo leer los capítulos sobre valores de Shapley y modelos locales (LIME) primero. 5.10.1 Definición El objetivo de SHAP es explicar la predicción de una instancia x calculando la contribución de cada característica a la predicción. El método de explicación SHAP calcula los valores de Shapley a partir de la teoría de juegos de coalición. Los valores de características de una instancia de datos actúan como jugadores en una coalición. Los valores de Shapley nos dicen cómo distribuir equitativamente el pago (= la predicción) entre las características. Un jugador puede ser un valor de característica individual, por ejemplo en datos tabulares. Un jugador también puede ser un grupo de valores de características. Por ejemplo, para explicar una imagen, los píxeles se pueden agrupar en superpíxeles y la predicción se puede distribuir entre ellos. Una innovación que SHAP trae a la mesa es que la explicación del valor de Shapley se representa como un método de atribución de características aditivas, un modelo lineal. Esa vista conecta los valores Shapley y LIME. SHAP especifica la explicación como: \\[g(z&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jz_j&#39;\\] donde g es el modelo de explicación, \\(z&#39;\\in\\{0,1\\}^M\\) es el vector de coalición, M es el tamaño máximo de la coalición y \\(\\phi_j\\in\\mathbb{R}\\) es la atribución de características para una característica j, los valores de Shapley. Lo que llamo vector de coalición se llama características simplificadas en el paper SHAP. Creo que este nombre fue elegido porque, por ejemplo, en datos de imágenes estas no se representan en el nivel de píxeles, sino que se agregan a superpíxeles. Creo que es útil pensar en las z como descripciones de coaliciones: En el vector de coalición, una entrada de 1 significa que el valor de la característica correspondiente está presente y 0 que está ausente. Esto debería sonarte familiar si conoces los valores de Shapley. Para calcular los valores de Shapley, simulamos que solo se están reproduciendo algunos valores de características (presente) y otros no (ausentes). La representación como modelo lineal de coaliciones es un truco para el cálculo de los \\(\\phi\\)s. Para x, la instancia de interés, el vector de coalición x es un vector de todos los 1, es decir, todos los valores de características están presentes. La fórmula se simplifica a: \\[g(x&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_j\\] Puedes encontrar esta fórmula en notación similar en el capítulo Valor de Shapley. Más tarde veremos más información sobre la estimación real. Hablemos primero sobre las propiedades de los \\(\\phi\\)s antes de entrar en los detalles de su estimación. Los valores de Shapley son la única solución que satisface las propiedades de eficiencia, simetría, simulación y aditividad. SHAP también los satisface, ya que calcula los valores de Shapley. En el trabajo SHAP, encontrarás discrepancias entre las propiedades SHAP y las propiedades Shapley. SHAP describe las siguientes tres propiedades deseables: 1) Precisión local \\[f(x)=g(x&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jx_j&#39;\\] Si defines \\(\\phi_0=E_X(\\hat{f}(x))\\) y estableces todos \\(x_j&#39;\\) en 1, esta es la propiedad de eficiencia Shapley. Solo con un nombre diferente y usando el vector de coalición. \\[f(x)=\\phi_0+\\sum_{j=1}^M\\phi_jx_j&#39;=E_X(\\hat{f}(X))+\\sum_{j=1}^M\\phi_j\\] 2) Ausencia \\[x_j&#39;=0\\Rightarrow\\phi_j=0\\] La ausencia dice que una característica faltante obtiene una atribución de cero. Ten en cuenta que \\(x_j&#39;\\) se refiere a las coaliciones, donde un valor de 0 representa la ausencia de un valor de característica. En la notación de coalición, todos los valores de características \\(x_j&#39;\\) de la instancia a explicar deben ser 1. La presencia de un 0 significaría que falta el valor de la característica para la instancia de interés. Esta propiedad no se encuentra entre las propiedades de los valores Shapley normales. Entonces, ¿por qué lo necesitamos para SHAP? Lundberg lo llama una propiedad menor de contabilidad. Una característica faltante podría, en teoría, tener un valor de Shapley arbitrario sin dañar la propiedad de precisión local, ya que se multiplica por \\(x_j&#39;= 0\\). La propiedad de ausencia exige que las características faltantes obtengan un valor Shapley de 0. En la práctica, esto solo es relevante para las características que son constantes. 3) Consistencia Deja que \\(f_x(z&#39;)=f(h_x(z&#39;))\\) y \\(z_{\\setminus{}j&#39;}\\) indiquen que \\(z_j&#39;=0\\). Para cualquiera de los dos modelos f y f que satisfacen: \\[f_x&#39;(z&#39;)-f_x&#39;(z_{\\setminus{}j}&#39;)\\geq{}f_x(z&#39;)-f_x(z_{\\setminus{}j}&#39;)\\] para todas las entradas \\(z&#39;\\in\\{0,1\\}^M\\), entonces: \\[\\phi_j(f&#39;,x)\\geq\\phi_j(f,x)\\] La propiedad de consistencia dice que cuando un modelo cambie de modo que la contribución marginal de un valor de entidad aumente o permanezca igual (independientemente de otras características), el valor de Shapley también debe aumentar o permanecer igual. A partir de la consistencia, siguen las propiedades de Shapley Linealidad, Dummies y Simetría, como se describe en el Apéndice de Lundberg y Lee. 5.10.2 KernelSHAP KernelSHAP estima para una instancia x las contribuciones de cada valor de característica a la predicción. KernelSHAP consta de 5 pasos: Ejemplos de coaliciones \\(z_k&#39;\\in\\{0,1\\}^M,\\quad{}k\\in\\{1,\\ldots,K\\}\\) (1 = característica presente en la coalición, 0 = característica ausente). Obtiene las predicciones para cada \\(z_k&#39;\\) convirtiendo primero \\(z_k&#39;\\) en el espacio de características original y luego aplicando el modelo f: \\(f(h_x(z_k&#39;))\\) Calcula el peso de cada \\(z_k&#39;\\) con el kernel SHAP. Ajusta el modelo lineal ponderado. Devuelva los valores de Shapley \\(\\phi_k\\), los coeficientes del modelo lineal. Podemos crear una coalición aleatoria mediante lanzamientos de monedas repetidos hasta que tengamos una cadena de 0 y 1. Por ejemplo, el vector de (1,0,1,0) significa que tenemos una coalición de las características primera y tercera. Las coaliciones de muestra K se convierten en el conjunto de datos para el modelo de regresión. El objetivo del modelo de regresión es la predicción para una coalición. (¡Espera!, Dirás, El modelo no ha sido entrenado en estos datos de coalición binaria y no puede hacer predicciones para ellos). Para pasar de coaliciones de valores de entidades a instancias de datos válidas, necesitamos una función \\(h_x(z&#39;)=z\\) donde \\(h_x:\\{0,1\\}^M\\rightarrow\\mathbb{R}^p\\). La función \\(h_x\\) asigna 1s al valor correspondiente de la instancia x que queremos explicar. Para los datos tabulares, asigna los 0 a los valores de otra instancia que tomamos de los datos. Esto significa que equiparamos el valor de la característica está ausente con el valor de la característica se reemplaza por el valor de la característica aleatorio de los datos. Para datos tabulares, la siguiente figura visualiza la asignación de coaliciones a valores de características: FIGURA 5.47: La función \\(h_x\\) asigna una coalición a una instancia válida. Para las características actuales (1), \\(h_x\\) asigna a los valores de característica de x. Para características ausentes (0), \\(h_x\\) se asigna a los valores de una instancia de datos muestreados aleatoriamente. En un mundo perfecto, \\(h_x\\) muestrea los valores de características ausentes condicionales a los valores de características actuales: \\[f(h_x(z&#39;))=E_{X_C|X_S}[f(x)|x_S]\\] donde \\(X_C\\) es el conjunto de características ausentes y \\(X_S\\) es el conjunto de características actuales. Sin embargo, como se definió anteriormente, \\(h_x\\) para datos tabulares trata a \\(X_C\\) y \\(X_S\\) como independientes e integra sobre la distribución marginal: \\[f(h_x(z&#39;))=E_{X_C}[f(x)]\\] El muestreo de la distribución marginal significa ignorar la estructura de dependencia entre las características presentes y ausentes. Por lo tanto, KernelSHAP sufre el mismo problema que todos los métodos de interpretación basados en permutación. La estimación pone demasiado peso en casos improbables. Los resultados pueden volverse poco confiables. Como veremos más adelante, TreeSHAP para los conjuntos de árboles no se ve afectado por este problema. Para las imágenes, la siguiente figura describe una posible función de mapeo: FIGURA 5.48: La función \\(h_x\\) asigna coaliciones de superpíxeles (sp) a imágenes. Los superpíxeles son grupos de píxeles. Para las características actuales (1), \\(h_x\\) devuelve la parte correspondiente del original imagen. Para las funciones ausentes (0), \\(h_x\\) atenúa el área correspondiente. Asignar el color promedio de los píxeles circundantes o similar también sería una opción. La gran diferencia con LIME es la ponderación de las instancias en el modelo de regresión. LIME pondera las instancias según lo cerca que estén de la instancia original. Cuantos más ceros haya en el vector de coalición, menor será el peso en LIME. SHAP pondera las instancias muestreadas de acuerdo con el peso que obtendría la coalición en la estimación del valor de Shapley. Las coaliciones pequeñas (pocos 1) y las coaliciones grandes (es decir, muchos 1) obtienen los mayores pesos. La intuición detrás de esto es: Aprendemos más sobre las características individuales si podemos estudiar sus efectos de forma aislada. Si una coalición consta de una sola característica, podemos aprender sobre el efecto principal aislado de las características en la predicción. Si una coalición consta de todas las características menos una, podemos aprender sobre el efecto total de esta característica (efecto principal más interacciones de características). Si una coalición consta de la mitad de las características, aprendemos poco acerca de una contribución de características individuales, ya que hay muchas coaliciones posibles con la mitad de las características. Para lograr la ponderación compatible con Shapley, Lundberg et. Al proponen el núcleo SHAP: \\[\\pi_{x}(z&#39;)=\\frac{(M-1)}{\\binom{M}{|z&#39;|}|z&#39;|(M-|z&#39;|)}\\] Aquí, M es el tamaño máximo de la coalición y \\(|z&#39;|\\) el número de características presentes en la instancia z. Lundberg y Lee muestran que la regresión lineal con este peso del Kernel produce valores de Shapley. Si usa el núcleo SHAP con LIME en los datos de la coalición, LIME también estimaría los valores de Shapley. Podemos ser un poco más inteligentes sobre el muestreo de coaliciones: Las coaliciones más pequeñas y más grandes ocupan la mayor parte del peso. Obtenemos mejores estimaciones del valor de Shapley utilizando parte del presupuesto de muestreo K para incluir estas coaliciones de alto peso en lugar de muestrear a ciegas. Comenzamos con todas las coaliciones posibles con características 1 y M-1, lo que hace 2 veces más coaliciones M en total. Cuando nos queda suficiente presupuesto (el presupuesto actual es K - 2M), podemos incluir coaliciones con dos características y con características M-2 y así sucesivamente. De los tamaños de coalición restantes, tomamos muestras con pesos reajustados. Tenemos los datos, el objetivo y los pesos. Todo para construir nuestro modelo de regresión lineal ponderado: \\[g(z&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jz_j&#39;\\] Entrenamos el modelo lineal g optimizando la siguiente función de pérdida L: \\[L(f,g,\\pi_{x})=\\sum_{z&#39;\\in{}Z}[f(h_x(z&#39;))-g(z&#39;)]^2\\pi_{x}(z&#39;)\\] donde Z son los datos de entrenamiento. Esta es la vieja y aburrida suma de errores al cuadrado que generalmente optimizamos para los modelos lineales. Los coeficientes estimados del modelo, los \\(\\phi_j\\)s son los valores de Shapley. Como estamos en una configuración de regresión lineal, también podemos hacer uso de las herramientas estándar para la regresión. Por ejemplo, podemos agregar términos de regularización para que el modelo sea escaso. Si agregamos una penalización L1 a la pérdida L, podemos crear explicaciones dispersas. (No estoy tan seguro de si los coeficientes resultantes seguirían siendo valores válidos de Shapley) 5.10.3 TreeSHAP Lundberg et. al (2018)44 propuso TreeSHAP, una variante de SHAP para modelos de aprendizaje automático basados en árboles, como árboles de decisión, random forest y gradient boosted trees. TreeSHAP es rápido, calcula los valores exactos de Shapley y calcula correctamente los valores de Shapley cuando las características dependen. En comparación, KernelSHAP es costoso de calcular y solo se aproxima a los valores reales de Shapley. ¿Cuánto más rápido es TreeSHAP? Para los valores exactos de Shapley, reduce la complejidad computacional de \\(O(TL2^M)\\) a \\(O(TLD^2)\\), donde T es el número de árboles, L es el número máximo de hojas en cualquier árbol y D la profundidad máxima de cualquier árbol. TreeSHAP estima la expectativa condicional correcta \\(E_{X_S|X_C}(f(x)|x_S)\\). Te daré una idea de cómo podemos calcular la predicción esperada para un solo árbol, una instancia xy un subconjunto de características S. Si condicionáramos todas las características (si S fuera el conjunto de todas las características) entonces la predicción del nodo en el que cae la instancia x sería la predicción esperada. Si no condicionáramos ninguna característica (si S estuviera vacío) usaríamos el promedio ponderado de las predicciones de todos los nodos terminales. Si S contiene algunas, pero no todas las características, ignoramos las predicciones de nodos inalcanzables. Inalcanzable significa que la ruta de decisión que conduce a este nodo contradice los valores en \\(x_S\\). A partir de los nodos terminales restantes, promediamos las predicciones ponderadas por el tamaño de los nodos (es decir, el número de muestras de entrenamiento en ese nodo). La media de los nodos terminales restantes, ponderada por el número de instancias por nodo, es la predicción esperada para x dada S. El problema es que tenemos que aplicar este procedimiento para cada posible subconjunto S de los valores de la característica. Afortunadamente, TreeSHAP calcula en tiempo polinómico en lugar de exponencial. La idea básica es empujar todos los subconjuntos posibles S hacia abajo en el árbol al mismo tiempo. Para cada nodo de decisión tenemos que hacer un seguimiento de la cantidad de subconjuntos. Esto depende de los subconjuntos en el nodo principal y de la función de división. Por ejemplo, cuando la primera división en un árbol está en la característica x3, todos los subconjuntos que contienen la característica x3 irán a un nodo (el que va x). Los subconjuntos que no contienen la característica x3 van a ambos nodos con un peso reducido. Desafortunadamente, los subconjuntos de diferentes tamaños tienen diferentes pesos. El algoritmo debe realizar un seguimiento del peso general de los subconjuntos en cada nodo. Esto complica el algoritmo. Me refiero al trabajo original para obtener detalles de TreeSHAP. El cálculo se puede ampliar a más árboles: Gracias a la propiedad de Aditividad de los valores Shapley, los valores Shapley de un conjunto de árboles son el promedio (ponderado) de los valores Shapley de los árboles individuales. A continuación, veremos las explicaciones de SHAP en acción. 5.10.4 Ejemplos Entrené a un clasificador random forest con 100 árboles para predecir el riesgo de cáncer cervical. Usaremos SHAP para explicar las predicciones individuales. Podemos utilizar el método de estimación rápido TreeSHAP en lugar del método más lento KernelSHAP, ya que un random forest es un conjunto de árboles. Como SHAP calcula los valores de Shapley, la interpretación es la misma que en el capítulo del valor de Shapley. Pero con el paquete Python shap viene una visualización diferente: Puedes visualizar las atribuciones de características como los valores de Shapley como fuerzas. Cada valor de característica es una fuerza que aumenta o disminuye la predicción. La predicción comienza desde la línea de base. La línea de base para los valores de Shapley es el promedio de todas las predicciones. En la gráfica, cada valor de Shapley es una flecha que empuja para aumentar (valor positivo) o disminuir (valor negativo) la predicción. Estas fuerzas se equilibran entre sí en la predicción real de la instancia de datos. La siguiente figura muestra gráficos de fuerza de explicación SHAP para dos mujeres del conjunto de datos de cáncer cervical: FIGURA 5.49: Valores SHAP para explicar las probabilidades pronosticadas de cáncer de dos personas. La línea de base -la probabilidad pronosticada promedio- es 0.066. La primera mujer tiene un riesgo bajo predicho de 0.06. Efectos que aumentan el riesgo tales como las ETS se compensan con la disminución de los efectos, como la edad. La segunda mujer tiene un alto riesgo previsto de 0,71. La edad de 51 y 34 años de fumar aumenta su riesgo de cáncer previsto Estas fueron explicaciones para las predicciones individuales. Los valores de Shapley se pueden combinar en explicaciones globales. Si ejecutamos SHAP para cada instancia, obtenemos una matriz de valores de Shapley. Esta matriz tiene una fila por instancia de datos y una columna por entidad. Podemos interpretar todo el modelo analizando los valores de Shapley en esta matriz. Comenzamos con la importancia de la función SHAP. 5.10.5 Importancia de la función SHAP La idea detrás de la importancia de la función SHAP es simple: Las características con grandes valores absolutos de Shapley son importantes. Como queremos la importancia global, promediamos los valores absolutos de Shapley por característica en los datos: \\[I_j=\\sum_{i=1}^n{}|\\phi_j^{(i)}|\\] A continuación, clasificamos las características disminuyendo la importancia y las graficamos. La siguiente figura muestra la importancia de la característica SHAP para el random forest entrenado antes para predecir el cáncer cervical. FIGURA 5.50: Importancia de la característica SHAP medida como los valores medios absolutos de Shapley. El número de años con anticonceptivos hormonales fue la característica más importante, cambiando la probabilidad absoluta pronosticada de cáncer en un promedio de 2.4 puntos porcentuales (0.024 en x -axis). La importancia de la característica SHAP es una alternativa a importancia de la característica de permutación. Hay una gran diferencia entre ambas medidas de importancia: La importancia de la característica de permutación se basa en la disminución del rendimiento del modelo. La importancia de la característica de permutación se basa en la disminución del rendimiento del modelo. SHAP se basa en la magnitud de las atribuciones de características. El diagrama de importancia de la característica es útil, pero no contiene información más allá de las importancias. Para una trama más informativa, veremos a continuación la trama de resumen. 5.10.6 Gráfico de resumen SHAP La gráfica de resumen combina la importancia de la característica con los efectos de la característica. Cada punto en el diagrama de resumen es un valor de Shapley para una entidad y una instancia. La posición en el eje y está determinada por la característica y en el eje x por el valor de Shapley. El color representa el valor de la característica de menor a mayor. Los puntos superpuestos se fluctúan en la dirección del eje y, por lo que tenemos una idea de la distribución de los valores de Shapley por entidad. Las características se ordenan según su importancia. FIGURA 5.51: Gráfico de resumen SHAP. El bajo número de años con anticonceptivos hormonales reduce el riesgo de cáncer previsto, un gran número de años aumenta el riesgo. Recordatorio habitual: Todos los efectos describen el comportamiento del modelo y son no necesariamente causales en el mundo real En el gráfico de resumen, vemos las primeras indicaciones de la relación entre el valor de una característica y el impacto en la predicción. Pero para ver la forma exacta de la relación, tenemos que mirar las gráficas de dependencia SHAP. 5.10.7 SHAP Gráfico de dependencia La dependencia de la función SHAP podría ser la trama de interpretación global más simple: 1) Elige una característica. 2) Para cada instancia de datos, traza un punto con el valor de la característica en el eje xy el valor de Shapley correspondiente en el eje y. 3) Hecho. Matemáticamente, la gráfica contiene los siguientes puntos: \\(\\{(x_j^{(i)},\\phi_j^{(i)})\\}_{i=1}^n\\) La siguiente figura muestra la dependencia de la función SHAP para la variable años de los anticonceptivos hormonales: FIGURA 5.52: Gráfico de dependencia SHAP para la variable años de anticonceptivos hormonales. En comparación con 0 años, 1 años disminuyen la probabilidad pronosticada y un alto número de años aumenta la probabilidad pronosticada de cáncer Las gráficas de dependencia SHAP son una alternativa a las gráficas de dependencia parcial y efectos locales acumulados. Mientras que la gráfica PDP y ALE muestran efectos promedio, la dependencia SHAP también muestra la varianza en el eje y. Especialmente en caso de interacciones, la gráfica de dependencia SHAP estará mucho más dispersa en el eje y. El diagrama de dependencia se puede mejorar resaltando estas interacciones de características. 5.10.8 Valores de interacción SHAP El efecto de interacción es el efecto de característica combinada adicional después de tener en cuenta los efectos de característica individuales. El índice de interacción Shapley de la teoría de juegos se define como: \\[\\phi_{i,j}=\\sum_{S\\subseteq\\setminus\\{i,j\\}}\\frac{|S|!(M-|S|-2)!}{2(M-1)!}\\delta_{ij}(S)\\] cuando \\(i\\neq{}j\\) y: \\[\\delta_{ij}(S)=f_x(S\\cup\\{i,j\\})-f_x(S\\cup\\{i\\})-f_x(S\\cup\\{j\\})+f_x(S)\\] Esta fórmula resta el efecto principal de las características para que obtengamos el efecto de interacción pura después de tener en cuenta los efectos individuales. Promediamos los valores sobre todas las posibles coaliciones de características S, como en el cálculo del valor de Shapley. Cuando calculamos los valores de interacción SHAP para todas las características, obtenemos una matriz por instancia con dimensiones MxM, donde M es el número de características. ¿Cómo podemos usar el índice de interacción? Por ejemplo, para colorear automáticamente el gráfico de dependencia de la función SHAP con la interacción más fuerte: FIGURA 5.53: SHAP presenta un gráfico de dependencia con visualización de interacción. Los años con anticonceptivos hormonales interactúan con las ETS. En casos cercanos a 0 años, la aparición de una ETS aumenta el riesgo de cáncer previsto. Durante más años con anticonceptivos, la aparición de una ETS reduce el riesgo previsto. Una vez más, este no es un modelo causal. Los efectos pueden deberse a confusión (por ejemplo, las ETS y un menor riesgo de cáncer podrían correlacionarse con más visitas al médico). 5.10.9 Agrupando valores SHAP Puedes agrupar tus datos con la ayuda de los valores de Shapley. El objetivo de la agrupación es encontrar grupos de instancias similares. Normalmente, la agrupación se basa en características. Las características son a menudo en diferentes escalas. Por ejemplo, la altura se puede medir en metros, la intensidad del color de 0 a 100 y algo de salida del sensor entre -1 y 1. La dificultad es calcular distancias entre instancias con características tan diferentes y no comparables. La agrupación SHAP funciona agrupando en valores Shapley de cada instancia. Esto significa que agrupa las instancias por similitud de explicación. Todos los valores SHAP tienen la misma unidad: la unidad del espacio de predicción. Puedes usar cualquier método de agrupación. El siguiente ejemplo utiliza agrupación jerárquica aglomerativa para ordenar las instancias. La trama consta de muchas gráficas de fuerza, cada una de las cuales explica la predicción de una instancia. Rotamos las gráficas de fuerza verticalmente y las colocamos una al lado de la otra según su similitud de agrupamiento. FIGURA 5.54: Explicaciones SHAP apiladas agrupadas por similitud de explicación. Cada posición en el eje x es una instancia de los datos. Los valores SHAP rojos aumentan la predicción, los valores azules la disminuyen. Un grupo se destaca: A la derecha hay un grupo con un alto riesgo de cáncer previsto 5.10.10 Ventajas Dado que SHAP calcula los valores de Shapley, se aplican todas las ventajas de los valores de Shapley: SHAP tiene una base teórica sólida en teoría de juegos. La predicción está bastante distribuida entre los valores de las características. Obtenemos explicaciones contrastantes que comparan la predicción con la predicción promedio. SHAP conecta los valores Shapley y LIME. Esto es muy útil para comprender mejor ambos métodos. También ayuda a unificar el campo del aprendizaje automático interpretable. SHAP tiene una implementación rápida para modelos basados en árboles. Creo que esto fue clave para la popularidad de SHAP, porque la mayor barrera para la adopción de los valores de Shapley es el cálculo lento. El cálculo rápido permite calcular los muchos valores de Shapley necesarios para las interpretaciones del modelo global. Los métodos de interpretación global incluyen la importancia de la característica, la dependencia de la característica, las interacciones, la agrupación y las gráficas de resumen. Con SHAP, las interpretaciones globales son consistentes con las explicaciones locales, ya que los valores de Shapley son la unidad atómica de las interpretaciones globales. Si usas LIME para explicaciones locales y gráficos de dependencia parcial más la importancia de la característica de permutación para explicaciones globales, careces de una base común. 5.10.11 Desventajas KernelSHAP es lento. Esto hace que KernelSHAP sea poco práctico de usar cuando desees calcular los valores de Shapley para muchas instancias. Además, todos los métodos SHAP globales, como la importancia de la función SHAP, requieren calcular los valores de Shapley para muchas instancias. KernelSHAP ignora la dependencia de características. La mayoría de los otros métodos de interpretación basados en permutación tienen este problema. Al reemplazar los valores de entidad con valores de instancias aleatorias, generalmente es más fácil muestrear aleatoriamente de la distribución marginal. Sin embargo, si las características son dependientes, por ejemplo correlacionadas, esto lleva a poner demasiado peso en puntos de datos poco probables. TreeSHAP resuelve este problema modelando explícitamente la predicción condicional esperada. Las desventajas de los valores de Shapley también se aplican a SHAP: Los valores de Shapley pueden malinterpretarse y se necesita acceso a los datos para calcularlos para nuevos datos (a excepción de TreeSHAP). 5.10.12 Software Los autores implementaron SHAP en el paquete Python shap. Esta implementación funciona para modelos basados en árboles en la biblioteca de aprendizaje automático scikit-learn para Python. El paquete shap también se usó para los ejemplos de este capítulo. SHAP está integrado en los marcos de refuerzo de árbol xgboost y LightGBM. En R, está el paquete shapper. SHAP también se incluye en el paquete R xgboost. Lundberg, Scott M., and Su-In Lee. A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems. 2017. Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. arXiv preprint arXiv:1802.03888 (2018). "],["basadoenejemplos.html", "Capítulo 6 Explicaciones basadas en ejemplos", " Capítulo 6 Explicaciones basadas en ejemplos Los métodos de explicación basados en ejemplos seleccionan instancias particulares del conjunto de datos para explicar el comportamiento de los modelos de aprendizaje automático o para explicar la distribución de datos subyacente. Las explicaciones basadas en ejemplos son en su mayoría independientes del modelo, porque hacen que cualquier modelo de aprendizaje automático sea más interpretable. La diferencia con los métodos independientes del modelo es que los métodos basados en ejemplos explican un modelo seleccionando instancias del conjunto de datos y no creando resúmenes de características (como importancia de la característica o dependencia parcial). Las explicaciones basadas en ejemplos solo tienen sentido si podemos representar una instancia de los datos de una manera humanamente comprensible. Esto funciona bien para las imágenes, porque podemos verlas directamente. En general, los métodos basados en ejemplos funcionan bien si los valores de características de una instancia tienen más contexto, lo que significa que los datos tienen una estructura, como lo hacen las imágenes o los textos. Es más difícil representar datos tabulares de manera significativa, porque una instancia puede constar de cientos o miles de características (menos estructuradas). Enumerar todos los valores de características para describir una instancia generalmente no es útil. Funciona bien si solo hay un puñado de características o si tenemos una manera de resumir una instancia. Las explicaciones basadas en ejemplos ayudan a los humanos a construir modelos mentales del modelo de aprendizaje automático y los datos sobre los que se ha entrenado el modelo de aprendizaje automático. Especialmente ayuda a comprender distribuciones de datos complejas. Pero, ¿qué quiero decir con explicaciones basadas en ejemplos? A menudo los usamos en nuestros trabajos y en nuestra vida diaria. Comencemos con algunos ejemplos45. Un médico atiende a un paciente con tos inusual y fiebre leve. Los síntomas del paciente le recuerdan a otro paciente que tuvo años atrás con síntomas similares. Ella sospecha que su paciente actual podría tener la misma enfermedad y toma una muestra de sangre para detectar esta enfermedad específica. Un científico de datos trabaja en un nuevo proyecto para uno de sus clientes: Análisis de los factores de riesgo que conducen a la falla de las máquinas de producción para teclados. El científico de datos recuerda un proyecto similar en el que trabajó y reutiliza partes del código del proyecto anterior porque cree que el cliente desea el mismo análisis. Un gatito se sienta en la parte baja de la ventana de una casa en llamas y deshabitada. El departamento de bomberos ya llegó y uno de los bomberos reflexiona por un segundo si puede arriesgarse a entrar al edificio para salvar al gatito. Él recuerda casos similares en su vida como bombero: Las viejas casas de madera que han estado ardiendo lentamente durante algún tiempo a menudo eran inestables y finalmente colapsaron. Debido a la similitud de este caso, decide no ingresar, porque el riesgo de que la casa se derrumbe es demasiado grande. Afortunadamente, el gatito salta por la ventana, aterriza con seguridad y nadie resulta herido en el fuego. Final feliz. Estas historias ilustran cómo los humanos pensamos en ejemplos o analogías. El modelo de explicaciones basadas en ejemplos es: La cosa B es similar a la cosa A y A causó Y, así que predigo que B también causará Y. Implícitamente, algunos enfoques de aprendizaje automático funcionan basados en ejemplos. Los árboles de decisión dividen los datos en nodos según las similitudes de los puntos de datos en las características que son importantes para predecir el objetivo. Un árbol de decisión obtiene la predicción para una nueva instancia de datos al encontrar las instancias que son similares (= en el mismo nodo terminal) y devolver el promedio de los resultados de esas instancias como la predicción. El método de vecinos más cercanos a k (knn) funciona explícitamente con predicciones basadas en ejemplos. Para una nueva instancia, un modelo knn localiza los k vecinos más cercanos (por ejemplo, k = 3 instancias más cercanas) y devuelve el promedio de los resultados de esos vecinos como predicción. La predicción de un knn puede explicarse devolviendo los k vecinos, lo cual, nuevamente, solo es significativo si tenemos una buena manera de representar una sola instancia. Los capítulos de esta parte cubren los siguientes métodos de interpretación basados en ejemplos: Explicaciones contrafactuales nos dice cómo debe cambiar una instancia para cambiar significativamente su predicción. Al crear instancias contrafácticas, aprendemos cómo el modelo hace sus predicciones y puede explicar predicciones individuales. Ejemplos adversos son contrafactuales utilizados para engañar a los modelos de aprendizaje automático. El énfasis está en voltear la predicción y no explicarla. Prototipos son una selección de instancias representativas de los datos y las excepciones son instancias que no están bien representadas por esos prototipos. Instancias influyentes son los puntos de datos de entrenamiento que fueron los más influyentes para los parámetros de un modelo de predicción o las predicciones mismas. Identificar y analizar instancias influyentes ayuda a encontrar problemas con los datos, depurar el modelo y comprender mejor el comportamiento del modelo. k-modelo de vecinos más cercanos: un modelo de aprendizaje automático (interpretable) basado en ejemplos. Aamodt, Agnar, and Enric Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. AI communications 7.1 (1994): 39-59. "],["contrafactual.html", "6.1 Explicaciones contrafácticas", " 6.1 Explicaciones contrafácticas Una explicación contrafáctica describe una situación causal en la forma: Si X no hubiera ocurrido, Y no habría ocurrido. Por ejemplo: Si no hubiera tomado un sorbo de este café caliente, no me habría quemado la lengua. El evento Y es que me quemé la lengua; porque X es que tomé un café caliente. Pensar en contrafácticas requiere imaginar una realidad hipotética que contradiga los hechos observados (por ejemplo, un mundo en el que no he bebido el café caliente), de ahí el nombre de contrafáctico. La capacidad de pensar en contrafáctico nos hace humanos tan inteligentes en comparación con otros animales. En el aprendizaje automático interpretable, se pueden usar explicaciones contrafácticas para explicar las predicciones de instancias individuales. El evento es el resultado predicho de una instancia, las causas son los valores de características particulares de esta instancia que se introdujeron en el modelo y causaron cierta predicción. Mostrada como un gráfico, la relación entre las entradas y la predicción es muy simple: Los valores de las características causan la predicción. FIGURA 6.1: Las relaciones causales entre las entradas de un modelo de aprendizaje automático y las predicciones, cuando el modelo se ve simplemente como una caja negra. Las entradas causan la predicción (no necesariamente refleja la relación causal real de los datos). Incluso si en realidad la relación entre las entradas y el resultado a predecir podría no ser causal, podemos ver las entradas de un modelo como la causa de la predicción. Dado este simple gráfico, es fácil ver cómo podemos simular contrafácticas para predicciones de modelos de aprendizaje automático: Simplemente cambiamos los valores de las características de una instancia antes de hacer las predicciones y analizamos cómo cambia la predicción. Estamos interesados en escenarios en los que la predicción cambia de manera relevante, como un cambio en la clase pronosticada (por ejemplo, solicitud de crédito aceptada o rechazada) o en la que la predicción alcanza un cierto umbral (por ejemplo, la probabilidad de cáncer alcanza el 10%). Una explicación contrafáctica de una predicción describe el cambio más pequeño en los valores de la característica que cambia la predicción a una salida predefinida. El método de explicación contrafáctico es independiente del modelo, ya que solo funciona con las entradas y salidas del modelo. Este método también se sentiría como en casa en el capítulo de modelos agnósticos, ya que la interpretación puede expresarse como un resumen de las diferencias en los valores de las características (cambiar las características A y B para cambiar la predicción). Pero una explicación contrafáctica es en sí misma una nueva instancia, por lo que vive en este capítulo (a partir de la instancia X, cambie A y B para obtener una instancia contrafáctica). A diferencia de prototipos, los contrafácticos no tienen que ser instancias reales de los datos de entrenamiento, sino que pueden ser una nueva combinación de valores de características. Antes de discutir cómo crear contrafácticos, me gustaría discutir algunos casos de uso para contrafácticos y cómo se ve una buena explicación contrafáctica. En este primer ejemplo, Pedro solicita un préstamo y es rechazado por el software bancario (con tecnología de aprendizaje automático). Se pregunta por qué se rechazó su solicitud y cómo podría mejorar sus posibilidades de obtener un préstamo. La pregunta de por qué puede formularse como contrafáctico: ¿Cuál es el cambio más pequeño en las características (ingresos, número de tarjetas de crédito, edad, ) que cambiarían la predicción de rechazado a aprobado? Una posible respuesta podría ser: Si Pedro ganara 10.000 euros más por año, obtendría el préstamo. O si Pedro tuviera menos tarjetas de crédito y no hubiera incumplido un préstamo hace 5 años, obtendría el préstamo. Pedro nunca sabrá los motivos del rechazo, ya que el banco no tiene interés en la transparencia, pero esa es otra historia. En nuestro segundo ejemplo, queremos explicar un modelo que predice un resultado continuo con explicaciones contrafácticas. Anna quiere alquilar su apartamento, pero no está segura de cuánto cobrar por él, por lo que decide entrenar un modelo de aprendizaje automático para predecir el alquiler. Por supuesto, dado que Anna es científica de datos, así es como resuelve sus problemas. Después de ingresar todos los detalles sobre el tamaño, la ubicación, si se permiten mascotas, etc., el modelo le dice que puede cobrar 900 euros. Esperaba 1000 euros o más, pero confía en su modelo y decide jugar con los valores característicos del apartamento para ver cómo puede mejorar el valor del apartamento. Ella descubre que el apartamento podría alquilarse por más de 1000 euros, si fuera 15 m2 más grande. Conocimiento interesante, pero no procesable, porque no puede ampliar su departamento. Finalmente, al ajustar solo los valores de las características bajo su control (cocina incorporada sí / no, mascotas permitidas sí / no, tipo de piso, etc.), descubre que si permite mascotas e instala ventanas con mejor aislamiento, puede cobrar 1000 euros. Anna había trabajado intuitivamente con los contrafácticos para cambiar el resultado. Los contrafácticos son explicaciones amigables para los humanos, porque son contrastantes con la instancia actual y porque son selectivos, lo que significa que generalmente se centran en un pequeño número de cambios de características. Pero los contrafácticos sufren el efecto Rashomon. Rashomon es una película japonesa en la que diferentes personas cuentan el asesinato de un samurai. Cada una de las historias explica el resultado igualmente bien, pero las historias se contradicen entre sí. Lo mismo también puede suceder con los contrafácticos, ya que generalmente hay múltiples explicaciones contrafácticas diferentes. Cada contrafáctico cuenta una historia diferente de cómo se alcanzó cierto resultado. Un contrafáctico podría decir que cambie la característica A, el otro contrafáctico podría decir que deje a A igual pero cambie la característica B, lo cual es una contradicción. Este problema de las verdades múltiples puede abordarse ya sea informando todas las explicaciones contrafácticas o teniendo un criterio para evaluar las contrafácticas y seleccionar la mejor. Hablando de criterios, ¿cómo definimos una buena explicación contrafáctica? Primero, el usuario de una explicación contrafáctica define un cambio relevante en la predicción de una instancia (= la realidad alternativa), por lo que un primer requisito obvio es que una instancia contrafáctica produce la predicción predefinida lo más cerca posible. No siempre es posible hacer coincidir exactamente la salida predefinida. En un entorno de clasificación con dos clases, una clase rara y una clase frecuente, el modelo siempre podría clasificar una instancia como la clase frecuente. Cambiar los valores de entidad para que la etiqueta predicha cambie de la clase común a la clase rara podría ser imposible. Por lo tanto, queremos relajar el requisito de que el resultado predicho del contrafáctico debe corresponder exactamente al resultado definido. En el ejemplo de clasificación, podríamos buscar un contrafáctico en el que la probabilidad predicha de la clase rara se incremente al 10% en lugar del 2% actual. La pregunta es, ¿cuáles son los cambios mínimos en las características para que la probabilidad pronosticada cambie del 2% al 10% (o cerca del 10%)? Otro criterio de calidad es que un contrafáctico debe ser lo más similar posible a la instancia con respecto a los valores de características. Esto requiere una medida de distancia entre dos instancias. El contrafáctico no solo debe estar cerca de la instancia original, sino que también debe cambiar la menor cantidad de características posible. Esto se puede lograr seleccionando una medida de distancia apropiada, como la distancia de Manhattan. El último requisito es que una instancia contrafáctica debe tener valores de características que probablemente. No tendría sentido generar una explicación contrafáctica para el ejemplo de alquiler donde el tamaño de un apartamento es negativo o el número de habitaciones se establece en 200. Es incluso mejor cuando el contrafáctico es probable de acuerdo con la distribución conjunta de los datos, por ejemplo un apartamento con 10 habitaciones y 20 m2 no debe considerarse una explicación contrafáctica. 6.1.1 Generando explicaciones contrafácticas Un enfoque simple e ingenuo para generar explicaciones contrafácticas es buscar por ensayo y error. Este enfoque implica cambiar aleatoriamente los valores de las características de la instancia de interés y detenerse cuando se predice la salida deseada. Como el ejemplo en el que Anna trató de encontrar una versión de su departamento por la cual pudiera cobrar más renta. Pero hay mejores enfoques que prueba y error. Primero, definimos una función de pérdida que toma como entrada la instancia de interés, un resultado contrafáctico y el resultado deseado (contrafáctico). La pérdida mide qué tan lejos está el resultado predicho del contrafáctico del resultado predefinido y qué tan lejos está el contrafáctico de la instancia de interés. Podemos optimizar la pérdida directamente con un algoritmo de optimización o buscando alrededor de la instancia, como se sugiere en el método Growing Spheres (consulte Software y alternativas). En esta sección, presentaré el enfoque sugerido por Wachter et. al (2017)46. Sugieren minimizar la siguiente pérdida. \\[L(x,x^\\prime,y^\\prime,\\lambda)=\\lambda\\cdot(\\hat{f}(x^\\prime)-y^\\prime)^2+d(x,x^\\prime)\\] El primer término es la distancia cuadrática entre la predicción del modelo para el contrafáctico x y el resultado deseado y, que el usuario debe definir de antemano. El segundo término es la distancia d entre la instancia x que se explicará y la contrafáctica x , pero más sobre esto más adelante. El parámetro \\(\\lambda\\) equilibra la distancia en predicción (primer término) contra la distancia en valores de características (segundo término). La pérdida se resuelve para un \\(\\lambda\\) dado y devuelve un x contrafáctico. Un valor más alto de \\(\\lambda\\) significa que preferimos los contrafácticos que se acercan al resultado deseado y, un valor más bajo significa que preferimos los contrafácticos x que son muy similares a x en los valores de la característica. Si \\(\\lambda\\) es muy grande, se seleccionará la instancia con la predicción más cercana a y, independientemente de qué tan lejos esté de x. Finalmente, el usuario debe decidir cómo equilibrar el requisito de que la predicción para el contrafáctico coincida con el resultado deseado con el requisito de que el contrafáctico es similar a x. Los autores del método sugieren en lugar de seleccionar un valor para \\(\\lambda\\) para seleccionar una tolerancia \\(\\epsilon\\) para la distancia a la que se permite la predicción de la instancia contrafáctica de y. Esta restricción se puede escribir como: \\[|\\hat{f}(x^\\prime)-y^\\prime|\\leq\\epsilon\\] Para minimizar esta función de pérdida, se puede usar cualquier algoritmo de optimización adecuado, por ejemplo Nelder-Mead. Si tienes acceso a los gradientes del modelo de aprendizaje automático, puedes utilizar métodos basados en gradientes como ADAM. La instancia x que se explicará, la salida deseada y y el parámetro de tolerancia \\(\\epsilon\\) se deben establecer de antemano. La función de pérdida se minimiza para x y la contrafáctica óptima (localmente) x se devuelve al aumentar \\(\\lambda\\) hasta que se encuentre una solución suficientemente cercana (= dentro del parámetro de tolerancia). \\[\\arg\\min_{x^\\prime}\\max_{\\lambda}L(x,x^\\prime,y^\\prime,\\lambda)\\] La función d para medir la distancia entre la instancia x y la contrafáctica x es la característica de distancia ponderada de Manhattan con la desviación absoluta media inversa (MAD). \\[d(x,x^\\prime)=\\sum_{j=1}^p\\frac{|x_j-x^\\prime_j|}{MAD_j}\\] La distancia total es la suma de todas las distancias p en función de las características, es decir, las diferencias absolutas de los valores de características entre la instancia x y la contrafáctica x. Las distancias en función de las características se escalan mediante la inversa de la desviación absoluta media de la característica j sobre el conjunto de datos definido como: \\[MAD_j=\\text{median}_{i\\in{}\\{1,\\ldots,n\\}}(|x_{i,j}-\\text{median}_{l\\in{}\\{1,\\ldots,n\\}}(x_{l,j})|)\\] La mediana de un vector es el valor en el que la mitad de los valores del vector son mayores y la otra mitad es menor. El MAD es el equivalente de la varianza de una entidad, pero en lugar de usar la media como el centro y sumar sobre las distancias cuadradas, usamos la mediana como el centro y la suma sobre las distancias absolutas. La función de distancia propuesta tiene la ventaja sobre la distancia euclidiana de que introduce escasez. Esto significa que dos puntos están más cerca uno del otro cuando menos características son diferentes. Y es más robusto para los valores atípicos. Es necesario escalar con MAD para llevar todas las características a la misma escala; no importa si mide el tamaño de un apartamento en metros cuadrados o pies cuadrados. La receta para producir los contrafácticos es simple: Selecciona una instancia x para explicar, el resultado deseado y, una tolerancia \\(\\epsilon\\) y un valor inicial (bajo) para \\(\\lambda\\). Muestrea una instancia aleatoria como contrafáctico inicial. Optimiza la pérdida con el contrafáctico muestreado inicialmente como punto de partida. Mientras \\(|\\hat{f}(x^\\prime)-y^\\prime|&gt;\\epsilon\\):: Aumenta \\(\\lambda\\). Optimiza la pérdida con el contrafáctico actual como punto de partida. Devuelve el contrafáctico que minimiza la pérdida. Repite los pasos 2 a 4 y devuelva la lista de contrafácticos o la que minimiza la pérdida. 6.1.2 Ejemplos Ambos ejemplos son del trabajo de Wachter et. al (2017). En el primer ejemplo, los autores entrenan una red neuronal totalmente conectada de tres capas para predecir la calificación promedio de un estudiante del primer año en la facultad de derecho, con base en el promedio de calificaciones (GPA) antes del examen de ingreso a la facultad de derecho, la raza y las puntuaciones en el examen. El objetivo es encontrar explicaciones contrafácticas para cada estudiante que respondan a la siguiente pregunta: ¿Cómo deberían cambiarse las características de entrada para obtener una puntuación prevista de 0? Como los puntajes se han normalizado anteriormente, un estudiante con un puntaje de 0 es tan bueno como el promedio de los estudiantes. Un puntaje negativo significa un resultado por debajo del promedio, un puntaje positivo un resultado por encima del promedio. La siguiente tabla muestra los contrafácticos aprendidos: Score GPA LSAT Race GPA x LSAT x Race x 0.17 3.1 39.0 0 3.1 34.0 0 0.54 3.7 48.0 0 3.7 32.4 0 -0.77 3.3 28.0 1 3.3 33.5 0 -0.83 2.4 28.5 1 2.4 35.8 0 -0.57 2.7 18.3 0 2.7 34.9 0 La primera columna contiene la puntuación pronosticada, las siguientes 3 columnas los valores de características originales y las últimas 3 columnas los valores de características contrafácticos que dan como resultado una puntuación cercana a 0. Las primeras dos filas son estudiantes con predicciones superiores al promedio, las otras tres filas inferiores al promedio. Los contrafácticos para las dos primeras filas describen cómo las características del estudiante tendrían que cambiar para disminuir la puntuación prevista y para los otros tres casos cómo tendrían que cambiar para aumentar la puntuación al promedio. Los contrafácticos para aumentar el puntaje siempre cambian la raza de negro (codificado con 1) a blanco (codificado con 0), lo que muestra un sesgo racial del modelo. El GPA no se cambia en los contrafácticos, pero sí LSAT. El segundo ejemplo muestra explicaciones contrafácticas para el riesgo predicho de diabetes. Una red neuronal totalmente conectada de tres capas está entrenada para predecir el riesgo de diabetes según la edad, el IMC, el número de embarazos, etc. para las mujeres de ascendencia Pima. Los contrafácticos responden a la pregunta: ¿Qué valores de las características deben cambiarse para aumentar o disminuir la puntuación de riesgo de diabetes a 0.5? Se encontraron los siguientes contrafácticos: Persona 1: si su nivel de insulina en suero a las 2 horas fuera 154.3, tendría una puntuación de 0.51 Persona 2: si su nivel de insulina en suero a las 2 horas fuera 169.5, tendría un puntaje de 0.51 Persona 3: si su concentración de glucosa en plasma fuera 158.3 y su nivel de insulina sérica a las 2 horas fuera 160.5, tendría un puntaje de 0.51 6.1.3 Ventajas La interpretación de las explicaciones contrafácticas es muy clara. Si los valores de característica de una instancia se cambian de acuerdo con el contrafáctico, la predicción cambia a la predicción predefinida. No hay suposiciones adicionales ni magia en el fondo. Esto también significa que no es tan peligroso como métodos como LIME, donde no está claro hasta qué punto podemos extrapolar el modelo local para la interpretación. El método contrafáctico crea una nueva instancia, pero también podemos resumir un contrafáctico al informar qué valores de características han cambiado. Esto nos da dos opciones para informar nuestros resultados. Puedes informar la instancia contrafáctica o resaltar qué características se han cambiado entre la instancia de interés y la instancia contrafáctica. El método contrafáctico no requiere acceso a los datos o al modelo. Solo requiere acceso a la función de predicción del modelo, que también funcionaría a través de una API web, por ejemplo. Esto es atractivo para las empresas que son auditadas por terceros o que ofrecen explicaciones a los usuarios sin revelar el modelo o los datos. Una empresa tiene interés en proteger el modelo y los datos debido a secretos comerciales o razones de protección de datos. Las explicaciones contrafácticas ofrecen un equilibrio entre explicar las predicciones del modelo y proteger los intereses del propietario del modelo. El método también funciona con sistemas que no utilizan aprendizaje automático. Podemos crear contrafácticos para cualquier sistema que reciba entradas y devuelva salidas. El sistema que predice alquileres de apartamentos también podría consistir en reglas escritas a mano, y las explicaciones contrafácticas seguirían funcionando. El método de explicación contrafáctico es relativamente fácil de implementar, ya que es esencialmente una función de pérdida que se puede optimizar con bibliotecas estándar. Se deben tener en cuenta algunos detalles adicionales, como limitar los valores de las características a rangos significativos (por ejemplo, solo tamaños de apartamentos positivos). 6.1.4 Desventajas Para cada caso, generalmente encontrará múltiples explicaciones contrafácticas (efecto Rashomon). Esto es inconveniente: la mayoría de las personas prefieren explicaciones simples sobre la complejidad del mundo real. También es un desafío práctico. Digamos que generamos 23 explicaciones contrafácticas para una instancia. ¿Los estamos informando a todos? ¿Solo lo mejor? ¿Qué pasa si todos son relativamente buenos, pero muy diferentes? Estas preguntas deben ser respondidas de nuevo para cada proyecto. También puede ser ventajoso tener múltiples explicaciones contrafácticas, porque entonces los humanos pueden seleccionar las que corresponden a sus conocimientos previos. No hay garantía de que para una tolerancia dada \\(\\epsilon\\) se encuentre una instancia contrafáctica. Eso no es necesariamente culpa del método, sino que depende de los datos. El método propuesto no maneja características categóricas con muchos niveles diferentes bien. Los autores del método sugirieron ejecutar el método por separado para cada combinación de valores de características de las características categóricas, pero esto conducirá a una explosión combinatoria si tiene múltiples características categóricas con muchos valores. Por ejemplo, 6 características categóricas con 10 niveles únicos significarían 1 millón de ejecuciones. Martens et. al (2014) Propusieron una solución solo para características categóricas47. En el paquete Python Alibi se implementa una solución que maneja variables numéricas y categóricas con una forma de principio de generar perturbaciones para variables categóricas. .html). 6.1.5 Software y alternativas Las explicaciones contrafácticas se implementan en el paquete Python Alibi. Los autores del paquete implementan un método contrafáctico simple así como un método extendido que utiliza prototipos de clase para mejorar la interpretabilidad y la convergencia de las salidas del algoritmo48. Martens et. al (2014) Propusieron un enfoque muy similar. al para explicar las clasificaciones de documentos. En su trabajo, se enfocan en explicar por qué un documento fue o no clasificado como una clase en particular. La diferencia con el método presentado en este capítulo es que Martens et. al (2014) se centran en clasificadores de texto, que tienen apariciones de palabras como entradas. Una forma alternativa de buscar datos contrafácticos es el algoritmo Growing Spheres de Laugel et. al (2017)49. El método primero dibuja una esfera alrededor del punto de interés, muestrea puntos dentro de esa esfera, verifica si uno de los puntos muestreados produce la predicción deseada, contrae o expande la esfera en consecuencia hasta que se encuentra un contrafáctico (escaso) y finalmente se devuelve. No usan la palabra contrafáctico en su trabajo, pero el método es bastante similar. También definen una función de pérdida que favorece los contrafácticos con el menor número posible de cambios en los valores de las características. En lugar de optimizar directamente la función, sugieren la búsqueda mencionada anteriormente con esferas. FIGURA 6.2: Una ilustración de Growing Spheres y seleccionando contrafácticos dispersos de Laugel et. al (2017). Las anclas son lo opuesto a los contrafácticos. Las anclas responden la pregunta: ¿Qué características son suficientes para anclar una predicción, es decir, cambiar las otras características no puede cambiar la predicción? Una vez que hayamos encontrado características que sirven como anclajes para una predicción, ya no encontraremos instancias contrafácticas cambiando las características no utilizadas en el ancla. FIGURA 6.3: Ejemplos de anclajes de Ribeiro et. al (2018). Wachter, Sandra, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. (2017). Martens, David, and Foster Provost. Explaining data-driven document classifications. (2014). Van Looveren, Arnaud, and Janis Klaise. Interpretable Counterfactual Explanations Guided by Prototypes. arXiv preprint arXiv:1907.02584 (2019). Laugel, Thibault, et al. Inverse classification for comparison-based interpretability in machine learning. arXiv preprint arXiv:1712.08443 (2017). "],["adversarial.html", "6.2 Ejemplos adversos", " 6.2 Ejemplos adversos Un ejemplo adverso es una instancia con pequeñas perturbaciones de características intencionales que hacen que un modelo de aprendizaje automático haga una predicción falsa. Recomiendo leer el capítulo sobre Explicaciones contrafácticas primero, ya que los conceptos son muy similares. Los ejemplos adversos son ejemplos contrafácticos con el objetivo de engañar al modelo, no interpretarlo. ¿Por qué nos interesan los ejemplos adversos? ¿No son solo subproductos curiosos de los modelos de aprendizaje automático sin relevancia práctica? La respuesta es un claro no. Los ejemplos adversos hacen que los modelos de aprendizaje automático sean vulnerables a los ataques, como en los siguientes escenarios. Un auto sin conductor choca con otro porque ignora una señal de alto. Alguien había colocado una imagen sobre el letrero, que parece una señal de alto con un poco de suciedad para los humanos, pero fue diseñado para parecerse a un letrero de prohibición de estacionamiento para el software de reconocimiento de letreros del automóvil. Un detector de spam no puede clasificar un correo electrónico como spam. El correo no deseado ha sido diseñado para parecerse a un correo electrónico normal, pero con la intención de engañar al destinatario. Un escáner automático de aprendizaje automático escanea las maletas en busca de armas en el aeropuerto. Se desarrolló un cuchillo para evitar la detección al hacer que el sistema piense que es un paraguas. Echemos un vistazo a algunas formas de crear ejemplos adversos. 6.2.1 Métodos y ejemplos Existen muchas técnicas para crear ejemplos adversos. La mayoría de los enfoques sugieren minimizar la distancia entre el ejemplo de confrontación y la instancia a manipular, mientras se cambia la predicción al resultado deseado (de confrontación). Algunos métodos requieren acceso a los gradientes del modelo, que por supuesto solo funciona con modelos basados en gradientes como las redes neuronales, otros métodos solo requieren acceso a la función de predicción, lo que hace que estos métodos sean independientes del modelo. Los métodos de esta sección se centran en clasificadores de imágenes con redes neuronales profundas, ya que se realiza mucha investigación en esta área y la visualización de imágenes adversas es muy educativa. Ejemplos adversos de imágenes son imágenes con píxeles intencionalmente perturbados con el objetivo de engañar al modelo durante el tiempo de aplicación. Los ejemplos demuestran de manera impresionante la facilidad con que las redes neuronales profundas para el reconocimiento de objetos pueden ser engañadas por imágenes que parecen inofensivas para los humanos. Si aún no has visto estos ejemplos, puedes sorprenderte, porque los cambios en las predicciones son incomprensibles para un observador humano. Los ejemplos adversos son como ilusiones ópticas pero para máquinas. Algo está mal con mi perro Szegedy et. al (2013)50 utilizó un enfoque de optimización basado en gradiente en su trabajo Propiedades intrigantes de las redes neuronales para encontrar ejemplos adversos para redes neuronales profundas. FIGURA 6.4: Ejemplos adversos para AlexNet de Szegedy et. al (2013). Todas las imágenes en la columna izquierda están clasificadas correctamente. La columna central muestra el error (ampliado) agregado a las imágenes para producir las imágenes en la columna derecha, todas clasificadas (incorrectamente) como Avestruz. Estos ejemplos adversos se generaron minimizando la siguiente función con respecto a r: \\[loss(\\hat{f}(x+r),l)+c\\cdot|r|\\] En esta fórmula, x es una imagen (representada como un vector de píxeles), r son los cambios en los píxeles para crear una imagen de confrontación (x + r produce una nueva imagen), l es la clase de resultado deseada y el parámetro c se usa para equilibrar la distancia entre imágenes y la distancia entre predicciones. El primer término es la distancia entre el resultado predicho del ejemplo de confrontación y la clase deseada l, el segundo término mide la distancia entre el ejemplo de confrontación y la imagen original. Esta formulación es casi idéntica a la función de pérdida para generar explicaciones contrafácticas. Existen restricciones adicionales para r, de modo que los valores de píxeles permanecen entre 0 y 1. Los autores sugieren resolver este problema de optimización con un L-BFGS con restricción de caja, un algoritmo de optimización que funciona con gradientes. Panda perturbado: método de signo de gradiente rápido Goodfellow et. al (2014)51 inventaron el método de signo de gradiente rápido para generar imágenes adversas. El método de signo de gradiente usa el gradiente del modelo subyacente para encontrar ejemplos adversos. La imagen original x se manipula agregando o restando un pequeño error \\(\\epsilon\\) a cada píxel. Si sumamos o restamos \\(\\epsilon\\) depende de si el signo del gradiente de un píxel es positivo o negativo. Agregar errores en la dirección del gradiente significa que la imagen se altera intencionalmente para que la clasificación del modelo falle. FIGURA 6.5: Goodfellow et. al (2014) hacen que un panda se vea como un gibón para una red neuronal. Al agregar pequeñas perturbaciones (imagen central) a los píxeles originales del panda (imagen izquierda), los autores crean un ejemplo de confrontación que se clasifica como un gibón (imagen de la derecha) pero parece un panda para los humanos. La siguiente fórmula describe el núcleo del método de signo de gradiente rápido: \\[x^\\prime=x+\\epsilon\\cdot{}sign(\\bigtriangledown_x{}J(\\theta,x,y))\\] donde \\(\\bigtriangledown_x{}J\\) es el gradiente de la función de pérdida de modelos con respecto al vector de píxeles de entrada original x, y es el verdadero vector de etiqueta para x y \\(\\theta\\) es el vector de parámetros del modelo. Del vector de gradiente (que es tan largo como el vector de los píxeles de entrada) solo necesitamos el signo: El signo del gradiente es positivo (+1) si un aumento en la intensidad de píxeles aumenta la pérdida (el error que comete el modelo) y negativo (-1) si una disminución en la intensidad de píxeles aumenta la pérdida. Esta vulnerabilidad se produce cuando una red neuronal trata una relación entre una intensidad de píxel de entrada y la puntuación de la clase de forma lineal. En particular, las arquitecturas de redes neuronales que favorecen la linealidad, como LSTM, redes maxout, redes con unidades de activación ReLU u otros algoritmos lineales de aprendizaje automático, como la regresión logística, son vulnerables al método de signo de gradiente. El ataque se lleva a cabo por extrapolación. La linealidad entre la intensidad del píxel de entrada y los puntajes de la clase conduce a la vulnerabilidad a los valores atípicos, es decir, el modelo puede ser engañado moviendo los valores de los píxeles a áreas fuera de la distribución de datos. Se esperaba que estos ejemplos adversos fueran bastante específicos para una arquitectura de red neuronal dada. Pero resulta que puedes reutilizar ejemplos adversos para engañar a las redes con una arquitectura diferente entrenada en la misma tarea. Goodfellow et. al (2014) sugirieron agregar ejemplos adversos a los datos de entrenamiento para aprender modelos robustos. Una medusa  No, espera. Una bañera: ataques de 1 píxel El enfoque presentado por Goodfellow y colegas (2014) requiere que se cambien muchos píxeles, aunque solo sea un poco. Pero, ¿y si solo puedes cambiar un solo píxel? ¿Serías capaz de engañar a un modelo de aprendizaje automático? Su et al. (2019)52 demostraron que en realidad es posible engañar a los clasificadores de imágenes cambiando un solo píxel. FIGURA 6.6: Al cambiar intencionalmente un solo píxel (marcado con círculos), una red neuronal entrenada en ImageNet es engañada para predecir la clase incorrecta en lugar de la clase original. Trabajo de Su et. al (2019). De manera similar a los contrafácticos, el ataque de 1 píxel busca un ejemplo modificado de x que se acerque a la imagen original x, pero cambie la predicción a un resultado adverso. Sin embargo, la definición de cercanía difiere: solo un píxel puede cambiar. El ataque de 1 píxel utiliza la evolución diferencial para descubrir qué píxel se va a cambiar y cómo. La evolución diferencial se inspira libremente en la evolución biológica de las especies. Una población de individuos llamados soluciones candidatas se recombina generación por generación hasta encontrar una solución. Cada solución candidata codifica una modificación de píxeles y está representada por un vector de cinco elementos: las coordenadas x e y y los valores rojo, verde y azul (RGB). La búsqueda comienza con, por ejemplo, 400 soluciones candidatas (= sugerencias de modificación de píxeles) y crea una nueva generación de soluciones candidatas (secundarias) a partir de la generación principal utilizando la siguiente fórmula: \\[x_{i}(g+1)=x_{r1}(g)+F\\cdot(x_{r2}(g)+x_{r3}(g))\\] donde cada \\(x_i\\) es un elemento de una solución candidata (ya sea coordenada x, coordenada y, rojo, verde o azul), g es la generación actual, F es un parámetro de escala (establecido en 0.5) y r1, r2 y r3 son números aleatorios diferentes. Cada nueva solución candidata secundaria es a su vez un píxel con los cinco atributos de ubicación y color y cada uno de esos atributos es una mezcla de tres píxeles primarios aleatorios. La creación de elementos secundarios se detiene si una de las soluciones candidatas es un ejemplo contradictorio, lo que significa que se clasifica como una clase incorrecta o si se alcanza el número de iteraciones máximas especificadas por el usuario. Todo es una tostadora: parche adversarial Uno de mis métodos favoritos trae ejemplos adversos a la realidad física. Brown et. al (2017)53 diseñaron una etiqueta imprimible que se puede pegar junto a los objetos para que parezcan tostadoras para un clasificador de imágenes. Trabajo brillante! FIGURA 6.7: Una calcomanía que hace que un clasificador VGG16 entrenado en ImageNet clasifique una imagen de un plátano como tostadora. Trabajo de Brown et. al (2017). Este método difiere de los métodos presentados hasta ahora para los ejemplos de confrontación, ya que se elimina la restricción de que la imagen de confrontación debe estar muy cerca de la imagen original. En cambio, el método reemplaza completamente una parte de la imagen con un parche que puede tomar cualquier forma. La imagen del parche se optimiza sobre diferentes imágenes de fondo, con diferentes posiciones del parche en las imágenes, a veces movidas, a veces más grandes o más pequeñas y giradas, para que el parche funcione en muchas situaciones. Al final, esta imagen optimizada puede imprimirse y usarse para engañar a los clasificadores de imágenes en la naturaleza. Nunca traigas una tortuga impresa en 3D a un tiroteo, incluso si tu computadora cree que es una buena idea: ejemplos adversos robustos El siguiente método es literalmente agregar otra dimensión a la tostadora: Athalye et. al (2017)54 imprimieron en 3D una tortuga que fue diseñada para parecerse a un rifle en una red neuronal profunda desde casi todos los ángulos posibles. Sí, leíste eso bien. ¡Un objeto físico que parece una tortuga para los humanos parece un rifle para la computadora! FIGURA 6.8: Una tortuga impresa en 3D que es reconocida como un rifle por el clasificador InceptionV3 pre-entrenado estándar de TensorFlow. Trabajo de Athalye et. al (2017) Los autores han encontrado una manera de crear un ejemplo de confrontación en 3D para un clasificador 2D que es adversario sobre las transformaciones, como todas las posibilidades de rotar a la tortuga, acercar, etc. Otros enfoques, como el método de gradiente rápido, ya no funcionan cuando se gira la imagen o cambian los ángulos de visión. Athalye et. al (2017) proponen el algoritmo Expectation Over Transformation (EOT), que es un método para generar ejemplos adversos que incluso funcionan cuando la imagen se transforma. La idea principal detrás de EOT es optimizar ejemplos adversos en muchas transformaciones posibles. En lugar de minimizar la distancia entre el ejemplo de confrontación y la imagen original, EOT mantiene la distancia esperada entre los dos por debajo de un cierto umbral, dada una distribución seleccionada de posibles transformaciones. La distancia esperada bajo transformación se puede escribir como: \\[\\mathbb{E}_{t\\sim{}T}[d(t(x^\\prime),t(x))]\\] donde x es la imagen original, t(x) la imagen transformada (por ejemplo, rotada), x el ejemplo de confrontación y t(x) su versión transformada. Además de trabajar con una distribución de transformaciones, el método EOT sigue el patrón familiar de enmarcar la búsqueda de ejemplos adversos como un problema de optimización. Intentamos encontrar un ejemplo de confrontación x que maximice la probabilidad de la clase seleccionada \\(y_t\\) (por ejemplo, rifle) en la distribución de posibles transformaciones T: \\[\\arg\\max_{x^\\prime}\\mathbb{E}_{t\\sim{}T}[log{}P(y_t|t(x^\\prime))]\\] Con la restricción de que la distancia esperada sobre todas las posibles transformaciones entre el ejemplo de confrontación x y la imagen original x permanece por debajo de cierto umbral: \\[\\mathbb{E}_{t\\sim{}T}[d(t(x^\\prime),t(x))]&lt;\\epsilon\\quad\\text{and}\\quad{}x\\in[0,1]^d\\] Creo que deberíamos preocuparnos por las posibilidades que ofrece este método. Los otros métodos se basan en la manipulación de imágenes digitales. Sin embargo, estos ejemplos adversos robustos impresos en 3D se pueden insertar en cualquier escena real y engañar a una computadora para clasificar erróneamente un objeto. Vamos a darle la vuelta: ¿Qué pasa si alguien crea un rifle que se parece a una tortuga? El adversario con los ojos vendados: ataque de caja negra Imagina el siguiente escenario: Te doy acceso a mi gran clasificador de imágenes a través de la API web. Puedes obtener predicciones del modelo, pero no tienes acceso a los parámetros del modelo. Desde la comodidad de tu sofá, puedes enviar datos y ver mis respuestas de servicio con las clasificaciones correspondientes. La mayoría de los ataques adversos no están diseñados para funcionar en este escenario porque requieren acceso al gradiente de la red neuronal profunda subyacente para encontrar ejemplos adversos. Papernot et al. (2017)55 demostraron que es posible crear ejemplos adversos sin información interna del modelo y sin acceso a los datos de entrenamiento. Este tipo de ataque (casi) de conocimiento cero se llama ataque de caja negra. Cómo funciona: Comienza con algunas imágenes que provienen del mismo dominio que los datos de entrenamiento, por ejemplo, si el clasificador a atacar es un clasificador de dígitos, use imágenes de dígitos. Se requiere el conocimiento del dominio, pero no el acceso a los datos de entrenamiento. Obtén predicciones para el conjunto actual de imágenes de caja negra. Entrena a un modelo sustituto en el conjunto actual de imágenes (por ejemplo, una red neuronal). Crea un nuevo conjunto de imágenes sintéticas utilizando una heurística que examine el conjunto actual de imágenes en qué dirección manipular los píxeles para que la salida del modelo tenga más variación. Repite los pasos 2 a 4 para un número predefinido de épocas. Crea ejemplos adversos para el modelo sustituto utilizando el método de gradiente rápido (o similar). Ataca el modelo original con ejemplos adversos. El objetivo del modelo sustituto es aproximar los límites de decisión del modelo de caja negra, pero no necesariamente para lograr la misma precisión. Los autores probaron este enfoque atacando clasificadores de imágenes entrenados en varios servicios de aprendizaje automático en la nube. Estos servicios entrenan a los clasificadores de imágenes en imágenes y etiquetas cargadas por el usuario. El software entrena el modelo automáticamente, a veces con un algoritmo desconocido para el usuario, y lo implementa. El clasificador da predicciones para las imágenes cargadas, pero el modelo en sí no puede ser inspeccionado o descargado. Los autores pudieron encontrar ejemplos adversos para varios proveedores, con hasta el 84% de los ejemplos adversos clasificados erróneamente. El método incluso funciona si el modelo de caja negra a ser engañado no es una red neuronal. Esto incluye modelos de aprendizaje automático sin gradientes, como árboles de decisión. 6.2.2 La perspectiva de ciberseguridad El aprendizaje automático trata con incógnitas conocidas: prediciendo puntos de datos desconocidos a partir de una distribución conocida. La defensa contra ataques se ocupa de incógnitas desconocidas: prediciendo de manera sólida puntos de datos desconocidos a partir de una distribución desconocida de entradas adversas. A medida que el aprendizaje automático se integra en más y más sistemas, como vehículos autónomos o dispositivos médicos, también se están convirtiendo en puntos de entrada para ataques. Incluso si las predicciones de un modelo de aprendizaje automático en un conjunto de datos de prueba son 100% correctas, se pueden encontrar ejemplos adversos para engañar al modelo. La defensa de los modelos de aprendizaje automático contra los ciberataques es una nueva parte del campo de la ciberseguridad. Biggio et. al (2018)56 ofrecen una buena revisión de diez años de investigación sobre aprendizaje automático adversarial, en el que se basa esta sección. La ciberseguridad es una carrera armamentista en la que los atacantes y defensores se burlan mutuamente una y otra vez. Hay tres reglas de oro en ciberseguridad: 1) conoce a tu adversario 2) sé proactivo y 3) protégete. Las diferentes aplicaciones tienen diferentes adversos. Las personas que intentan defraudar a otras personas por correo electrónico por su dinero son agentes adversos de los usuarios y proveedores de servicios de correo electrónico. Los proveedores quieren proteger a sus usuarios para que puedan seguir usando su programa de correo, los atacantes quieren que las personas les den dinero. Conocer a tus adversos significa conocer sus objetivos. Suponiendo que no sabes que existen estos spammers y que el único abuso del servicio de correo electrónico es enviar copias pirateadas de música, la defensa sería diferente (por ejemplo, escaneando los archivos adjuntos en busca de material protegido por derechos de autor en lugar de analizar el texto en busca de indicadores de spam). Ser proactivo significa probar e identificar activamente los puntos débiles del sistema. Eres proactivo cuando tratas de engañar activamente al modelo con ejemplos adversos y luego te defiendes de ellos. El uso de métodos de interpretación para comprender qué características son importantes y cómo las características afectan la predicción también es un paso proactivo para comprender las debilidades de un modelo de aprendizaje automático. Como científico de datos, ¿confías en tu modelo en este mundo peligroso sin haber visto más allá del poder predictivo en un conjunto de datos de prueba? ¿Has analizado cómo se comporta el modelo en diferentes escenarios, identificado las entradas más importantes, verificado las explicaciones de predicción para algunos ejemplos? ¿Has intentado encontrar entradas adversas? La interpretabilidad de los modelos de aprendizaje automático juega un papel importante en la ciberseguridad. Ser reactivo, lo opuesto a proactivo, significa esperar hasta que el sistema haya sido atacado y solo entonces comprender el problema e instalar algunas medidas defensivas. ¿Cómo podemos proteger nuestros sistemas de aprendizaje automático contra ejemplos adversos? Un enfoque proactivo es el reentrenamiento iterativo del clasificador con ejemplos adversos, también llamado entrenamiento adversario. Otros enfoques se basan en la teoría de juegos, como aprender transformaciones invariables de las características u optimización robusta (regularización). Otro método propuesto es usar múltiples clasificadores en lugar de uno solo y hacer que voten la predicción (conjunto), pero eso no tiene garantía de funcionar, ya que todos podrían sufrir ejemplos adversos similares. Otro enfoque que tampoco funciona bien es el enmascaramiento de gradiente, que construye un modelo sin gradientes útiles mediante el uso de un clasificador vecino más cercano en lugar del modelo original. Podemos distinguir los tipos de ataques según cuánto sepa un atacante sobre el sistema. Los atacantes pueden tener un conocimiento perfecto (ataque de caja blanca), lo que significa que saben todo sobre el modelo, como el tipo de modelo, los parámetros y los datos de entrenamiento; los atacantes pueden tener un conocimiento parcial (ataque de caja gris), lo que significa que solo pueden conocer la representación de características y el tipo de modelo que se utilizó, pero no tienen acceso a los datos de entrenamiento o los parámetros; los atacantes pueden tener cero conocimiento (ataque de caja negra), lo que significa que solo pueden consultar el modelo caja negra pero no tienen acceso a los datos de entrenamiento o información sobre los parámetros del modelo. Dependiendo del nivel de información, los atacantes pueden usar diferentes técnicas para atacar el modelo. Como hemos visto en los ejemplos, incluso en el caso de la caja negra se pueden crear ejemplos adversos, de modo que ocultar información sobre los datos y el modelo no es suficiente para protegerse contra los ataques. Dada la naturaleza del juego del gato y el ratón entre atacantes y defensores, veremos mucho desarrollo e innovación en esta área. Solo piensa en los diferentes tipos de correos electrónicos no deseados que evolucionan constantemente. Se inventan nuevos métodos de ataques contra modelos de aprendizaje automático y se proponen nuevas medidas defensivas contra estos nuevos ataques. Se desarrollan ataques más potentes para evadir las últimas defensas y así sucesivamente hasta el infinito. Con este capítulo espero sensibilizar sobre el problema de los ejemplos adversos y que solo estudiando proactivamente los modelos de aprendizaje automático podemos descubrir y remediar las debilidades. Szegedy, Christian, et al. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013). Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014). Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural networks. IEEE Transactions on Evolutionary Computation (2019). Brown, Tom B., et al. Adversarial patch. arXiv preprint arXiv:1712.09665 (2017). Athalye, Anish, and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397 (2017). Papernot, Nicolas, et al. Practical black-box attacks against machine learning. Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017). Biggio, Battista, and Fabio Roli. Wild Patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition 84 (2018): 317-331. "],["proto.html", "6.3 Prototipos y excepciones", " 6.3 Prototipos y excepciones Un prototipo es una instancia de datos que es representativa de todos los datos. Una excepción es una instancia de datos que no está bien representada por el conjunto de prototipos. El propósito de una excepción es proporcionar ideas junto con los prototipos, especialmente para los puntos de datos que los prototipos no representan bien. Los prototipos y excepciones se pueden usar independientemente de un modelo de aprendizaje automático para describir los datos, pero también se pueden usar para crear un modelo interpretable o para hacer que un modelo de caja negra sea interpretable. En este capítulo utilizo la expresión punto de datos para referirme a una sola instancia, para enfatizar la interpretación de que una instancia también es un punto en un sistema de coordenadas donde cada entidad es una dimensión. La siguiente figura muestra una distribución de datos simulada, con algunas de las instancias elegidas como prototipos y otras como excepciones. Los puntos pequeños son los datos, los puntos grandes los prototipos y los cuadrados grandes las excepciones. Los prototipos se seleccionan (manualmente) para cubrir los centros de distribución de datos y las excepciones son puntos en un grupo sin un prototipo. Los prototipos y excepciones son siempre ejemplos reales de los datos. FIGURA 6.9: Prototipos y excepciones para una distribución de datos con dos características x1 y x2. Seleccioné los prototipos manualmente, lo que no escala bien y probablemente conduce a malos resultados. Hay muchos enfoques para encontrar prototipos en los datos. Uno de estos es k-medoids, un algoritmo de agrupamiento relacionado con el algoritmo k-means. Cualquier algoritmo de agrupación que devuelva puntos de datos reales como centros de agrupación calificaría para seleccionar prototipos. Pero la mayoría de estos métodos solo encuentran prototipos, no excepciones. Este capítulo presenta MMD-critic por Kim et. al (2016)57, un enfoque que combina prototipos y excepciones en un solo marco. MMD-critic compara la distribución de los datos y la distribución de los prototipos seleccionados. Este es el concepto central para entender el método crítico de MMD-critic. MMD-critic selecciona prototipos que minimizan la discrepancia entre las dos distribuciones. Los puntos de datos en áreas con alta densidad son buenos prototipos, especialmente cuando los puntos se seleccionan de diferentes grupos de datos. Los puntos de datos de regiones que no están bien explicados por los prototipos se seleccionan como excepciones. Profundicemos más en la teoría. 6.3.1 Teoría El procedimiento de MMD-critic a alto nivel se puede resumir brevemente: Selecciona la cantidad de prototipos y excepciones que deseas encontrar. Encuentra prototipos con greedy search. Los prototipos se seleccionan de modo que la distribución de los prototipos esté cerca de la distribución de datos. Encuentra excepciones con greedy search. Los puntos se seleccionan como excepciones donde la distribución de prototipos difiere de la distribución de los datos. Necesitamos un par de ingredientes para encontrar prototipos y excepciones para un conjunto de datos con MMD-critical. Como ingrediente más básico, necesitamos una función del núcleo para estimar las densidades de datos. Un núcleo es una función que pesa dos puntos de datos según su proximidad. En base a las estimaciones de densidad, necesitamos una medida que nos diga cuán diferentes son las dos distribuciones para que podamos determinar si la distribución de los prototipos que seleccionamos está cerca de la distribución de datos. Esto se resuelve midiendo la discrepancia media máxima (MMD). También basado en la función del núcleo, necesitamos la función testigo para decirnos cuán diferentes son dos distribuciones en un punto de datos particular. Con la función testigo, podemos seleccionar excepciones, es decir, puntos de datos en los que la distribución de prototipos y datos diverge y la función testigo adquiere grandes valores absolutos. El último ingrediente es una estrategia de búsqueda de buenos prototipos y excepciones, que se resuelve con una simple greedy search. Comencemos con la discrepancia media máxima (MMD), que mide la discrepancia entre dos distribuciones. La selección de prototipos crea una distribución de densidad de prototipos. Queremos evaluar si la distribución de prototipos difiere de la distribución de datos. Estimamos ambos con funciones de densidad del núcleo. La discrepancia media máxima mide la diferencia entre dos distribuciones, que es el supremum sobre un espacio funcional de diferencias entre las expectativas según las dos distribuciones. ¿Todo claro? Personalmente, entiendo estos conceptos mucho mejor cuando veo cómo se calcula algo con los datos. La siguiente fórmula muestra cómo calcular la medida de MMD al cuadrado (MMD2): \\[MMD^2=\\frac{1}{m^2}\\sum_{i,j=1}^m{}k(z_i,z_j)-\\frac{2}{mn}\\sum_{i,j=1}^{m,n}k(z_i,x_j)+\\frac{1}{n^2}\\sum_{i,j=1}^n{}k(x_i,x_j)\\] k es una función del núcleo que mide la similitud de dos puntos, pero más sobre esto más adelante. m es el número de prototipos z, y n es el número de puntos de datos x en nuestro conjunto de datos original. Los prototipos z son una selección de puntos de datos x. Cada punto es multidimensional, es decir, puede tener múltiples características. El objetivo de MMD-critic es minimizar MMD2. Cuanto más cerca esté MMD2 de cero, mejor se ajustará la distribución de los prototipos a los datos. La clave para llevar MMD2 a cero es el término en el medio, que calcula la proximidad promedio entre los prototipos y todos los demás puntos de datos (multiplicado por 2). Si este término se suma al primer término (la proximidad promedio de los prototipos entre sí) más el último término (la proximidad promedio de los puntos de datos entre sí), entonces los prototipos explican los datos perfectamente. Prueba lo que sucedería con la fórmula si usaras todos los n puntos de datos como prototipos. El siguiente gráfico ilustra la medida MMD2. El primer gráfico muestra los puntos de datos con dos características, por lo que la estimación de la densidad de datos se muestra con un fondo sombreado. Cada una de las otras gráficas muestra diferentes selecciones de prototipos, junto con la medida MMD2 en los títulos de las gráficas. Los prototipos son los puntos grandes y su distribución se muestra como líneas de contorno. La selección de los prototipos que mejor cubren los datos en estos escenarios (abajo a la izquierda) tiene el valor de discrepancia más bajo. FIGURA 6.10: La medida de discrepancia media máxima al cuadrado (MMD2) para un conjunto de datos con dos características y diferentes selecciones de prototipos. Una opción para el núcleo es el núcleo de la función de base radial: \\[k(x,x^\\prime)=exp\\left(-\\gamma||x-x^\\prime||^2\\right)\\] donde ||x-x||2 es la distancia euclidiana entre dos puntos y \\(\\gamma\\) es un parámetro de escala. El valor del núcleo disminuye con la distancia entre los dos puntos y varía entre cero y uno: Cero cuando los dos puntos están infinitamente separados; uno cuando los dos puntos son iguales. Combinamos la medida MMD2, el núcleo y la búsqueda para encontrar prototipos: Comienza con una lista vacía de prototipos. Mientras el número de prototipos esté por debajo del número elegido m: Para cada punto en el conjunto de datos, verifica cuánto MMD2 se reduce cuando el punto se agrega a la lista de prototipos. Agrega el punto de datos que minimiza el MMD2 a la lista. Devuelve la lista de prototipos. El ingrediente restante para encontrar excepciones es la función de testigo, que nos dice cuánto difieren las dos estimaciones de densidad en un punto particular. Se puede estimar usando: \\[ testigo (x) = \\ frac {1} {n} \\ sum_ {i = 1} ^ nk (x, x_i) - \\ frac {1} {m} \\ sum_ {j = 1} ^ mk (x, z_j) \\] Para dos conjuntos de datos (con las mismas características), la función testigo le proporciona los medios para evaluar en qué distribución empírica el punto x se ajusta mejor. Para encontrar excepciones, buscamos valores extremos de la función testigo en direcciones negativas y positivas. El primer término en la función de testigo es la proximidad promedio entre el punto x y los datos, y, respectivamente, el segundo término es la proximidad promedio entre el punto x y los prototipos. Si la función testigo para un punto x es cercana a cero, la función de densidad de los datos y los prototipos están muy juntos, lo que significa que la distribución de prototipos se parece a la distribución de los datos en el punto x. Una función testigo negativa en el punto x significa que la distribución del prototipo sobreestima la distribución de datos (por ejemplo, si seleccionamos un prototipo pero solo hay pocos puntos de datos cercanos); una función testigo positiva en el punto x significa que la distribución del prototipo subestima la distribución de datos (por ejemplo, si hay muchos puntos de datos alrededor de x pero no hemos seleccionado ningún prototipo cerca). Para darte más intuición, reutilicemos los prototipos de la trama de antemano con el MMD2 más bajo y visualicemos la función testigo para algunos puntos seleccionados manualmente. Las etiquetas en la siguiente gráfica muestran el valor de la función testigo para varios puntos marcados como triángulos. Solo el punto en el medio tiene un alto valor absoluto y, por lo tanto, es un buen candidato para una excepción. FIGURA 6.11: Evaluaciones de la función testigo en diferentes puntos. La función testigo nos permite buscar explícitamente instancias de datos que no están bien representadas por los prototipos. Las excepciones son puntos con alto valor absoluto en la función testigo. Al igual que los prototipos, las excepciones también se encuentran a través de greedy search. Pero en lugar de reducir el MMD2 general, estamos buscando puntos que maximicen una función de costo que incluya la función de testigo y un término de regularizador. El término adicional en la función de optimización impone la diversidad en los puntos, lo cual es necesario para que los puntos provengan de diferentes grupos. Este segundo paso es independiente de cómo se encuentran los prototipos. También podría haber seleccionado algunos prototipos y utilizar el procedimiento descrito aquí para conocer las excepciones. O los prototipos podrían provenir de cualquier procedimiento de agrupación, como k-medoides. Eso es con las partes importantes de la teoría de MMD-critic. Queda una pregunta: ¿Cómo se puede utilizar el MMD-critic para el aprendizaje automático interpretable? MMD-critic puede agregar interpretabilidad de tres maneras: Al ayudar a comprender mejor la distribución de datos; construyendo un modelo interpretable; haciendo que un modelo de caja negra sea interpretable. Si aplicas MMD-critic a tus datos para encontrar prototipos y excepciones, mejorará tu comprensión de los datos, especialmente si tienes una distribución de datos compleja con casos extremos. ¡Pero con MMD-critic puedes lograr más! Por ejemplo, puedes crear un modelo de predicción interpretable: el llamado modelo prototipo más cercano. La función de predicción se define como: \\[\\hat{f}(x)=argmax_{i\\in{}S}k(x,x_i)\\] lo que significa que seleccionamos el prototipo i del conjunto de prototipos S que está más cerca del nuevo punto de datos, en el sentido de que produce el valor más alto de la función del núcleo. El prototipo en sí mismo se devuelve como una explicación para la predicción. Este procedimiento tiene tres parámetros de ajuste: El tipo de núcleo, el parámetro de escala del núcleo y el número de prototipos. Todos los parámetros se pueden optimizar dentro de un bucle de validación cruzada. Las excepciones no se utilizan en este enfoque. Como tercera opción, podemos usar MMD-critic para hacer que cualquier modelo de aprendizaje automático sea globalmente explicable mediante el examen de prototipos y excepciones junto con sus predicciones de modelo. El procedimiento es el siguiente: Encuentra prototipos y excepciones con MMD-critic. Entrena un modelo de aprendizaje automático como de costumbre. Predice resultados para los prototipos y excepciones con el modelo de aprendizaje automático. Analiza las predicciones: ¿En qué casos estuvo mal el algoritmo? Ahora tienes una serie de ejemplos que representan bien los datos y te ayudan a encontrar las debilidades del modelo de aprendizaje automático. ¿Cómo ayuda eso? ¿Recuerdas cuando el clasificador de imágenes de Google identificaba a los negros como gorilas? Quizás deberían haber utilizado el procedimiento descrito aquí antes de implementar su modelo de reconocimiento de imagen. No es suficiente solo verificar el rendimiento del modelo, porque si fuera 99% correcto, este problema aún podría estar en el 1%. ¡Y las etiquetas también pueden estar equivocadas! Revisar todos los datos de entrenamiento y realizar una verificación de cordura si la predicción es problemática podría haber revelado el problema, pero sería inviable. Pero la selección de, digamos unos miles, prototipos y excepciones es factible y podría haber revelado un problema con los datos: Podría haber demostrado que faltan imágenes de personas con piel oscura, lo que indica un problema con la diversidad en el conjunto de datos. O podría haber mostrado una o más imágenes de una persona con piel oscura como prototipo o (probablemente) como una crítica con la notoria clasificación de gorila. No prometo que MMD-critic ciertamente interceptará este tipo de errores, pero es un buen control de cordura. 6.3.2 Ejemplos He tomado los ejemplos del artículo de MMD-critic. Ambas aplicaciones se basan en conjuntos de datos de imágenes. Cada imagen estaba representada por incrustaciones de imágenes con 2048 dimensiones. Una incrustación de imágenes es un vector con números que capturan atributos abstractos de una imagen. Los vectores de incrustación generalmente se extraen de redes neuronales que están entrenadas para resolver una tarea de reconocimiento de imágenes, en este caso el desafío de ImageNet. Las distancias del núcleo entre las imágenes se calcularon utilizando estos vectores de inclusión. El primer conjunto de datos contiene diferentes razas de perros del conjunto de datos ImageNet. MMD-critic se aplica a los datos de dos clases de razas de perros. Con los perros a la izquierda, los prototipos suelen mostrar la cara del perro, mientras que las excepciones son imágenes sin las caras del perro o en diferentes colores (como blanco y negro). En el lado derecho, los prototipos contienen imágenes de perros al aire libre. Las excepciones contienen perros con disfraces y otros casos inusuales. FIGURA 6.12: Prototipos y excepciones para dos tipos de razas de perros del conjunto de datos de ImageNet. Otra ilustración de MMD-critic utiliza un conjunto de datos de dígitos escritos a mano. Al observar los prototipos y las excepciones reales, puedes notar que la cantidad de imágenes por dígito es diferente. Esto se debe a que se buscó un número fijo de prototipos y excepciones en todo el conjunto de datos y no con un número fijo por clase. Como se esperaba, los prototipos muestran diferentes formas de escribir los dígitos. Las excepciones incluyen ejemplos con líneas inusualmente gruesas o delgadas, pero también dígitos irreconocibles. FIGURA 6.13: Prototipos y excepciones para un conjunto de datos de dígitos escritos a mano. 6.3.3 Ventajas En un estudio de usuarios, los autores de MMD-critical dieron imágenes a los participantes, que tuvieron que hacer coincidir visualmente con uno de los dos conjuntos de imágenes, cada una representando una de dos clases (por ejemplo, dos razas de perros). Los participantes se desempeñaron mejor cuando los sets mostraron prototipos y excepciones en lugar de imágenes aleatorias de una clase. Eres libre de elegir el número de prototipos y excepciones. MMD-critic trabaja con estimaciones de densidad de los datos. Esto funciona con cualquier tipo de datos y cualquier tipo de modelo de aprendizaje automático. El algoritmo es fácil de implementar. MMD-critic es muy flexible en la forma en que se utiliza para aumentar la capacidad de interpretación. Se puede utilizar para comprender distribuciones de datos complejas. Se puede usar para construir un modelo de aprendizaje automático interpretable. O puede arrojar luz sobre la toma de decisiones de un modelo de aprendizaje automático de caja negra. Encontrar excepciones es independiente del proceso de selección de los prototipos. Pero tiene sentido seleccionar prototipos de acuerdo con MMD-critic, ya que tanto los prototipos como las excepciones se crean utilizando el mismo método de comparación de prototipos y densidades de datos. 6.3.4 Desventajas Si bien, matemáticamente, los prototipos y as excepciones se definen de manera diferente, su distinción se basa en un valor de corte (el número de prototipos). Supón que eliges un número demasiado bajo de prototipos para cubrir la distribución de datos. Las excepciones terminarían en áreas que no están tan bien explicadas. Pero si tuvieras que agregar más prototipos, también terminarían en las mismas áreas. Cualquier interpretación debe tener en cuenta que las excepciones dependen en gran medida de los prototipos existentes y del valor de corte (arbitrario) para el número de prototipos. Debes elegir el número de prototipos y excepciones. Por mucho que esto sea agradable de tener, también es una desventaja. ¿Cuántos prototipos y excepciones necesitamos realmente? ¿Mientras más, mejor? ¿Cuanto menos mejor? Una solución es seleccionar el número de prototipos y excepciones midiendo cuánto tiempo tienen los humanos para la tarea de mirar las imágenes, que depende de la aplicación en particular. Solo cuando usamos MMD-critic para construir un clasificador tenemos una manera de optimizarlo directamente. Una solución podría ser un diagrama de pantalla que muestre el número de prototipos en el eje xy la medida MMD2 en el eje y. Elegiríamos el número de prototipos donde la curva MMD2 se aplana. Los otros parámetros son la elección del núcleo y el parámetro de escala del núcleo. Tenemos el mismo problema que con la cantidad de prototipos y excepciones: ¿Cómo seleccionamos un núcleo y su parámetro de escala? Nuevamente, cuando usamos MMD-critic como un clasificador prototipo más cercano, podemos ajustar los parámetros del kernel. Sin embargo, para los casos de uso no supervisado de MMD-critic, no está claro. (Tal vez soy un poco duro aquí, ya que todos los métodos sin supervisión tienen este problema). Toma todas las características como entrada, sin tener en cuenta el hecho de que algunas características pueden no ser relevantes para predecir el resultado de interés. Una solución es utilizar solo funciones relevantes, por ejemplo, incrustaciones de imágenes en lugar de píxeles sin formato. Esto funciona siempre que tengamos una manera de proyectar la instancia original en una representación que contenga solo información relevante. Hay algo de código disponible, pero aún no está implementado como software bien empaquetado y documentado. 6.3.5 Código y alternativas Puedes encontrar una implementación de MMD-critic aquí: https://github.com/BeenKim/MMD-critic. La alternativa más simple para encontrar prototipos es k-medoids de Kaufman et. al (1987).[^kmedoids] Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. Examples are not enough, learn to criticize! Criticism for interpretability. Advances in Neural Information Processing Systems (2016). "],["influyente.html", "6.4 Instancias influyentes", " 6.4 Instancias influyentes Los modelos de aprendizaje automático son, en última instancia, un producto de datos de entrenamiento y la eliminación de una de las instancias de entrenamiento puede afectar el modelo resultante. Llamamos a una instancia de entrenamiento influyente cuando su eliminación de los datos de entrenamiento cambia considerablemente los parámetros o predicciones del modelo. Al identificar instancias de entrenamiento influyentes, podemos depurar los modelos de aprendizaje automático y explicar mejor sus comportamientos y predicciones. Este capítulo te muestra dos enfoques para identificar instancias influyentes, a saber, diagnósticos de eliminación y funciones de influencia. Ambos enfoques se basan en estadísticas sólidas, que proporcionan métodos estadísticos que se ven menos afectados por valores atípicos o violaciones de los supuestos del modelo. Las estadísticas robustas también proporcionan métodos para medir qué tan robustas son las estimaciones de los datos (como una estimación media o los pesos de un modelo de predicción). Imagina que deseas estimar el ingreso promedio de las personas en tu ciudad y preguntar a diez personas aleatorias en la calle cuánto ganan. Además del hecho de que tu muestra probablemente sea realmente mala, ¿cuánto puede influir la estimación de ingresos promedio por una sola persona? Para responder a esta pregunta, puedes volver a calcular el valor medio omitiendo respuestas individuales o derivar matemáticamente a través de funciones de influencia cómo se puede influir en el valor medio. Con el enfoque de eliminación, recalculamos el valor medio diez veces, omitiendo uno de los estados de resultados cada vez y medimos cuánto cambia la estimación media. Un gran cambio significa que una instancia fue muy influyente. El segundo enfoque pondera a una de las personas en un peso infinitesimalmente pequeño, que corresponde al cálculo de la primera derivada de una estadística o modelo. Este enfoque también se conoce como enfoque infinitesimal o función de influencia. La respuesta es, por cierto, que la estimación media puede estar muy influenciada por una sola respuesta, ya que la media se escala linealmente con valores únicos. Una opción más sólida es la mediana (el valor en el que la mitad de las personas gana más y la otra mitad menos), porque incluso si la persona con el ingreso más alto en su muestra ganara diez veces más, la mediana resultante no cambiaría. Los diagnósticos de eliminación y las funciones de influencia también se pueden aplicar a los parámetros o predicciones de los modelos de aprendizaje automático para comprender mejor su comportamiento o explicar predicciones individuales. Antes de analizar estos dos enfoques para encontrar instancias influyentes, examinaremos la diferencia entre una instancia atípica y una instancia influyente. Valores atípicos Un valor atípico es una instancia que está muy lejos de las otras instancias del conjunto de datos. Lejos significa que la distancia, por ejemplo euclidiana, es muy grande. En un conjunto de datos de recién nacidos, un recién nacido que pesa 6 kg se consideraría un valor atípico. En un conjunto de datos de cuentas bancarias con cuentas corrientes en su mayoría, una cuenta de préstamo dedicada (saldo negativo grande, pocas transacciones) se consideraría un valor atípico. La siguiente figura muestra un valor atípico para una distribución unidimensional. FIGURA 6.14: La variable x sigue una distribución gaussiana con un valor atípico en x = 8. Los valores atípicos pueden ser puntos de datos interesantes (por ejemplo, excepciones). Cuando un valor atípico influye en el modelo, también es una instancia influyente. Instancias influyentes Una instancia influyente es una instancia de datos cuya eliminación tiene un fuerte efecto en el modelo entrenado. Cuanto más cambien los parámetros o predicciones del modelo cuando el modelo se vuelva a entrenar con una instancia particular eliminada de los datos de entrenamiento, más influyente será esa instancia. Si una instancia es influyente para un modelo entrenado también depende de su valor para el objetivo y. La siguiente figura muestra una instancia influyente para un modelo de regresión lineal. FIGURA 6.15: Un modelo lineal con una característica. Entrenado una vez en los datos completos y una vez sin la instancia influyente. Eliminar la instancia influyente cambia drásticamente la pendiente ajustada (peso / coeficiente). ¿Por qué las instancias influyentes ayudan a entender el modelo? La idea clave detrás de las instancias influyentes para la interpretabilidad es rastrear los parámetros y predicciones del modelo hasta donde comenzó todo: los datos de entrenamiento. Un alumno, es decir, el algoritmo que genera el modelo de aprendizaje automático, es una función que toma datos de entrenamiento que constan de las características X y el objetivo Y y genera un modelo de aprendizaje automático. Por ejemplo, el alumno de un árbol de decisión es un algoritmo que selecciona las características divididas y los valores en los que se divide. Un alumno para una red neuronal utiliza la propagación hacia atrás para encontrar los mejores pesos. FIGURA 6.16: Un alumno aprende un modelo a partir de datos de entrenamiento (características más objetivo). El modelo hace predicciones para nuevos datos. Preguntamos cómo cambiarían los parámetros del modelo o las predicciones si eliminamos instancias de entrenamiento en el proceso de entrenamiento. Esto contrasta con otros enfoques de interpretabilidad que analizan cómo cambia la predicción cuando manipulamos las características de las instancias a predecir, como gráficos de dependencia parcial o importancia de la característica. Con instancias influyentes, no tratamos el modelo como fijo, sino como una función de los datos de entrenamiento. Las instancias influyentes nos ayudan a responder preguntas sobre el comportamiento del modelo global y las predicciones individuales. ¿Cuáles fueron las instancias más influyentes para los parámetros del modelo o las predicciones en general? ¿Cuáles fueron las instancias más influyentes para una predicción particular? Las instancias influyentes nos dicen para qué instancias el modelo podría tener problemas, qué instancias de entrenamiento deberían verificarse en busca de errores y dar una impresión de la solidez del modelo. Es posible que no confiemos en un modelo si una sola instancia tiene una fuerte influencia en las predicciones y parámetros del modelo. Al menos eso nos haría investigar más a fondo. ¿Cómo podemos encontrar instancias influyentes? Tenemos dos formas de medir la influencia: Nuestra primera opción es eliminar la instancia de los datos de entrenamiento, volver a entrenar el modelo en el conjunto de datos de entrenamiento reducido y observar la diferencia en los parámetros o predicciones del modelo (individualmente o sobre el conjunto de datos completo). La segunda opción es aumentar el peso de una instancia de datos aproximando los cambios de parámetros en función de los gradientes de los parámetros del modelo. El enfoque de eliminación es más fácil de entender y motiva el enfoque de mejora, por lo que comenzamos con el primero. 6.4.1 Diagnóstico de eliminación Los estadísticos ya han investigado mucho en el área de instancias influyentes, especialmente para los modelos de regresión lineal (generalizados). Cuando busca observaciones influyentes, los primeros resultados de búsqueda se refieren a medidas como DFBETA y la distancia de Cook. DFBETA mide el efecto de eliminar una instancia en los parámetros del modelo. La distancia de Cook (Cook, 197758) mide el efecto de eliminar una instancia en las predicciones del modelo. Para ambas medidas tenemos que volver a entrenar el modelo repetidamente, omitiendo instancias individuales cada vez. Los parámetros o predicciones del modelo con todas las instancias se comparan con los parámetros o predicciones del modelo con una de las instancias eliminadas de los datos de entrenamiento. DFBETA se define como: \\[DFBETA_{i}=\\beta-\\beta^{(-i)}\\] donde \\(\\beta\\) es el vector de peso cuando el modelo se entrena en todas las instancias de datos, y \\(\\beta^{(-i)}\\) el vector de peso cuando el modelo se entrena sin instancia i. Muy intuitivo, dirías. DFBETA funciona solo para modelos con parámetros de peso, como regresión logística o redes neuronales, pero no para modelos como árboles de decisión, conjuntos de árboles, algunas máquinas de vectores de soporte, etc. La distancia de Cook se inventó para los modelos de regresión lineal y existen aproximaciones para los modelos de regresión lineal generalizados. La distancia de Cook para una instancia de entrenamiento se define como la suma (a escala) de las diferencias al cuadrado en el resultado predicho cuando la i-ésima instancia se elimina del modelo de entrenamiento. \\[D_i=\\frac{\\sum_{j=1}^n(\\hat{y}_j-\\hat{y}_{j}^{(-i)})^2}{p\\cdot{}MSE}\\] donde el numerador es la diferencia al cuadrado entre la predicción del modelo con y sin la i-ésima instancia, sumada sobre el conjunto de datos. El denominador es el número de características p veces el error cuadrático medio. El denominador es el mismo para todas las instancias, sin importar qué instancia se elimine. La distancia de Cook nos dice cuánto cambia la salida predicha de un modelo lineal cuando eliminamos la i-ésima instancia del entrenamiento. ¿Podemos usar la distancia de Cook y DFBETA para cualquier modelo de aprendizaje automático? DFBETA requiere parámetros de modelo, por lo que esta medida solo funciona para modelos parametrizados. La distancia de Cook no requiere ningún parámetro del modelo. Curiosamente, la distancia de Cook generalmente no se ve fuera del contexto de los modelos lineales y los modelos lineales generalizados, pero la idea de tomar la diferencia entre las predicciones del modelo antes y después de la eliminación de una instancia particular es muy general. Un problema con la definición de la distancia de Cook es el MSE, que no es significativo para todos los tipos de modelos de predicción (por ejemplo, clasificación). La medida de influencia más simple para el efecto en las predicciones del modelo se puede escribir de la siguiente manera: \\[\\text{Influence}^{(-i)}=\\frac{1}{n}\\sum_{j=1}^{n}\\left|\\hat{y}_j-\\hat{y}_{j}^{(-i)}\\right|\\] Esta expresión es básicamente el numerador de la distancia de Cook, con la diferencia de que la diferencia absoluta se suma en lugar de las diferencias al cuadrado. Esta fue una elección que hice, porque tiene sentido para los ejemplos posteriores. La forma general de las medidas de diagnóstico de eliminación consiste en elegir una medida (como el resultado pronosticado) y calcular la diferencia de la medida para el modelo entrenado en todas las instancias y cuando se elimina la instancia. Podemos desglosar fácilmente la influencia para explicar para la predicción de la instancia j cuál fue la influencia de la i-ésima instancia de entrenamiento: \\[\\text{Influence}_{j}^{(-i)}=\\left|\\hat{y}_j-\\hat{y}_{j}^{(-i)}\\right|\\] Esto también funcionaría para la diferencia en los parámetros del modelo o la diferencia en la pérdida. En el siguiente ejemplo usaremos estas simples medidas de influencia. Ejemplo de diagnóstico de eliminación En el siguiente ejemplo, entrenamos una SVM para predecir cáncer cervical dados los factores de riesgo y medir qué instancias de entrenamiento fueron más influyentes en general y para una predicción particular. Dado que la predicción del cáncer es un problema de clasificación, medimos la influencia como la diferencia en la probabilidad pronosticada de cáncer. Una instancia es influyente si la probabilidad pronosticada aumenta o disminuye en promedio en el conjunto de datos cuando la instancia se elimina del entrenamiento del modelo. La medición de la influencia para todas las instancias de entrenamiento 858 requiere entrenar el modelo una vez en todos los datos y volver a entrenarlo 858 veces (= tamaño de los datos de entrenamiento) con una de las instancias eliminadas hora. La instancia más influyente tiene una medida de influencia de aproximadamente 0.01. Una influencia de 0.01 significa que si eliminamos la instancia 540, la probabilidad pronosticada cambia en 1 punto porcentual en promedio. Esto es bastante considerable teniendo en cuenta que la probabilidad pronosticada promedio de cáncer es 6.4%. El valor medio de las medidas de influencia sobre todas las eliminaciones posibles es 0.2 puntos porcentuales. Ahora sabemos cuáles de las instancias de datos fueron más influyentes para el modelo. Esto ya es útil para depurar los datos. ¿Hay alguna instancia problemática? ¿Hay errores de medición? Las instancias influyentes son las primeras que deben verificarse en busca de errores, porque cada error en ellas influye fuertemente en las predicciones del modelo. Además de la depuración del modelo, ¿podemos aprender algo para comprender mejor el modelo? Simplemente imprimir las 10 instancias más influyentes no es muy útil, porque es solo una tabla de instancias con muchas características. Todos los métodos que devuelven instancias como salida solo tienen sentido si tenemos una buena manera de representarlos. Pero podemos entender mejor qué tipo de instancias influyen cuando preguntamos: ¿Qué distingue una instancia influyente de una instancia no influyente? Podemos convertir esta pregunta en un problema de regresión y modelar la influencia de una instancia en función de sus valores característicos. Somos libres de elegir cualquier modelo del capítulo sobre Modelos de aprendizaje automático interpretables. Para este ejemplo, elegí un árbol de decisión (figura siguiente) que muestra que los datos de mujeres de 35 años o más fueron los más influyentes para la máquina de vectores de soporte. De todas las mujeres en el conjunto de datos 153 de 858 tenían más de 35 años. En el capítulo sobre Gráficos de dependencia parcial, hemos visto que después de 40 hay un fuerte aumento en la probabilidad pronosticada de cáncer y la [Importancia de la característica] (#importanciadecaracteristicas) también ha detectado la edad como uno de las características más importantes. El análisis de influencia nos dice que el modelo se vuelve cada vez más inestable al predecir el cáncer para edades más altas. Esto en sí mismo es información valiosa. Esto significa que los errores en estos casos pueden tener un fuerte efecto en el modelo. FIGURA 6.17: Un árbol de decisión que modela la relación entre la influencia de las instancias y sus características. La profundidad máxima del árbol se establece en 2. Este primer análisis de influencia reveló la instancia general más influyente. Ahora seleccionamos una de las instancias, a saber, la instancia número 7, para la que queremos explicar la predicción al encontrar las instancias de datos de entrenamiento más influyentes. Es como una pregunta contrafactual: ¿Cómo cambiaría la predicción por ejemplo 7 si omitimos la instancia i del proceso de entrenamiento? Repetimos esta eliminación para todas las instancias. Luego seleccionamos las instancias de entrenamiento que resultan en el mayor cambio en la predicción de la instancia 7 cuando se omiten del entrenamiento y las usamos para explicar la predicción del modelo para esa instancia. Elegí explicar la predicción, por ejemplo, 7 porque es la instancia con la mayor probabilidad pronosticada de cáncer (7.35%), que pensé fue un caso interesante para analizar más profundamente. Podríamos devolver, por ejemplo, las 10 instancias más influyentes para predecir la instancia número 7 impresa como una tabla. No es muy útil, porque no pudimos ver mucho. Nuevamente, tiene más sentido descubrir qué distingue las instancias influyentes de las instancias no influyentes analizando sus características. Usamos un árbol de decisión entrenado para predecir la influencia dadas las características, pero en realidad lo usamos mal solo para encontrar una estructura y no para predecir algo. El siguiente árbol de decisión muestra qué tipo de instancias de entrenamiento fueron más influyentes para predecir la instancia 7. FIGURA 6.18: Árbol de decisión que explica qué instancias fueron más influyentes para predecir la 7-ésima instancia. Los datos de mujeres que fumaron durante 18.5 años o más tuvieron una gran influencia en el predicción de la 7-ésima instancia, con un cambio promedio en la predicción absoluta en 11.7 puntos porcentuales de probabilidad de cáncer Las instancias de datos de mujeres que fumaron o han estado fumando durante 18.5 años o más tienen una gran influencia en la predicción de la 7-ésima instancia. La mujer detrás de la 7 instancia fumaba por 34 años. En los datos, 12 mujeres (1.40%) fumaron 18.5 años o más. Cualquier error cometido en la recolección del número de años de fumar de una de estas mujeres tendrá un gran impacto en el resultado previsto para la 7-ésima instancia. El cambio más extremo en la predicción ocurre cuando eliminamos el número de instancia 663. El paciente supuestamente fumó por 22 años, alineado con los resultados del árbol de decisiones. La probabilidad predicha para la 7-ésima instancia cambia de 7.35% a 66.60% si eliminamos la instancia 663! Si observamos más de cerca las características de la instancia más influyente, podemos ver otro posible problema. Los datos dicen que la mujer tiene 28 años y ha estado fumando durante 22 años. O es un caso realmente extremo y ella realmente comenzó a fumar a los 6, o esto es un error de datos. Tiendo a creer lo último. Esta es ciertamente una situación en la que debemos cuestionar la precisión de los datos. Estos ejemplos mostraron lo útil que es identificar instancias influyentes para depurar modelos. Un problema con el enfoque propuesto es que el modelo necesita ser reentrenado para cada instancia de entrenamiento. Todo el reentrenamiento puede ser bastante lento, porque si tienes miles de instancias de entrenamiento, tendrás que reentrenar tu modelo miles de veces. Suponiendo que el modelo demore un día en entrenarse y que tenga 1000 instancias de entrenamiento, el cálculo de instancias influyentes, sin paralelización, tomará casi 3 años. Nadie tiene tiempo para esto. En el resto de este capítulo, te mostraré un método que no requiere volver a entrenar el modelo. 6.4.2 Funciones de influencia Tú: Quiero saber la influencia que tiene una instancia de entrenamiento en una predicción particular. Investigación: puede eliminar la instancia de entrenamiento, volver a entrenar el modelo y medir la diferencia en la predicción. Tú:¡Genial! ¿Pero tienes un método para mí que funcione sin reentrenamiento? Toma mucho tiempo Investigación: ¿Tiene un modelo con una función de pérdida que sea dos veces diferenciable con respecto a sus parámetros? Tú: Entrené una red neuronal con la pérdida logística. Entonces sí. Investigación : Entonces, puede aproximar la influencia de la instancia en los parámetros del modelo y en la predicción con funciones de influencia. La función de influencia es una medida de cuán fuertemente dependen los parámetros o predicciones del modelo de una instancia de entrenamiento. En lugar de eliminar la instancia, el método compensa la instancia en la pérdida en un paso muy pequeño. Este método implica aproximar la pérdida alrededor de los parámetros del modelo actual usando el gradiente y la matriz de Hesse. La pérdida de peso es similar a eliminar la instancia. Tú: Genial, ¡eso es lo que estoy buscando! Koh y Liang (2017)59 sugirieron usar funciones de influencia, un método de estadísticas robustas, para medir cómo una instancia influye en los parámetros o predicciones del modelo. Al igual que con los diagnósticos de eliminación, las funciones de influencia rastrean los parámetros y predicciones del modelo hasta la instancia de entrenamiento responsable. Sin embargo, en lugar de eliminar instancias de entrenamiento, el método se aproxima a cuánto cambia el modelo cuando la instancia se compensa en el riesgo empírico (suma de la pérdida sobre los datos de entrenamiento). El método de las funciones de influencia requiere acceso al gradiente de pérdida con respecto a los parámetros del modelo, que solo funciona para un subconjunto de modelos de aprendizaje automático. La regresión logística, las redes neuronales y las máquinas de vectores de soporte califican, los métodos basados en árboles como los random forest no. Las funciones de influencia ayudan a comprender el comportamiento del modelo, depurar el modelo y detectar errores en el conjunto de datos. La siguiente sección explica la intuición y las matemáticas detrás de las funciones de influencia. Matemáticas detrás de las funciones de influencia La idea clave detrás de las funciones de influencia es aumentar la pérdida de una instancia de entrenamiento mediante un paso infinitesimalmente pequeño \\(\\epsilon\\), que da como resultado nuevos parámetros del modelo: \\[\\hat{\\theta}_{\\epsilon,z}=\\arg\\min_{\\theta{}\\in\\Theta}(1-\\epsilon)\\frac{1}{n}\\sum_{i=1}^n{}L(z_i,\\theta)+\\epsilon{}L(z,\\theta)\\] donde \\(\\theta\\) es el vector de parámetros del modelo y \\(\\hat{\\theta}_{\\epsilon,z}\\) es el vector de parámetros después de aumentar z por un número muy pequeño \\(\\epsilon\\). L es la función de pérdida con la que se entrenó el modelo, \\(z_i\\) son los datos de entrenamiento y z es la instancia de entrenamiento que queremos aumentar para simular su eliminación. La intuición detrás de esta fórmula es: ¿Cuánto cambiará la pérdida si aumentamos un poco de peso una instancia particular \\(z_i\\) de los datos de entrenamiento (\\(\\epsilon\\)) y atenuamos las otras instancias de datos en consecuencia? ¿Cómo sería el vector de parámetros para optimizar esta nueva pérdida combinada? La función de influencia de los parámetros, es decir, la influencia de la instancia de entrenamiento de ponderación z sobre los parámetros, se puede calcular de la siguiente manera. \\[I_{\\text{up,params}}(z)=\\left.\\frac{d{}\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\right|_{\\epsilon=0}=-H_{\\hat{\\theta}}^{-1}\\nabla_{\\theta}L(z,\\hat{\\theta})\\] La última expresión \\(\\nabla_{\\theta}L(z,\\hat{\\theta})\\) es el gradiente de pérdida con respecto a los parámetros para la instancia de entrenamiento ponderada. El gradiente es la tasa de cambio de la pérdida de la instancia de entrenamiento. Nos dice cuánto cambia la pérdida cuando cambiamos un poco los parámetros del modelo \\(\\hat{\\theta}\\). Una entrada positiva en el vector gradiente significa que un pequeño aumento en el parámetro del modelo correspondiente aumenta la pérdida, una entrada negativa significa que el aumento del parámetro reduce la pérdida. La primera parte \\(H^{-1}_{\\hat{\\theta}}\\) es la matriz Hessiana inversa (segunda derivada de la pérdida con respecto a los parámetros del modelo). La matriz Hessiana es la tasa de cambio del gradiente, o expresada como pérdida, es la tasa de cambio de la tasa de cambio de la pérdida. Se puede estimar usando: \\[H_{\\theta}=\\frac{1}{n}\\sum_{i=1}^n\\nabla^2_{\\hat{\\theta}}L(z_i,\\hat{\\theta})\\] Más informalmente: La matriz de Hesse registra cuán curvada es la pérdida en cierto punto. El Hessiano es una matriz y no solo un vector, porque describe la curvatura de la pérdida y la curvatura depende de la dirección en la que miremos. El cálculo real del Hessiano lleva mucho tiempo si tiene muchos parámetros. Koh y Liang sugirieron algunos trucos para calcularlo de manera eficiente, lo que va más allá del alcance de este capítulo. Actualizar los parámetros del modelo, como se describe en la fórmula anterior, es equivalente a dar un solo paso de Newton después de formar una expansión cuadrática alrededor de los parámetros estimados del modelo. ¿Qué intuición hay detrás de esta fórmula de función de influencia? La fórmula proviene de formar una expansión cuadrática alrededor de los parámetros \\(\\hat{\\theta}\\). Eso significa que en realidad no lo sabemos, o es demasiado complejo calcular cómo cambiará exactamente la pérdida de instancia z cuando se elimine/aumente. Aproximamos la función localmente mediante el uso de información sobre la inclinación (= gradiente) y la curvatura (= matriz de Hesse) en la configuración del parámetro del modelo actual. Con esta aproximación de pérdida, podemos calcular cómo se verían aproximadamente los nuevos parámetros si ponderamos la instancia z: \\[\\hat{\\theta}_{-z}\\approx\\hat{\\theta}-\\frac{1}{n}I_{\\text{up,params}}(z)\\] El vector de parámetro aproximado es básicamente el parámetro original menos el gradiente de la pérdida de z (porque queremos disminuir la pérdida) escalado por la curvatura (= multiplicado por la matriz Hessiana inversa) y escalado por 1 sobre n, porque ese es la peso de una sola instancia de entrenamiento. La siguiente figura muestra cómo funciona el aumento de peso. El eje x muestra el valor del parámetro \\(\\theta\\) y el eje y el valor correspondiente de la pérdida con instancia ponderada z. El parámetro modelo aquí es unidimensional para fines de demostración, pero en realidad suele ser de alta dimensión. Nos movemos solo 1 sobre n en la dirección de mejora de la pérdida, por ejemplo z. No sabemos cómo cambiaría realmente la pérdida cuando eliminamos z, pero con la primera y segunda derivada de la pérdida, creamos esta aproximación cuadrática alrededor de nuestro parámetro de modelo actual y pretendemos que así es como se comportaría la pérdida real. FIGURA 6.19: Actualizando el parámetro del modelo (eje x) formando una expansión cuadrática de la pérdida alrededor del parámetro del modelo actual, y moviendo 1/n en la dirección en que la pérdida con la instancia ponderada z (eje y) mejora más. Esta ponderación de la instancia z en la pérdida se aproxima a los cambios de parámetros si eliminamos z y entrenamos el modelo en los datos reducidos. En realidad, no necesitamos calcular los nuevos parámetros, pero podemos usar la función de influencia como una medida de la influencia de z en los parámetros. ¿Cómo cambian las predicciones cuando aumentamos de peso la instancia de entrenamiento z? Podemos calcular los nuevos parámetros y luego hacer predicciones usando el modelo recién parametrizado, o también podemos calcular la influencia de la instancia z en las predicciones directamente, ya que podemos calcular la influencia usando la regla de la cadena: \\[\\begin{align*}I_{up,loss}(z,z_{test})&amp;=\\left.\\frac{d{}L(z_{test},\\hat{\\theta}_{\\epsilon,z})}{d\\epsilon}\\right|_{\\epsilon=0}\\\\&amp;=\\left.\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T\\frac{d\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\right|_{\\epsilon=0}\\\\&amp;=-\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T{}H^{-1}_{\\theta}\\nabla_{\\theta}L(z,\\hat{\\theta})\\end{align*}\\] La primera línea de esta ecuación significa que medimos la influencia de una instancia de entrenamiento en una determinada predicción \\(z_ {test}\\) como un cambio en la pérdida de la instancia de prueba cuando aumentamos la instancia z y obtenemos nuevos parámetros \\(\\hat{\\theta}_{\\epsilon,z}\\). Para la segunda línea de la ecuación, hemos aplicado la regla de la cadena de derivados y obtenemos la derivada de la pérdida de la instancia de prueba con respecto a los parámetros multiplicados por la influencia de z en los parámetros. En la tercera línea, reemplazamos la expresión con la función de influencia para los parámetros. El primer término en la tercera línea \\(\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T{}\\) es el gradiente de la instancia de prueba con respecto a los parámetros del modelo. Tener una fórmula es excelente y la forma científica y precisa de mostrar las cosas. Pero creo que es muy importante hacerse una idea de lo que significa la fórmula. La fórmula para \\(I_{\\text{up,loss}}\\) establece que la función de influencia de la instancia de entrenamiento z en la predicción de una instancia \\(z_{test}\\) es qué tan fuerte reacciona la instancia ante un cambio del modelo parámetrosmultiplicados por\" cuánto cambian los parámetros cuando aumentamos el peso de la instancia z\". Otra forma de leer la fórmula: La influencia es proporcional a cuán grandes son los gradientes para el entrenamiento y la pérdida de pruebas. Cuanto mayor es el gradiente de la pérdida de entrenamiento, mayor es su influencia en los parámetros y mayor es la influencia en la predicción de la prueba. Cuanto mayor sea el gradiente de la predicción de prueba, más influyente será la instancia de prueba. Toda la construcción también se puede ver como una medida de la similitud (según lo aprendido por el modelo) entre el entrenamiento y la instancia de prueba. Eso es todo con teoría e intuición. La siguiente sección explica cómo se pueden aplicar las funciones de influencia. Aplicación de funciones de influencia Las funciones de influencia tienen muchas aplicaciones, algunas de las cuales ya se han presentado en este capítulo. Comprender el comportamiento del modelo Diferentes modelos de aprendizaje automático tienen diferentes formas de hacer predicciones. Incluso si dos modelos tienen el mismo rendimiento, la forma en que hacen predicciones a partir de las características puede ser muy diferente y, por lo tanto, fallar en diferentes escenarios. Comprender las debilidades particulares de un modelo mediante la identificación de instancias influyentes ayuda a formar un modelo mental del comportamiento del modelo de aprendizaje automático en su mente. La siguiente figura muestra un ejemplo en el que se entrenó una SVM y una red neuronal para distinguir imágenes de perros y peces. Las instancias más influyentes de una imagen de un pez fueron muy diferentes para ambos modelos. Para el SVM, las instancias fueron influyentes si eran similares en color. Para la red neuronal, las instancias eran influyentes si eran conceptualmente similares. Para la red neuronal, incluso una imagen de un perro se encontraba entre las imágenes más influyentes, lo que demuestra que aprendió los conceptos y no la distancia euclidiana en el espacio de color. FIGURA 6.20: Perro o pez? Para la predicción SVM (fila central), las imágenes que tenían colores similares a la imagen de prueba fueron las más influyentes. Para la predicción de la red neuronal (fila inferior), los peces en diferentes entornos fueron los más influyentes, pero también una imagen de perro (arriba a la derecha). Trabajo de Koh y Liang (2017). Manejo de desajustes de dominio / Errores de modelo de depuración El manejo de la falta de coincidencia de dominio está estrechamente relacionado para comprender mejor el comportamiento del modelo. La falta de coincidencia de dominio significa que la distribución de los datos de entrenamiento y prueba es diferente, lo que puede hacer que el modelo tenga un bajo rendimiento en los datos de prueba. Las funciones de influencia pueden identificar instancias de entrenamiento que causaron el error. Supón que has entrenado un modelo de predicción para el resultado de pacientes que se han sometido a cirugía. Todos estos pacientes provienen del mismo hospital. Ahora usas el modelo en otro hospital y observas que no funciona bien para muchos pacientes. Por supuesto, asumes que los dos hospitales tienen pacientes diferentes, y si observas tus datos, puedes ver que difieren en muchas características. ¿Pero cuáles son las características o instancias que han roto el modelo? Aquí también, las instancias influyentes son una buena manera de responder esta pregunta. Tomas uno de los nuevos pacientes, para quienes el modelo ha hecho una predicción falsa, encuentra y analiza las instancias más influyentes. Por ejemplo, esto podría mostrar que el segundo hospital tiene pacientes mayores en promedio y las instancias más influyentes de los datos de entrenamiento son los pocos pacientes mayores del primer hospital y el modelo simplemente carecía de los datos para aprender a predecir bien este subgrupo. La conclusión sería que el modelo debe entrenarse en más pacientes mayores para que funcionen bien en el segundo hospital. Fijación de datos de entrenamiento Si tienes un límite en la cantidad de instancias de entrenamiento que puede verificar para verificar que sean correctas, ¿cómo haces una selección eficiente? La mejor manera es seleccionar las instancias más influyentes, porque, por definición, tienen la mayor influencia en el modelo. Incluso si tuvieras una instancia con valores obviamente incorrectos, si la instancia no es influyente y solo necesitas los datos para el modelo de predicción, es una mejor opción verificar las instancias influyentes. Por ejemplo, entrenas un modelo para predecir si un paciente debe permanecer en el hospital o ser dado de alta temprano. Realmente deseas asegurarte de que el modelo sea robusto y haga predicciones correctas, porque una liberación incorrecta de un paciente puede tener malas consecuencias. Los registros de pacientes pueden ser muy desordenados, por lo que no tienes una confianza perfecta en la calidad de los datos. Pero verificar la información del paciente y corregirla puede llevar mucho tiempo, porque una vez que ha informado qué pacientes debe verificar, el hospital realmente necesita enviar a alguien para que revise los registros de los pacientes seleccionados más de cerca, lo que podría estar escrito a mano y acostado en algún archivo. Verificar los datos de un paciente puede llevar una hora o más. En vista de estos costos, tiene sentido verificar solo algunas instancias de datos importantes. La mejor manera es seleccionar pacientes que hayan tenido una gran influencia en el modelo de predicción. Koh y Liang (2017) mostraron que este tipo de selección funciona mucho mejor que la selección aleatoria o la selección de aquellos con la mayor pérdida o clasificación incorrecta. 6.4.3 Ventajas de identificar instancias influyentes Los enfoques de diagnóstico de eliminación y las funciones de influencia son muy diferentes de los enfoques basados principalmente en perturbaciones de características presentados en el Capítulo Modelos agnósticos. Una mirada a instancias influyentes enfatiza el papel de los datos de entrenamiento en el proceso de aprendizaje. Esto hace que las funciones de influencia y los diagnósticos de eliminación sean una de las mejores herramientas de depuración para los modelos de aprendizaje automático. De las técnicas presentadas en este libro, son las únicas que ayudan directamente a identificar las instancias que deben verificarse en busca de errores. Los diagnósticos de eliminación son independientes del modelo, lo que significa que el enfoque se puede aplicar a cualquier modelo. También las funciones de influencia basadas en los derivados se pueden aplicar a una amplia clase de modelos. Podemos usar estos métodos para comparar diferentes modelos de aprendizaje automático y comprender mejor sus diferentes comportamientos, yendo más allá de comparar solo el rendimiento predictivo. No hemos hablado sobre este tema en este capítulo, pero las funciones de influencia a través de derivados también se pueden usar para crear datos de entrenamiento adversos. Estas son instancias que se manipulan de tal manera que el modelo no puede predecir ciertas instancias de prueba correctamente cuando el modelo está entrenado en esas instancias manipuladas. La diferencia con los métodos en el Capítulo de ejemplos adversos es que el ataque tiene lugar durante el tiempo de entrenamiento, también conocido como ataques de envenenamiento. Si estás interesado, lee el artículo de Koh y Liang (2017). Para el diagnóstico de eliminación y las funciones de influencia, consideramos la diferencia en la predicción y para la función de influencia el aumento de la pérdida. Pero, realmente, el enfoque es generalizable a cualquier pregunta de la forma: ¿Qué le sucede a  cuando borramos o aumentamos la instancia z?, Donde puedes llenar  con cualquier función de su modelo de su deseo. Puedes analizar cuánto influye una instancia de entrenamiento en la pérdida general del modelo. Puedes analizar cuánto influye una instancia de entrenamiento en la importancia de la función. Puedes analizar cuánto influye una instancia de entrenamiento en qué característica se selecciona para la primera división al entrenar un árbol de decisión. 6.4.4 Desventajas de identificar instancias influyentes Los diagnósticos de eliminación son muy costosos de calcular porque requieren un nuevo entrenamiento. Pero la historia ha demostrado que los recursos informáticos aumentan constantemente. Un cálculo que hace 20 años era impensable en términos de recursos se puede realizar fácilmente con su teléfono inteligente. Puedes entrenar modelos con miles de instancias de entrenamiento y cientos de parámetros en una computadora portátil en segundos / minutos. Por lo tanto, no es un gran salto suponer que los diagnósticos de eliminación funcionarán sin problemas incluso con grandes redes neuronales en 10 años. Las funciones de influencia son una buena alternativa al diagnóstico de eliminación, pero solo para modelos con parámetros diferenciables, como las redes neuronales. No funcionan para métodos basados en árboles como bosques aleatorios, árboles potenciados o árboles de decisión. Incluso si tienes modelos con parámetros y una función de pérdida, la pérdida puede no ser diferenciable. Pero para el último problema, hay un truco: Usa una pérdida diferenciable como sustituto para calcular la influencia cuando, por ejemplo, el modelo subyacente usa la función de pérdida Hinge Loss en lugar de alguna pérdida diferenciable. La pérdida se reemplaza por una versión suavizada de la pérdida problemática para las funciones de influencia, pero el modelo aún se puede entrenar con la pérdida no uniforme. Las funciones de influencia son solo aproximadas, porque el enfoque forma una expansión cuadrática alrededor de los parámetros. La aproximación puede ser incorrecta y la influencia de una instancia en realidad es mayor o menor cuando se elimina. Koh y Liang (2017) mostraron para algunos ejemplos que la influencia calculada por la función de influencia estaba cerca de la medida de influencia obtenida cuando el modelo fue realmente reentrenado después de que se eliminó la instancia. Pero no hay garantía de que la aproximación siempre sea tan cercana. No hay límite claro de la medida de influencia en la que llamamos una instancia influyente o no influyente. Es útil ordenar las instancias por influencia, pero sería genial tener los medios no solo para ordenar las instancias, sino para distinguir entre influyentes y no influyentes. Por ejemplo, si identificas las 10 instancias de entrenamiento más influyentes para una instancia de prueba, algunas de ellas pueden no ser influyentes porque, por ejemplo, solo las 3 principales fueron realmente influyentes. Las medidas de influencia solo tienen en cuenta la eliminación de instancias individuales y no la eliminación de varias instancias a la vez. Grupos más grandes de instancias de datos pueden tener algunas interacciones que influyen fuertemente en el entrenamiento y predicción del modelo. Pero el problema radica en la combinatoria: Hay n posibilidades de eliminar una instancia individual de los datos. Hay n veces (n-1) posibilidades de eliminar dos instancias de los datos de entrenamiento. Hay n veces (n-1) veces (n-2) posibilidades de eliminar tres  Creo que puedes ver a dónde va esto, hay demasiadas combinaciones. 6.4.5 Software y alternativas Los diagnósticos de eliminación son muy simples de implementar. Eche un vistazo al código que escribí para los ejemplos en este capítulo. Para modelos lineales y modelos lineales generalizados, muchas medidas de influencia como la distancia de Cook se implementan en R en el paquete stats. Koh y Liang publicaron el código de Python para funciones de influencia de su artículo en un repositorio. Eso es genial! Desafortunadamente, es solo el código del documento y no un módulo Python mantenido y documentado. El código se centra en la biblioteca Tensorflow, por lo que no puedes usarlo directamente para modelos de caja negra que usan otros marcos, como sci-kit learn. Keita Kurita escribió una gran publicación de blog para funciones de influencia eso me ayudó a entender mejor el papel de Koh y Liang. La publicación del blog profundiza un poco más en las matemáticas detrás de las funciones de influencia para los modelos de caja negra y también habla sobre algunos de los trucos matemáticos con los que el método se implementa de manera eficiente. Cook, R. Dennis. Detection of influential observation in linear regression. Technometrics 19.1 (1977): 15-18. Koh, Pang Wei, and Percy Liang. Understanding black-box predictions via influence functions. arXiv preprint arXiv:1703.04730 (2017). "],["redes-neuronales.html", "Capítulo 7 Interpretación de redes neuronales", " Capítulo 7 Interpretación de redes neuronales Los siguientes capítulos se centran en los métodos de interpretación para redes neuronales. Los métodos visualizan características y conceptos aprendidos por una red neuronal, explican predicciones individuales y simplifican las redes neuronales. El aprendizaje profundo ha sido muy exitoso, especialmente en tareas que involucran imágenes y textos como la clasificación de imágenes y la traducción de idiomas. La historia de éxito de las redes neuronales profundas comenzó en 2012, cuando el desafío de clasificación de imágenes ImageNet60 fue ganado por un enfoque de aprendizaje profundo. Desde entonces, hemos sido testigos de una explosión cámbrica de arquitecturas de redes neuronales profundas, con una tendencia hacia redes más profundas con más y más parámetros de peso. Para hacer predicciones con una red neuronal, la entrada de datos se pasa a través de muchas capas de multiplicación con los pesos aprendidos y a través de transformaciones no lineales. Una sola predicción puede involucrar millones de operaciones matemáticas dependiendo de la arquitectura de la red neuronal. No hay posibilidad de que los humanos podamos seguir el mapeo exacto desde la entrada de datos hasta la predicción. Tendríamos que considerar millones de pesos que interactúan de manera compleja para comprender una predicción de una red neuronal. Para interpretar el comportamiento y las predicciones de las redes neuronales, necesitamos métodos de interpretación específicos. Los capítulos suponen que está familiarizado con el aprendizaje profundo, incluidas las redes neuronales convolucionales. Ciertamente, podemos usar métodos modelo-agnósticos, como modelos locales o gráficos de dependencia parcial, pero hay dos razones por las cuales tiene sentido considerar los métodos de interpretación desarrollados específicamente para redes neuronales: Primero, las redes neuronales aprenden características y conceptos en sus capas ocultas y necesitamos herramientas especiales para descubrirlas. En segundo lugar, el gradiente puede utilizarse para implementar métodos de interpretación que sean más eficientes desde el punto de vista computacional que los métodos independientes del modelo que observan el modelo desde afuera. Además, la mayoría de los otros métodos en este libro están destinados a la interpretación de modelos para datos tabulares. Los datos de imagen y texto requieren diferentes métodos. Los siguientes capítulos cubren los siguientes temas: Visualización de características: ¿Qué características ha aprendido la red neuronal? Los ejemplos adversos del capítulo Explicaciones basadas en ejemplos están estrechamente relacionados con la visualización de características: ¿Cómo podemos manipular las entradas para obtener una clasificación incorrecta? Conceptos (EN CURSO): ¿Qué conceptos más abstractos ha aprendido la red neuronal? Atribución de funciones (EN CURSO): ¿Cómo contribuyó cada entrada a una predicción particular? Destilación del modelo (EN CURSO): ¿Cómo podemos explicar una red neuronal con un modelo más simple? Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015 "],["cnn-features.html", "7.1 Características aprendidas", " 7.1 Características aprendidas Las redes neuronales convolucionales aprenden características y conceptos abstractos de los píxeles de imagen sin procesar. Visualización de características visualiza las características aprendidas mediante la maximización de la activación. Disección de red etiqueta las unidades de red neuronal (Ej., Canales) con conceptos humanos. Las redes neuronales profundas aprenden características de alto nivel en las capas ocultas. Esta es una de sus mayores fortalezas y reduce la necesidad de ingeniería de características. Supón que deseas construir un clasificador de imágenes con una SVM. Las matrices de píxeles sin procesar no son la mejor entrada para entrenar su SVM, por lo que creas nuevas características basadas en color, dominio de frecuencia, detectores de bordes, etc. Con redes neuronales convolucionales, la imagen se alimenta a la red en su forma cruda (píxeles). La red transforma la imagen muchas veces. Primero, la imagen atraviesa muchas capas convolucionales. En esas capas convolucionales, la red aprende características nuevas y cada vez más complejas en sus capas. Luego, la información de la imagen transformada atraviesa las capas completamente conectadas y se convierte en una clasificación o predicción. FIGURA 7.1: Características aprendidas por una red neuronal convolucional (Inception V1) entrenada en los datos de ImageNet. Las características van desde características simples en las capas convolucionales inferiores (izquierda) hasta características más abstractas en las capas convolucionales superiores (derecha). Figura de Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/. Las primeras capas convolucionales aprenden características como bordes y texturas simples. Las capas convolucionales posteriores aprenden características como texturas y patrones más complejos. Las últimas capas convolucionales aprenden características como objetos o partes de objetos. Las capas completamente conectadas aprenden a conectar las activaciones de las características de alto nivel a las clases individuales que se preverán. Bien. Pero, ¿cómo obtenemos esas imágenes alucinantes? 7.1.1 Visualización de características El enfoque de hacer explícitas las características aprendidas se llama Visualización de características. La visualización de características para una unidad de una red neuronal se realiza mediante la búsqueda de la entrada que maximiza la activación de esa unidad. Unidad se refiere a neuronas individuales, canales (también llamados mapas de características), capas enteras o la probabilidad de clase final en la clasificación (o la neurona pre-softmax correspondiente, que se recomienda). Las neuronas individuales son unidades atómicas de la red, por lo que obtendríamos la mayor cantidad de información creando visualizaciones de características para cada neurona. Pero hay un problema: Las redes neuronales a menudo contienen millones de neuronas. Mirar la visualización de las características de cada neurona llevaría demasiado tiempo. Los canales (a veces llamados mapas de activación) como unidades son una buena opción para la visualización de características. Podemos ir un paso más allá y visualizar una capa convolucional completa. Las capas como una unidad se usan para DeepDream de Google, que agrega repetidamente las características visualizadas de una capa a la imagen original, lo que resulta en una versión de entrada de ensueño. FIGURA 7.2: La visualización de características se puede hacer para diferentes unidades. A) Neurona de convolución, B) Canal de convolución, C) Capa de convolución, D) Neurona, E) Capa oculta, F) Neurona de probabilidad de clase (o correspondiente neurona pre-softmax) 7.1.1.1 Visualización de características a través de la optimización En términos matemáticos, la visualización de características es un problema de optimización. Suponemos que los pesos de la red neuronal son fijos, lo que significa que la red está entrenada. Estamos buscando una nueva imagen que maximice la activación (media) de una unidad, aquí una sola neurona: \\[img^*=\\arg\\max_{img}h_{n,x,y,z}(img)\\] La función \\(h\\) es la activación de una neurona, img la entrada de la red (una imagen), x e y describen la posición espacial de la neurona, n especifica la capa y z es el índice del canal. Para la activación media de todo un canal z en la capa n, maximizamos: \\[img^*=\\arg\\max_{img}\\sum_{x,y}h_{n,x,y,z}(img)\\] En esta fórmula, todas las neuronas en el canal z tienen la misma ponderación. Alternativamente, también puedes maximizar las direcciones aleatorias, lo que significa que las neuronas se multiplicarían por diferentes parámetros, incluidas las direcciones negativas. De esta manera, estudiamos cómo interactúan las neuronas dentro del canal. En lugar de maximizar la activación, también puedes minimizar la activación (que corresponde a maximizar la dirección negativa). Curiosamente, cuando maximizas la dirección negativa, obtienes características muy diferentes para la misma unidad: FIGURA 7.3: Activación positiva (izquierda) y negativa (derecha) de la neurona Inception V1 484 de la capa mixed4d pre relu. Mientras que la neurona se activa al máximo por las ruedas, algo que parece tener ojos produce una activación negativa. Código: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb Podemos abordar este problema de optimización de diferentes maneras. Primero, ¿por qué deberíamos generar nuevas imágenes? Simplemente podríamos buscar a través de nuestras imágenes de entrenamiento y seleccionar aquellas que maximicen la activación. Este es un enfoque válido, pero el uso de datos de entrenamiento tiene el problema de que los elementos de las imágenes se pueden correlacionar y no podemos ver lo que realmente está buscando la red neuronal. Si las imágenes que producen una alta activación de cierto canal muestran un perro y una pelota de tenis, no sabemos si la red neuronal mira al perro, la pelota de tenis o tal vez a ambos. El otro enfoque es generar nuevas imágenes, a partir del ruido aleatorio. Para obtener visualizaciones significativas, generalmente hay restricciones en la imagen, como que solo se permiten pequeños cambios. Para reducir el ruido en la visualización de funciones, puedes aplicar fluctuación, rotación o escala a la imagen antes del paso de optimización. Otras opciones de regularización incluyen penalización de frecuencia (por ejemplo, reducir la varianza de los píxeles vecinos) o generar imágenes con antecedentes aprendidos, por ejemplo con redes generativas de confrontación (GAN)[^sintetizar] o autoencoder denoising 61. FIGURA 7.4: Optimización iterativa de imagen aleatoria para maximizar la activación. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/. Si deseas profundizar mucho más en la visualización de características, echa un vistazo al diario en línea distill.pub, especialmente la publicación de visualización de características de Olah et al. 62, del cual utilicé muchas de las imágenes, y también sobre los bloques de construcción de la interpretabilidad 63. 7.1.1.2 Conexión a ejemplos adversarios Existe una conexión entre la visualización de características y ejemplos adversos: Ambas técnicas maximizan la activación de una unidad de red neuronal. Para ejemplos adversos, buscamos la activación máxima de la neurona para la clase adversaria (= incorrecta). Una diferencia es la imagen con la que comenzamos: Para ejemplos adversos, es la imagen para la que queremos generar la imagen adversaria. Para la visualización de características es, según el enfoque, ruido aleatorio. 7.1.1.3 Texto y datos tabulares La literatura se centra en la visualización de características para redes neuronales convolucionales para el reconocimiento de imágenes. Técnicamente, no hay nada que te impida encontrar la entrada que activa al máximo una neurona de una red neuronal completamente conectada para datos tabulares o una red neuronal recurrente para datos de texto. Es posible que ya no lo llame visualización de características, ya que la característica sería una entrada de datos tabular o texto. Para la predicción de incumplimiento de crédito, las entradas pueden ser la cantidad de créditos anteriores, la cantidad de contratos móviles, la dirección y docenas de otras características. La característica aprendida de una neurona sería una cierta combinación de las docenas de características. Para redes neuronales recurrentes, es un poco más agradable visualizar lo que aprendió la red: Karpathy et. al (2015) 64 mostró que las redes neuronales recurrentes tienen neuronas que aprenden características interpretables. Entrenaron un modelo a nivel de personaje, que predice el siguiente personaje en la secuencia de los personajes anteriores. Una vez que se produjo una llave de apertura (, una de las neuronas se activó altamente y se desactivó cuando se produjo la llave de cierre correspondiente ). Otras neuronas dispararon al final de una línea. Algunas neuronas dispararon en URL. La diferencia con la visualización de características para CNN es que los ejemplos no se encontraron a través de la optimización, sino al estudiar las activaciones de neuronas en los datos de entrenamiento. Algunas de las imágenes parecen mostrar conceptos bien conocidos como hocicos de perros o edificios. Pero, ¿cómo podemos estar seguros? El método de disección de red vincula conceptos humanos con unidades de redes neuronales individuales. Alerta de spoiler: la disección de red requiere conjuntos de datos adicionales que alguien ha etiquetado con conceptos humanos. 7.1.2 Disección de red El enfoque de disección de red de Bau &amp; Zhou et al. (2017) [^Disect] cuantifica la interpretabilidad de una unidad de una red neuronal convolucional. Vincula áreas altamente activadas de canales CNN con conceptos humanos (objetos, partes, texturas, colores, ). Los canales de una red neuronal convolucional aprenden nuevas características, como vimos en el capítulo sobre Visualización de características. Pero estas visualizaciones no prueban que una unidad haya aprendido un cierto concepto. Tampoco tenemos una medida de qué tan bien detecta una unidad, por ejemplo, rascacielos. Antes de entrar en detalles sobre la disección de red, tenemos que hablar sobre la gran hipótesis que está detrás de esa línea de investigación. La hipótesis es: Las unidades de una red neuronal (como canales convolucionales) aprenden conceptos desenredados. La cuestión de las características desenredadas ¿Las redes neuronales (convolucionales) aprenden características desenredadas? Las características desenredadas significan que las unidades de red individuales detectan conceptos específicos del mundo real. El canal convolucional 394 podría detectar rascacielos, el hocico del perro del canal 121, las franjas del canal 12 en un ángulo de 30 grados  Lo contrario de una red desenredada es una red completamente enredada. En una red completamente enredada, por ejemplo, no habría una unidad individual para hocicos de perros. Todos los canales contribuirían al reconocimiento de los hocicos de perros. Las características desenredadas implican que la red es altamente interpretable. Supongamos que tenemos una red con unidades completamente desenredadas que están etiquetadas con conceptos conocidos. Esto abriría la posibilidad de rastrear el proceso de toma de decisiones de la red. Por ejemplo, podríamos analizar cómo la red clasifica a los lobos contra los siberianos. Primero, identificamos la unidad siberianos. Podemos verificar si esta unidad depende de las unidades hocico del perro, piel esponjosa y nieve de la capa anterior. Si lo hace, sabemos que clasificará erróneamente una imagen de un husky con un fondo nevado como un lobo. En una red desenredada, podríamos identificar correlaciones no causales problemáticas. Podríamos enumerar automáticamente todas las unidades altamente activadas y sus conceptos para explicar una predicción individual. Podríamos detectar fácilmente el sesgo en la red neuronal. Por ejemplo, ¿la red aprendió una característica de piel blanca para predecir el salario? Alerta de spoiler: las redes neuronales convolucionales no están perfectamente desenredadas. Ahora veremos más de cerca la disección de red para descubrir qué tan interpretables son las redes neuronales. 7.1.2.1 Algoritmo de disección de red La disección de red tiene tres pasos: Obtén imágenes con conceptos visuales etiquetados como humanos, desde rayas hasta rascacielos. Mide las activaciones del canal CNN para estas imágenes. Cuantifica la alineación de activaciones y conceptos etiquetados. La siguiente figura visualiza cómo una imagen se reenvía a un canal y coincide con los conceptos etiquetados. FIGURA 7.5: Para una imagen de entrada dada y una red entrenada (pesos fijos), reenviamos la imagen hasta la capa objetivo, aumentamos las activaciones para que coincidan con el tamaño de la imagen original y comparamos las activaciones máximas con la segmentación de la verdad fundamental en cuanto a píxeles. Figura originalmente de Bau &amp; Zhou et. al. (2017). Paso 1: Ampliar conjunto de datos El primer paso difícil pero crucial es la recopilación de datos. La disección de red requiere imágenes etiquetadas en píxeles con conceptos de diferentes niveles de abstracción (desde colores hasta escenas callejeras). Bau y Zhou et. Todos combinaron un par de conjuntos de datos con conceptos de píxeles. Llamaron a este nuevo conjunto de datos Broden, que significa datos etiquetados de manera amplia y densa. El conjunto de datos de Broden se segmenta principalmente al nivel de píxeles, para algunos conjuntos de datos se etiqueta toda la imagen. Broden contiene 60,000 imágenes con más de 1,000 conceptos visuales en diferentes niveles de abstracción: 468 escenas, 585 objetos, 234 partes, 32 materiales, 47 texturas y 11 colores. La siguiente figura muestra imágenes de muestra del conjunto de datos de Broden. FIGURA 7.6: Imágenes de ejemplo del conjunto de datos de Broden. Figura originalmente de Bau &amp; Zhou et. al (2017). Paso 2: recuperar las activaciones de red A continuación, creamos las máscaras de las áreas activadas superiores por canal y por imagen. En este punto, las etiquetas de concepto aún no están involucradas. Para cada canal convolucional k: Para cada imagen x en el conjunto de datos de Broden Reenvía la imagen x a la capa objetivo que contiene el canal k. Extrae las activaciones de píxeles del canal convolucional k: \\(A_k(x)\\) Calcular la distribución de las activaciones de píxeles \\(\\alpha_k\\) sobre todas las imágenes Determina el nivel de cuantificación 0.005 \\(T_k\\) de las activaciones \\(\\alpha_k\\). Esto significa que el 0.5% de todas las activaciones del canal k para la imagen x son mayores que \\(T_k\\). Para cada imagen x en el conjunto de datos de Broden: Escala el (posiblemente) mapa de activación de baja resolución \\(A_k(x)\\) a la resolución de la imagen x. Llamamos al resultado \\(S_k(x)\\). Binariza el mapa de activación: un píxel está activado o desactivado, dependiendo de si excede el umbral de activación \\(T_k\\). La nueva máscara es \\(M_k(x)=S_k(x)\\geq{}T_k(x)\\). Paso 3: alineación del concepto de activación Después del paso 2 tenemos una máscara de activación por canal e imagen. Estas máscaras de activación marcan áreas altamente activadas. Para cada canal queremos encontrar el concepto humano que activa ese canal. Encontramos el concepto comparando las máscaras de activación con todos los conceptos etiquetados. Cuantificamos la alineación entre la máscara de activación k y la máscara de concepto c con la puntuación de Intersección sobre Unión (IoU): \\[IoU_{k,c}=\\frac{\\sum|M_k(x)\\bigcap{}L_c(x)|}{\\sum|M_k(x)\\bigcup{}L_c(x)|}\\] donde \\(|\\cdot|\\) es la cardinalidad de un conjunto. La intersección sobre la unión compara la alineación entre dos áreas. \\(IoU_{k,c}\\) puede interpretarse como la precisión con la que la unidad k detecta el concepto c. Llamamos a la unidad k un detector del concepto c cuando \\(IoU_{k,c}&gt;0.04\\). Este umbral fue elegido por Bau &amp; Zhou et. Alabama. La siguiente figura ilustra la intersección y unión de la máscara de activación y la máscara de concepto para una sola imagen: FIGURA 7.7: La Intersección sobre la Unión (IoU) se calcula comparando la anotación de verdad del terreno humano y los píxeles superiores activados. La siguiente figura muestra una unidad que detecta perros: FIGURA 7.8: Máscara de activación para el canal 750 de inicio_4e que detecta perros con \\(IoU = 0.203\\). Figura originalmente de Bau &amp; Zhou et. al (2017). 7.1.2.2 Experimentos Los autores de Network Dissection entrenaron diferentes arquitecturas de red (AlexNet, VGG, GoogleNet, ResNet) desde cero en diferentes conjuntos de datos (ImageNet, Places205, Places365). ImageNet contiene 1,6 millones de imágenes de 1000 clases que se centran en objetos. Places205 y Places365 contienen 2,4 millones / 1,6 millones de imágenes de 205/365 escenas diferentes. Además, entrenaron a AlexNet en tareas de entrenamiento auto supervisadas, como predecir el orden de los cuadros de video o colorear imágenes. Para muchas de estas configuraciones diferentes, contaron el número de detectores de conceptos únicos como una medida de interpretabilidad. Estos son algunos de los hallazgos: Las redes detectan conceptos de nivel inferior (colores, texturas) en capas inferiores y conceptos de nivel superior (partes, objetos) en capas superiores. Ya hemos visto esto en [Visualizaciones de características] (#visualización-características). La normalización por lotes reduce la cantidad de detectores de concepto únicos. Muchas unidades detectan el mismo concepto. Por ejemplo, hay 95 (!) Canales de perros en VGG entrenados en ImageNet cuando se usa \\(IoU \\geq 0.04\\) como límite de detección (4 en conv4_3, 91 en conv5_3, consulte sitio web del proyecto). Aumentar el número de canales en una capa aumenta el número de unidades interpretables. Las inicializaciones aleatorias (entrenamiento con diferentes semillas aleatorias) dan como resultado números ligeramente diferentes de unidades interpretables. ResNet es la arquitectura de red con el mayor número de detectores únicos, seguida por VGG, GoogleNet y AlexNet. El mayor número de detectores de conceptos únicos se aprende para Places356, seguido de Places205 e ImageNet en último lugar. El número de detectores de concepto únicos aumenta con el número de iteraciones de entrenamiento. FIGURA 7.9: ResNet entrenado en Places365 tiene el mayor número de detectores únicos. AlexNet con pesos aleatorios tiene el menor número de detectores únicos y sirve como línea de base. Figura originalmente de Bau &amp; Zhou et. al (2017). Las redes entrenadas en tareas auto supervisadas tienen menos detectores únicos en comparación con las redes entrenadas en tareas supervisadas. En el aprendizaje por transferencia, el concepto de un canal puede cambiar. Por ejemplo, un detector de perros se convirtió en un detector de cascada. Esto sucedió en un modelo que inicialmente se entrenó para clasificar objetos y luego se ajustó para clasificar escenas. En uno de los experimentos, los autores proyectaron los canales en una nueva base girada. Esto se hizo para la red VGG entrenada en ImageNet. Girado no significa que la imagen se haya girado. Girado significa que tomamos los 256 canales de la capa conv5 y calculamos los nuevos 256 canales como combinaciones lineales de los canales originales. En el proceso, los canales se enredan. La rotación reduce la capacidad de interpretación, es decir, el número de canales alineados con un concepto disminuye. La rotación fue diseñada para mantener el rendimiento del modelo igual. La primera conclusión: La interpretabilidad de las CNN depende del eje. Esto significa que las combinaciones aleatorias de canales tienen menos probabilidades de detectar conceptos únicos. La segunda conclusión: La interpretabilidad es independiente del poder discriminativo. Los canales se pueden transformar con transformaciones ortogonales, mientras que el poder discriminativo sigue siendo el mismo, pero la interpretabilidad disminuye. FIGURA 7.10: El número de detectores de concepto únicos disminuye cuando los 256 canales de AlexNet conv5 (entrenados en ImageNet) se cambian gradualmente a una base utilizando una transformación ortogonal aleatoria. Figura originalmente de Bau &amp; Zhou et. al (2017). Los autores también utilizaron Disección de red para redes adversas generativas (GAN). Puedes encontrar Disección de red para GAN en el sitio web del proyecto. 7.1.3 Ventajas Las visualizaciones de funciones brindan una visión única del funcionamiento de las redes neuronales, especialmente para el reconocimiento de imágenes. Dada la complejidad y la opacidad de las redes neuronales, la visualización de características es un paso importante en el análisis y la descripción de las redes neuronales. A través de la visualización de características, hemos aprendido que las redes neuronales primero aprenden detectores simples de bordes y texturas y detectores de partes y objetos más abstractos en capas superiores. La disección de red amplía esos conocimientos y hace que la capacidad de interpretación de las unidades de red sea medible. La disección de red nos permite vincular automáticamente unidades a conceptos, lo cual es muy conveniente. La visualización de características es una gran herramienta para comunicar de manera no técnica cómo funcionan las redes neuronales. Con la disección de red, también podemos detectar conceptos más allá de las clases en la tarea de clasificación. Pero necesitamos conjuntos de datos que contengan imágenes con conceptos etiquetados en píxeles. La visualización de características se puede combinar con métodos de atribución de características, que explican qué píxeles fueron importantes para la clasificación. La combinación de ambos métodos permite explicar una clasificación individual junto con la visualización local de las características aprendidas que estuvieron involucradas en la clasificación. Consulta Los componentes básicos de la interpretabilidad de distill.pub. Finalmente, las visualizaciones de características hacen excelentes fondos de escritorio y estampados de camisetas. 7.1.4 Desventajas Muchas imágenes de visualización de características no son interpretables, pero contienen algunas características abstractas para las cuales no tenemos palabras o conceptos mentales. La visualización de visualizaciones de características junto con datos de entrenamiento puede ayudar. Es posible que las imágenes aún no revelen a qué reaccionó la red neuronal y solo muestren algo como tal vez debe haber algo amarillo en las imágenes. Incluso con la disección de red, algunos canales no están vinculados a un concepto humano. Por ejemplo, la capa conv5_3 de VGG entrenada en ImageNet tiene 193 canales (de 512) que no se pueden combinar con un concepto humano. Hay demasiadas unidades para mirar, incluso cuando solo visualiza las activaciones del canal. Para Inception V1 ya hay más de 5000 canales de 9 capas convolucionales. Si también deseas mostrar las activaciones negativas más algunas imágenes de los datos de entrenamiento que activan el canal de manera máxima o mínima (digamos 4 imágenes positivas, 4 negativas), entonces ya debes mostrar más de 50 000 imágenes. Al menos sabemos, gracias a la disección de red, que no necesitamos investigar direcciones aleatorias. ¿Ilusión de interpretabilidad? Las visualizaciones de características pueden transmitir la ilusión de que entendemos lo que está haciendo la red neuronal. ¿Pero realmente entendemos lo que está sucediendo en la red neuronal? Incluso si observamos cientos o miles de visualizaciones de características, no podemos entender la red neuronal. Los canales interactúan de manera compleja, las activaciones positivas y negativas no están relacionadas, las neuronas múltiples pueden aprender características muy similares y para muchas de las características no tenemos conceptos humanos equivalentes. No debemos caer en la trampa de creer que entendemos completamente las redes neuronales solo porque creemos que vimos que la neurona 349 en la capa 7 es activada por las margaritas. La disección de red mostró que arquitecturas como ResNet o Inception tienen unidades que reaccionan a ciertos conceptos. Pero el IoU no es tan bueno y, a menudo, muchas unidades responden al mismo concepto y algunas a ningún concepto. Sus canales no están completamente desenredados y no podemos interpretarlos de forma aislada. Para la disección de red, necesitas conjuntos de datos que estén etiquetados en el nivel de píxel con los conceptos. Estos conjuntos de datos requieren mucho esfuerzo para recopilarse, ya que cada píxel debe etiquetarse, lo que generalmente funciona dibujando segmentos alrededor de los objetos de la imagen. La disección de red solo alinea los conceptos humanos con activaciones positivas pero no con activaciones negativas de canales. Como lo mostraron las visualizaciones de características, las activaciones negativas parecen estar vinculadas a conceptos. Esto podría solucionarse mirando adicionalmente el cuantil más bajo de activaciones. 7.1.5 Software y material adicional Hay una implementación de código abierto de visualización de características llamada Lucid. Puede probarlo convenientemente en su navegador utilizando los enlaces del cuaderno que se proporcionan en la página Lucid Github. No se requiere software adicional. Otras implementaciones son tf_cnnvis para TensorFlow, Keras Filters para Keras y [DeepVis] ( https://github.com/yosinski/deep-visualization-toolbox) para Caffe. Network Dissection tiene un excelente sitio web del proyecto. Junto a la publicación, el sitio web aloja material adicional como código, datos y visualizaciones de máscaras de activación. Nguyen, Anh, et al. Plug &amp; play generative networks: Conditional iterative generation of images in latent space. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. Olah, et al., Feature Visualization, Distill, 2017. Olah, et al., The Building Blocks of Interpretability, Distill, 2018. Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078 (2015). "],["futuro.html", "Capítulo 8 Una mirada a la bola de cristal", " Capítulo 8 Una mirada a la bola de cristal ¿Cuál es el futuro del aprendizaje automático interpretable? Este capítulo es un ejercicio mental especulativo y una suposición subjetiva de cómo se desarrollará el aprendizaje automático interpretable. Abrí el libro con historias cortas bastante pesimistas y me gustaría concluir con una perspectiva más optimista. He basado mis predicciones en tres premisas: Digitalización: Cualquier información (interesante) será digitalizada. Piensa en el dinero electrónico y las transacciones en línea. Piensa en libros electrónicos, música y videos. Piensa en todos los datos sensoriales sobre nuestro entorno, nuestro comportamiento humano, los procesos de producción industrial, etc. Los impulsores de la digitalización de todo son: computadoras / sensores / almacenamiento baratos, efectos de escala (el ganador se lo lleva todo), nuevos modelos de negocios, cadenas de valor modulares, presión de costos y mucho más. Automatización: cuando una tarea se puede automatizar y el costo de la automatización es menor que el costo de realizar la tarea con el tiempo, la tarea se automatizará. Incluso antes de la introducción de la computadora, teníamos un cierto grado de automatización. Por ejemplo, la máquina de tejer automatizada o la máquina de vapor automatizada. Pero las computadoras y la digitalización llevan la automatización al siguiente nivel. Simplemente el hecho de que puede programar loops for, escribir macros de Excel, automatizar respuestas de correo electrónico, etc., muestra cuánto puede automatizar un individuo. Las máquinas de boletos automatizan la compra de boletos de tren (ya no se necesita cajero), las lavadoras automatizan la lavandería, las órdenes permanentes automatizan las transacciones de pago, etc. La automatización de tareas libera tiempo y dinero, por lo que existe un gran incentivo económico y personal para automatizar las cosas. Actualmente estamos observando la automatización de la traducción de idiomas, la conducción y, en menor medida, incluso el descubrimiento científico. Especificación incorrecta: no podemos especificar perfectamente un objetivo con todas sus limitaciones. Piensa en el genio en una botella que siempre toma tus deseos literalmente: ¡Quiero ser la persona más rica del mundo! -&gt; Te conviertes en la persona más rica, pero como efecto secundario, la moneda que tienes se devalúa debido a la inflación. ¡Quiero ser feliz por el resto de mi vida! -&gt; Los siguientes 5 minutos te sientes muy feliz, luego el genio te mata. ¡Deseo la paz mundial! -&gt; El genio mata a todos los humanos. Especificamos objetivos incorrectamente, ya sea porque no conocemos todas las restricciones o porque no podemos medirlas. Veamos a las corporaciones como un ejemplo de especificación de objetivos imperfectos. Una corporación tiene el objetivo simple de ganar dinero para sus accionistas. Pero esta especificación no captura el verdadero objetivo con todas sus limitaciones por las que realmente nos esforzamos: Por ejemplo, no apreciamos que una compañía mate personas para ganar dinero, envenene ríos o simplemente imprima su propio dinero. Hemos inventado leyes, reglamentos, sanciones, procedimientos de cumplimiento, sindicatos y más para parchear la especificación de objetivos imperfectos. Otro ejemplo que puedes experimentar por ti mismo es Paperclips, un juego en el que juegas una máquina con el objetivo de producir tantos clips como sea posible. ADVERTENCIA: es adictivo. No quiero estropearlo demasiado, pero digamos que las cosas se descontrolan muy rápido. En el aprendizaje automático, las imperfecciones en la especificación del objetivo provienen de abstracciones de datos imperfectas (poblaciones sesgadas, errores de medición, ), funciones de pérdida sin restricciones, falta de conocimiento de las restricciones, cambio de la distribución entre los datos de entrenamiento y aplicación y mucho más. . La digitalización está impulsando la automatización. La especificación de objetivos imperfecta entra en conflicto con la automatización. Afirmo que este conflicto está mediado parcialmente por métodos de interpretación. El escenario para nuestras predicciones está listo, la bola de cristal está lista, ¡ahora miramos hacia dónde podría ir el campo! "],["el-futuro-del-aprendizaje-automático.html", "8.1 El futuro del aprendizaje automático", " 8.1 El futuro del aprendizaje automático Sin aprendizaje automático no puede haber aprendizaje automático interpretable. Por lo tanto, tenemos que adivinar hacia dónde se dirige el aprendizaje automático antes de poder hablar sobre la interpretabilidad. El aprendizaje automático (o AI) está asociado con muchas promesas y expectativas. Pero comencemos con una observación menos optimista: Si bien la ciencia desarrolla muchas herramientas sofisticadas de aprendizaje automático, en mi experiencia es bastante difícil integrarlas en los procesos y productos existentes. No porque no sea posible, sino simplemente porque lleva tiempo para que las empresas e instituciones se pongan al día. En la fiebre del oro de la actual exageración de la IA, las empresas abren laboratorios de IA, unidades de aprendizaje automático y contratan científicos de datos, expertos en aprendizaje automático, ingenieros de IA, etc., pero la realidad es que, en mi experiencia, bastante frustrante. A menudo, las empresas ni siquiera tienen datos en la forma requerida y los científicos de datos esperan inactivos durante meses. A veces las empresas tienen una expectativa tan alta de IA y ciencia de datos debido a los medios que los científicos de datos nunca podrían cumplirlas. Y a menudo nadie sabe cómo integrar a los científicos de datos en las estructuras existentes y muchos otros problemas. Esto lleva a mi primera predicción. El aprendizaje automático crecerá lenta pero constantemente. La digitalización está avanzando y la tentación de automatizar está constantemente tirando. Incluso si el camino de la adopción del aprendizaje automático es lento y pedregoso, el aprendizaje automático se mueve constantemente de la ciencia a los procesos comerciales, productos y aplicaciones del mundo real. Creo que debemos explicar mejor a los no expertos qué tipos de problemas pueden formularse como problemas de aprendizaje automático. Conozco a muchos científicos de datos altamente remunerados que realizan cálculos de Excel o inteligencia empresarial clásica con informes y consultas SQL en lugar de aplicar el aprendizaje automático. Pero algunas empresas ya están utilizando con éxito el aprendizaje automático, con las grandes empresas de Internet a la vanguardia. Necesitamos encontrar mejores formas de integrar el aprendizaje automático en procesos y productos, capacitar a las personas y desarrollar herramientas de aprendizaje automático que sean fáciles de usar. Creo que el aprendizaje automático será mucho más fácil de usar: Ya podemos ver que el aprendizaje automático se está volviendo más accesible, por ejemplo, a través de los servicios en la nube (Aprendizaje automático como un servicio, solo para lanzar algunas palabras de moda). Una vez que el aprendizaje automático ha madurado, y este niño ya ha dado sus primeros pasos, mi próxima predicción es: El aprendizaje automático alimentará muchas cosas. Basado en el principio Todo lo que se pueda automatizar se automatizará, concluyo que siempre que sea posible, Las tareas se formularán como problemas de predicción y se resolverán con el aprendizaje automático. El aprendizaje automático es una forma de automatización o al menos puede ser parte de ella. Muchas tareas que actualmente realizan los humanos son reemplazadas por el aprendizaje automático. Aquí hay algunos ejemplos de tareas donde el aprendizaje automático se usa para automatizar partes de él: Clasificación / toma de decisiones / finalización de documentos (por ejemplo, en compañías de seguros, el sector legal o empresas de consultoría) Decisiones basadas en datos, como las solicitudes de crédito. Descubrimiento de medicamento Controles de calidad en líneas de montaje. Autos sin conductor Diagnóstico de enfermedades. Traducción. Para este libro, utilicé un servicio de traducción llamado (DeepL) impulsado por redes neuronales profundas para mejorar mis oraciones traduciéndolas del inglés al alemán y nuevamente al inglés.  El avance para el aprendizaje automático no solo se logra a través de mejores computadoras / más datos / mejor software, sino también: Las herramientas de interpretación catalizan la adopción del aprendizaje automático. Basado en la premisa de que el objetivo de un modelo de aprendizaje automático nunca puede especificarse perfectamente, se deduce que el aprendizaje automático interpretable es necesario para cerrar la brecha entre el objetivo mal especificado y el objetivo real. En muchas áreas y sectores, la interpretabilidad será el catalizador para la adopción del aprendizaje automático. Alguna evidencia anecdótica: Muchas personas con las que he hablado no usan el aprendizaje automático porque no pueden explicar los modelos a otros. Creo que la interpretabilidad abordará este problema y hará que el aprendizaje automático sea atractivo para las organizaciones y las personas que exigen cierta transparencia. Además de la especificación errónea del problema, muchas industrias requieren interpretabilidad, ya sea por razones legales, debido a la aversión al riesgo o para obtener una idea de la tarea subyacente. El aprendizaje automático automatiza el proceso de modelado y aleja un poco al ser humano de los datos y la tarea subyacente: Esto aumenta el riesgo de problemas con el diseño experimental, la elección de la distribución del entrenamiento, el muestreo, la codificación de datos, la ingeniería de características, etc. Las herramientas de interpretación facilitan la identificación de estos problemas. "],["el-futuro-de-la-interpretabilidad.html", "8.2 El futuro de la interpretabilidad", " 8.2 El futuro de la interpretabilidad Echemos un vistazo al posible futuro de la interpretabilidad del aprendizaje automático. La atención se centrará en las herramientas de interpretación independientes del modelo. Es mucho más fácil automatizar la interpretabilidad cuando se desacopla del modelo de aprendizaje automático subyacente. La ventaja de la interpretabilidad agnóstica del modelo radica en su modularidad. Podemos reemplazar fácilmente el modelo subyacente de aprendizaje automático. Podemos reemplazar con la misma facilidad el método de interpretación. Por estas razones, los métodos modelo-agnósticos escalarán mucho mejor. Es por eso que creo que los métodos modelo-agnósticos serán más dominantes a largo plazo. Pero los métodos intrínsecamente interpretables también tendrán un lugar. El aprendizaje automático se automatizará y, con él, la interpretabilidad. Una tendencia ya visible es la automatización de la formación de modelos. Eso incluye ingeniería automatizada y selección de características, optimización automática de hiperparámetros, comparación de diferentes modelos y ensamblaje o apilamiento de los modelos. El resultado es el mejor modelo de predicción posible. Cuando utilizamos métodos de interpretación independientes del modelo, podemos aplicarlos automáticamente a cualquier modelo que surja del proceso automatizado de aprendizaje automático. En cierto modo, también podemos automatizar este segundo paso: Calcula automáticamente la importancia de la característica, traza la dependencia parcial, entrena un modelo sustituto, etc. Nadie te impide calcular automáticamente todas estas interpretaciones de modelos. La interpretación real todavía requiere personas. Imagínate: cargas un conjunto de datos, especificas el objetivo de predicción y con solo presionar un botón se entrena el mejor modelo de predicción y el programa escupe todas las interpretaciones del modelo. Ya hay primeros productos y sostengo que para muchas aplicaciones será suficiente utilizar estos servicios automatizados de aprendizaje automático. Hoy cualquiera puede crear sitios web sin conocer HTML, CSS y Javascript, pero todavía hay muchos desarrolladores web. Del mismo modo, creo que todos podrán entrenar modelos de aprendizaje automático sin saber cómo programar, y aún será necesario contar con expertos en aprendizaje automático. No analizamos datos, analizamos modelos. Los datos sin procesar en sí mismos son siempre inútiles. (Exagero a propósito. La realidad es que necesitas una comprensión profunda de los datos para realizar un análisis significativo). No me importan los datos; Me importa el conocimiento contenido en los datos. El aprendizaje automático interpretable es una excelente manera de extraer el conocimiento de los datos. Puede sondear ampliamente el modelo, el modelo reconoce automáticamente si las características son relevantes para la predicción y cómo lo hacen (muchos modelos tienen una selección de características incorporada), el modelo puede detectar automáticamente cómo se representan las relaciones y, si se entrena correctamente, El modelo final es una muy buena aproximación de la realidad. Muchas herramientas analíticas ya se basan en modelos de datos (porque se basan en supuestos de distribución): Pruebas de hipótesis simples como la prueba t de Student. Pruebas de hipótesis con ajustes para factores de confusión (generalmente GLM) Análisis de varianza (ANOVA) El coeficiente de correlación (el coeficiente de regresión lineal estandarizado está relacionado con el coeficiente de correlación de Pearson)  Lo que estoy diciendo aquí en realidad no es nada nuevo. Entonces, ¿por qué pasar de analizar modelos transparentes basados en suposiciones a analizar modelos de caja negra sin suposiciones? Porque hacer todos estos supuestos es problemático: Por lo general, están equivocados (a menos que creas que la mayor parte del mundo sigue una distribución gaussiana), son difíciles de verificar, son muy inflexibles y difíciles de automatizar. En muchos dominios, los modelos basados en suposiciones suelen tener un peor rendimiento predictivo en datos de prueba intactos que los modelos de aprendizaje automático de caja negra. Esto solo es cierto para grandes conjuntos de datos, ya que los modelos interpretables con buenas suposiciones a menudo funcionan mejor con conjuntos de datos pequeños que los modelos de caja negra. El enfoque de aprendizaje automático de caja negra requiere una gran cantidad de datos para funcionar bien. Con la digitalización de todo, tendremos conjuntos de datos cada vez más grandes y, por lo tanto, el enfoque del aprendizaje automático se vuelve más atractivo. No hacemos suposiciones, aproximamos la realidad lo más cerca posible (al tiempo que evitamos el sobreajuste de los datos de entrenamiento). Sostengo que deberíamos desarrollar todas las herramientas que tenemos en estadística para responder preguntas (pruebas de hipótesis, medidas de correlación, medidas de interacción, herramientas de visualización, intervalos de confianza, valores p, intervalos de predicción, distribuciones de probabilidad) y reescribirlas para modelos de caja negra. En cierto modo, esto ya está sucediendo: Tomemos un modelo lineal clásico: el coeficiente de regresión estandarizado ya es una medida de importancia característica. Con la medida de importancia de la característica de permutación, tenemos una herramienta que funciona con cualquier modelo. En un modelo lineal, los coeficientes miden el efecto de una sola característica en el resultado previsto. La versión generalizada de esto es el gráfico de dependencia parcial. Prueba si A o B es mejor: Para esto también podemos usar funciones de dependencia parcial. Lo que aún no tenemos (según mi leal saber y entender) son pruebas estadísticas para modelos arbitrarios de caja negra. Los científicos de datos se automatizarán a sí mismos. Creo que los científicos de datos eventualmente se automatizarán para muchas tareas de análisis y predicción. Para que esto suceda, las tareas deben estar bien definidas y debe haber algunos procesos y rutinas a su alrededor. Hoy, faltan estas rutinas y procesos, pero los científicos de datos y colegas están trabajando en ellos. A medida que el aprendizaje automático se convierta en una parte integral de muchas industrias e instituciones, muchas de las tareas se automatizarán. Los robots y los programas se explicarán por sí mismos. Necesitamos interfaces más intuitivas para las máquinas y los programas que hacen un uso intensivo del aprendizaje automático. Algunos ejemplos: Un automóvil autónomo que informa por qué se detuvo abruptamente (70% de probabilidad de que un niño cruce la calle); Un programa de incumplimiento de crédito que explica a un empleado del banco por qué se rechazó una solicitud de crédito (El solicitante tiene demasiadas tarjetas de crédito y está empleado en un trabajo inestable); Un brazo robótico que explica por qué movió el artículo de la cinta transportadora al contenedor de basura (El artículo tiene una mancha en la parte inferior). La interpretabilidad podría impulsar la investigación de inteligencia artificial. Me imagino que al investigar más sobre cómo los programas y las máquinas pueden explicarse, podemos mejorar nuestra comprensión de la inteligencia y mejorar la creación de máquinas inteligentes. Al final, todas estas predicciones son especulaciones y tenemos que ver lo que realmente trae el futuro. ¡Forma tu propia opinión y sigue aprendiendo! "],["contribute.html", "Capítulo 9 Contribuir al libro", " Capítulo 9 Contribuir al libro Gracias por leer mi libro sobre Aprendizaje Automático Interpretable. El libro está en continuo desarrollo. Se mejorará con el tiempo y se agregarán más capítulos. Muy similar a cómo se desarrolla el software. Todo el texto y el código del libro es de código abierto y disponible en github.com. En la página de Github puede sugerir soluciones y problemas abiertos si encuentras un error o si falta algo. Si deseas ayudar aún más, la página de problemas también es el mejor lugar para encontrar problemas que solucionar y una buena manera de contribuir al libro. Si estás interesado en una contribución mayor sobre el contenido del libro, envíame un mensaje con tu idea concreta: christoph.molnar.ai@gmail.com. Si tienes sugerencias sobre la traducción, puedes escribirle al traductor a federico.fliguer@gmail.com. "],["cita.html", "Capítulo 10 Citando este libro", " Capítulo 10 Citando este libro Si este libro te resulta útil para tu publicación de blog, artículo de investigación o producto, te agradecería que lo citaras. Puedes citar el libro así: Molnar, Christoph. \"Interpretable machine learning. A Guide for Making Black Box Models Expplainable\", 2019. https://christophm.github.io/interpretable-ml-book/. O usa la siguiente entrada bibtex: `@book{molnar2019, title = {Aprendizaje automático interpretable}, autor = {Christoph Molnar}, note = {\\url{https://christophm.github.io/interpretable-ml-book/}}, year = {2019}, subtitle = {Guide for Making Black Box Models Explainable}}` Siempre tengo curiosidad sobre dónde y cómo se utilizan los métodos de interpretación en la industria y la investigación. Si usas el libro como referencia, sería genial que me escribieras una línea y me dijeras para qué. Por supuesto, esto es opcional y solo sirve para satisfacer mi propia curiosidad y estimular intercambios interesantes. Mi correo es christoph.molnar.ai@gmail.com. "],["traducciones.html", "Capítulo 11 Traducciones", " Capítulo 11 Traducciones ¿Interesado en traducir el libro? Este libro está licenciado bajo Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Esto significa que puedes traducirlo y ponerlo en línea. Debes mencionarme como autor original y no puedes vender el libro. Si estás interesado en traducir el libro, puedes escribir un mensaje y puedo vincular tu traducción aquí. Mi dirección es christoph.molnar.ai@gmail.com. Inglés (versión original): https://christophm.github.io/interpretable-ml-book/ Versión original por Christoph Molnar. Lista de traducciones Chino: https://blog.csdn.net/wizardforcel/article/details/98992150 Traducción de la mayoría de los capítulos, por CSDN, una comunidad en línea de programadores. https://zhuanlan.zhihu.com/p/63408696. Traducción de algunos capítulos por  . El sitio web también incluye preguntas y respuestas de varios usuarios. Si conoces alguna otra traducción del libro o de capítulos individuales, te agradecería saberlo y enumerarlo aquí. Puedes contactarme por correo electrónico: christoph.molnar.ai@gmail.com. "],["agradecimientos.html", "Capítulo 12 Agradecimientos", " Capítulo 12 Agradecimientos Escribir este libro fue (y sigue siendo) muy divertido. Pero también es mucho trabajo y estoy muy contento con el apoyo que recibí. Mi mayor agradecimiento a Katrin, que tuvo el trabajo más difícil en términos de horas y esfuerzo: ella corrigió el libro de principio a fin y descubrió muchos errores de ortografía e inconsistencias que nunca habría encontrado. Estoy muy agradecido por su apoyo. Muchas gracias a Verena Haunschmid por escribir la sección sobre Explicaciones LIME para imágenes. Ella trabaja en ciencia de datos y le recomiendo seguirla en Twitter: @ExpectAPatronum. ¡También quiero agradecer a todos los primeros lectores que contribuyeron con correcciones en Github! Además, quiero agradecer a todos los que crearon ilustraciones: La portada fue diseñada por mi amigo @YvonneDoinel. Los gráficos en el Capítulo de Valores de Shapley fueron creados por Abi Aryan, utilizando iconos creados por Freepik de Flaticon. La rana asombrosa con la bola de cristal en el capítulo sobre el Futuro de la Interpretabilidad fue diseñada por @TopeconHeroes. Verena Haunschmid creó el gráfico en el capítulo RuleFit. Me gustaría agradecer a todos los investigadores que me permitieron usar imágenes de sus artículos de investigación. En al menos tres aspectos, la forma en que publiqué este libro es poco convencional. Primero, está disponible como sitio web y como libro electrónico / pdf. El software que utilicé para crear este libro se llama bookdown, escrito por Yihui Xie, quien creó muchos paquetes R que facilitan la combinación de código R y texto. ¡Muchas gracias! En segundo lugar, este libro está publicado en la plataforma Leanpub, en lugar de trabajar con una editorial tradicional. Y tercero, publiqué el libro como libro en progreso, lo que me ha ayudado enormemente a obtener comentarios y monetizarlos en el camino. Muchas gracias a leanpub por hacer esto posible y manejar las regalías de manera justa. También me gustaría agradecerte, querido lector, por leer este libro sin una gran editorial detrás. Estoy agradecido por la financiación de mi investigación sobre aprendizaje automático interpretable por parte del Ministerio de Ciencia y Artes del Estado de Baviera en el marco del Centro de Digitalización de Baviera (ZD.B). "],["references.html", "References", " References Definición de algoritmo. https://www.merriam-webster.com/dictionary/algorithm. (2017) Aamodt, Agnar, and Enric Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. AI communications 7.1 (1994): 39-59. Alberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. Tubespam: comment spam filtering on YouTube. In Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 13843. IEEE. (2015). Alvarez-Melis, David, and Tommi S. Jaakkola. On the robustness of interpretability methods. arXiv preprint arXiv:1806.08049 (2018). Apley, Daniel W. Visualizing the effects of predictor variables in black box supervised learning models. arXiv preprint arXiv:1612.08468 (2016). Athalye, Anish, and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397 (2017). Bau, David, et al. Network dissection: Quantifying interpretability of deep visual representations. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. Biggio, Battista, and Fabio Roli. Wild Patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition 84 (2018): 317-331. Breiman, Leo. Random Forests. Machine Learning 45 (1). Springer: 5-32 (2001). Brown, Tom B., et al. Adversarial patch. arXiv preprint arXiv:1712.09665 (2017). Cohen, William W. Fast effective rule induction. Machine Learning Proceedings (1995). 115-123. Cook, R. Dennis. Detection of influential observation in linear regression. Technometrics 19.1 (1977): 15-18. Doshi-Velez, Finale, and Been Kim. Towards a rigorous science of interpretable machine learning, no. Ml: 113. http://arxiv.org/abs/1702.08608 ( 2017). Emilie Kaufmann and Shivaram Kalyanakrishnan. Information Complexity in Bandit Subset Selection. Proceedings of Machine Learning Research (2013). Fanaee-T, Hadi, and Joao Gama. Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence. Springer Berlin Heidelberg, 115. doi:10.1007/s13748-013-0040-3. (2013). Fernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. Transfer learning with partial observability applied to cervical cancer screening. In Iberian Conference on Pattern Recognition and Image Analysis, 24350. Springer. (2017). Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. Model Class Reliance: Variable importance measures for any machine learning model class, from the Rashomon perspective. http://arxiv.org/abs/1801.01489 (2018). Fokkema, Marjolein, and Benjamin Christoffersen. Pre: Prediction rule ensembles. https://CRAN.R-project.org/package=pre (2017). Friedman, Jerome H, and Bogdan E Popescu. Predictive learning via rule ensembles. The Annals of Applied Statistics. JSTOR, 91654. (2008). Friedman, Jerome H. Greedy function approximation: A gradient boosting machine. Annals of statistics (2001): 1189-1232. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009). Fürnkranz, Johannes, Dragan Gamberger, and Nada Lavra. Foundations of rule learning. Springer Science &amp; Business Media, (2012). Goldstein, Alex, et al. Package ICEbox. (2017). Goldstein, Alex, et al. Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics 24.1 (2015): 44-65. Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014). Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. A simple and effective model-based variable importance measure. arXiv preprint arXiv:1805.04755 (2018). Heider, Fritz, and Marianne Simmel. An experimental study of apparent behavior. The American Journal of Psychology 57 (2). JSTOR: 24359. (1944). Holte, Robert C. Very simple classification rules perform well on most commonly used datasets. Machine learning 11.1 (1993): 63-90. Hooker, Giles. Discovering additive structure in black box functions. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004). Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. Feature relevance quantification in explainable AI: A causal problem. International Conference on Artificial Intelligence and Statistics. PMLR, 2020. Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. Feature relevance quantification in explainable AI: A causality problem. arXiv preprint arXiv:1910.13413 (2019). Kahneman, Daniel, and Amos Tversky. The Simulation Heuristic. Stanford Univ CA Dept of Psychology. (1981). Kaufman, Leonard, and Peter Rousseeuw. Clustering by means of medoids. North-Holland (1987). Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. Examples are not enough, learn to criticize! Criticism for interpretability. Advances in Neural Information Processing Systems (2016). Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. Examples are not enough, learn to criticize! Criticism for interpretability. Advances in Neural Information Processing Systems (2016). Kim, Been, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). arXiv preprint arXiv:1711.11279 (2017). Koh, Pang Wei, and Percy Liang. Understanding black-box predictions via influence functions. arXiv preprint arXiv:1703.04730 (2017). Laugel, Thibault, et al. Inverse classification for comparison-based interpretability in machine learning. arXiv preprint arXiv:1712.08443 (2017). Letham, Benjamin, et al. Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics 9.3 (2015): 1350-1371. Lipton, Peter. Contrastive explanation. Royal Institute of Philosophy Supplements 27 (1990): 247-266. Lipton, Zachary C. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, (2016). Lundberg, Scott M., and Su-In Lee. A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems. 2017. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. Anchors: High-Precision Model-Agnostic Explanations. AAAI Conference on Artificial Intelligence (AAAI), 2018 Martens, David, and Foster Provost. Explaining data-driven document classifications. (2014). Miller, Tim. Explanation in artificial intelligence: Insights from the social sciences. arXiv Preprint arXiv:1706.07269. (2017). Nguyen, Anh, et al. Plug &amp; play generative networks: Conditional iterative generation of images in latent space. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. Nguyen, Anh, et al. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in Neural Information Processing Systems. 2016. Nickerson, Raymond S. Confirmation Bias: A ubiquitous phenomenon in many guises. Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998). Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015 Papernot, Nicolas, et al. Practical black-box attacks against machine learning. Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations. AAAI Conference on Artificial Intelligence (2018). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability of machine learning. ICML Workshop on Human Interpretability in Machine Learning. (2016). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. Why should I trust you?: Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016). Shapley, Lloyd S. A value for n-person games. Contributions to the Theory of Games 2.28 (1953): 307-317. Staniak, Mateusz, and Przemyslaw Biecek. Explanations of model predictions with live and breakDown packages. arXiv preprint arXiv:1804.01955 (2018). Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural networks. IEEE Transactions on Evolutionary Computation (2019). Sundararajan, Mukund, and Amir Najmi. The many Shapley values for model explanation. arXiv preprint arXiv:1908.08474 (2019). Sundararajan, Mukund, and Amir Najmi. The many Shapley values for model explanation. arXiv preprint arXiv:1908.08474 (2019). Szegedy, Christian, et al. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013). Van Looveren, Arnaud, and Janis Klaise. Interpretable Counterfactual Explanations Guided by Prototypes. arXiv preprint arXiv:1907.02584 (2019). Wachter, Sandra, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. (2017). Yang, Hongyu, Cynthia Rudin, and Margo Seltzer. Scalable Bayesian rule lists. Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. Zhao, Qingyuan, and Trevor Hastie. Causal interpretations of black-box models. Journal of Business &amp; Economic Statistics, to appear. (2017). trumbelj, Erik, and Igor Kononenko. A general method for visualizing and explaining black-box regression models. In International Conference on Adaptive and Natural Computing Algorithms, 2130. Springer. (2011). trumbelj, Erik, and Igor Kononenko. Explaining prediction models and individual predictions with feature contributions. Knowledge and information systems 41.3 (2014): 647-665. "],["r-packages-used-for-examples.html", "R Packages Used for Examples", " R Packages Used for Examples base. R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. data.table. Matt Dowle and Arun Srinivasan (2020). data.table: Extension of data.frame. R package version 1.13.2. https://CRAN.R-project.org/package=data.table dplyr. Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2020). dplyr: A Grammar of Data Manipulation. R package version 1.0.2. https://CRAN.R-project.org/package=dplyr ggplot2. Hadley Wickham, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani and Dewey Dunnington (2020). ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.3.2. https://CRAN.R-project.org/package=ggplot2 iml. Christoph Molnar and Patrick Schratz (2020). iml: Interpretable Machine Learning. R package version 0.10.1. https://CRAN.R-project.org/package=iml knitr. Yihui Xie (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30. https://CRAN.R-project.org/package=knitr libcoin. Torsten Hothorn (2020). libcoin: Linear Test Statistics for Permutation Inference. R package version 1.0-6. https://CRAN.R-project.org/package=libcoin memoise. Hadley Wickham, Jim Hester, Kirill Müller and Daniel Cook (2017). memoise: Memoisation of Functions. R package version 1.1.0. https://CRAN.R-project.org/package=memoise mlr. Bernd Bischl, Michel Lang, Lars Kotthoff, Patrick Schratz, Julia Schiffner, Jakob Richter, Zachary Jones, Giuseppe Casalicchio and Mason Gallo (2020). mlr: Machine Learning in R. R package version 2.18.0. https://CRAN.R-project.org/package=mlr mvtnorm. Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi and Torsten Hothorn (2020). mvtnorm: Multivariate Normal and t Distributions. R package version 1.1-1. https://CRAN.R-project.org/package=mvtnorm NLP. Kurt Hornik (2020). NLP: Natural Language Processing Infrastructure. R package version 0.2-1. https://CRAN.R-project.org/package=NLP ParamHelpers. Bernd Bischl, Michel Lang, Jakob Richter, Jakob Bossek, Daniel Horn and Pascal Kerschke (2020). ParamHelpers: Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning. R package version 1.14. https://CRAN.R-project.org/package=ParamHelpers partykit. Torsten Hothorn and Achim Zeileis (2020). partykit: A Toolkit for Recursive Partytioning. R package version 1.2-10. https://CRAN.R-project.org/package=partykit pre. Marjolein Fokkema and Benjamin Christoffersen (2020). pre: Prediction Rule Ensembles. R package version 1.0.0. https://CRAN.R-project.org/package=pre readr. Hadley Wickham and Jim Hester (2020). readr: Read Rectangular Text Data. R package version 1.4.0. https://CRAN.R-project.org/package=readr rpart. Terry Therneau and Beth Atkinson (2019). rpart: Recursive Partitioning and Regression Trees. R package version 4.1-15. https://CRAN.R-project.org/package=rpart tidyr. Hadley Wickham (2020). tidyr: Tidy Messy Data. R package version 1.1.2. https://CRAN.R-project.org/package=tidyr tm. Ingo Feinerer and Kurt Hornik (2020). tm: Text Mining Package. R package version 0.7-8. https://CRAN.R-project.org/package=tm viridis. Simon Garnier (2018). viridis: Default Color Maps from matplotlib. R package version 0.5.1. https://CRAN.R-project.org/package=viridis viridisLite. Simon Garnier (2018). viridisLite: Default Color Maps from matplotlib (Lite Version). R package version 0.3.0. https://CRAN.R-project.org/package=viridisLite "]]
