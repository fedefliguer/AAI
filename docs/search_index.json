[
["interpretable-ml.html", "Aprendizaje automatico interpretable Una guia para hacer que los modelos de caja negra sean explicables. Prefacio Chapter 1 Introducción 1.1 Hora del cuento 1.2 ¿Qué es el aprendizaje automático? 1.3 Terminología Chapter 2 Interpretabilidad 2.1 Importancia de la interpretabilidad 2.2 Taxonomía de los métodos de interpretación 2.3 Alcance de la interpretabilidad 2.4 Evaluación de la interpretabilidad 2.5 Propiedades de las explicaciones 2.6 Explicaciones amigables para los humanos Chapter 3 Conjuntos de datos 3.1 Alquiler de bicicletas (Regresión) 3.2 Comentarios de spam de YouTube (clasificación de texto) 3.3 Factores de riesgo para el cáncer de cuello uterino (Clasificación)", " Aprendizaje automatico interpretable Una guia para hacer que los modelos de caja negra sean explicables. Christoph Molnar 2020-02-21 ﻿{r setup, cache=FALSE, include=FALSE} devtools::load_all() is.html = !is.null(output) &amp;&amp; output == &quot;html&quot; only.in.html = &quot;*This chapter is currently only available in this web version. ebook and print will follow.*&quot; devtools::install_github(&quot;viadee/anchorsOnR&quot;) Prefacio knitr::include_graphics(&#39;images/title_page.jpg&#39;, dpi = NA) El aprendizaje automático tiene un gran potencial para mejorar productos, procesos e investigación. Pero las computadoras generalmente no explican sus predicciones, lo cual es una barrera para la adopción del aprendizaje automático. Este libro trata de hacer que los modelos de aprendizaje automático y sus decisiones sean interpretables. Después de explorar los conceptos de interpretabilidad, aprenderás sobre modelos simples e interpretables como árboles de decisión, reglas de decisión y regresión lineal. Los capítulos posteriores se centran en métodos generales independientes del modelo para interpretar modelos de caja negra como la importancia de la característica y los efectos locales acumulados, y explicar las predicciones individuales con valores de Shapley y LIME. Todos los métodos de interpretación se explican en profundidad y se analizan críticamente. ¿Cómo funcionan detrás de escena? ¿Cuales son sus fortalezas y debilidades? ¿Cómo se pueden interpretar sus resultados? Este libro te permitirá seleccionar y aplicar correctamente el método de interpretación más adecuado para su proyecto de aprendizaje automático. El libro se enfoca en modelos de aprendizaje automático para datos tabulares (también llamados datos relacionales o estructurados) y no se enfoca en visión artifical ni en procesamiento de lenguaje natural. Se recomienda leer el libro para profesionales del aprendizaje automático, científicos de datos, estadísticos y cualquier otra persona interesada en hacer que los modelos de aprendizaje automático sean interpretables. Podés comprar la versión en PDF y en e-book (epub, mobi) del libro en leanpub.com. Podés comprar la versión en papel en lulu.com. Sobre el autor: Mi nombre es Christoph Molnar, Soy estadístico y un profesional del aprendizaje automático. Mi objetivo es hacer al aprendizaje automático interpretable. Mail: christoph.molnar.ai@gmail.com Website: https://christophm.github.io/ Seguime en Twitter! @ChristophMolnar Portada por @YvonneDoinel Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ﻿ Chapter 1 Introducción Este libro te explica cómo hacer que los modelos de aprendizaje automático (supervisados) sean interpretables. Los capítulos contienen algunas fórmulas matemáticas, pero deberías poder comprender las ideas detrás de los métodos, incluso sin las fórmulas. Este libro no es para personas que intentan aprender el aprendizaje automático desde cero. Si sos nuevo en el aprendizaje automático, hay muchos libros y otros recursos para aprender los conceptos básicos. Recomiendo el libro “Los elementos del aprendizaje estadístico” de Hastie, Tibshirani y Friedman (2009) 1 y el curso en línea “Machine Learning” de Andrew Ng en la plataforma de aprendizaje en línea coursera.com para comenzar con el aprendizaje automático. ¡Tanto el libro como el curso están disponibles de forma gratuita! Se publican nuevos métodos para la interpretación de modelos de aprendizaje automático a una velocidad vertiginosa. Mantenerse al día con todo lo que se publica sería una locura y simplemente imposible. Es por eso que no encontrarás los métodos más novedosos y sofisticados en este libro, sino los métodos establecidos y los conceptos básicos de la capacidad de interpretación del aprendizaje automático. Estos conceptos básicos te preparan para hacer que los modelos de aprendizaje automático sean interpretables. La internalización de los conceptos básicos, además, te permitirá comprender y evaluar mejor cualquier documento nuevo sobre interpretabilidad publicado en arxiv.org en los últimos 5 minutos desde que comenzaste a leer este libro (podría estar exagerando la tasa de publicación). Este libro comienza con algunas historias cortas (distópicas) que no son necesarias para entender el libro, pero con suerte te entretendrán y te harán pensar. Luego, el libro explora los conceptos de interpretabilidad del aprendizaje automático. Discutiremos cuándo la interpretabilidad es importante y qué diferentes tipos de explicaciones hay. Los términos utilizados a lo largo de todo el libro se pueden consultar en el Capítulo de terminología. La mayoría de los modelos y métodos explicados se presentan utilizando ejemplos de datos reales que se describen en el Capítulo de conuntos de datos. Una forma de hacer que el aprendizaje automático sea interpretable es usar modelos interpretables, como modelos lineales o árboles de decisión. La otra opción es el uso de herramientas de interpretación modelo-agnósticas que se pueden aplicar a cualquier modelo supervisado de aprendizaje automático. El capítulo Métodos modelo-agnósticos trata con métodos tales como gráficas de dependencia parcial (PDP) e importancia de la característica de permutación. Los métodos modelo-agnósticos funcionan cambiando la entrada del modelo de aprendizaje automático y midiendo los cambios en la salida de predicción. Los métodos independientes al modelo que devuelven observaciones como explicaciones se analizan en el capítulo Explicaciones basadas en ejemplos. Los métodos independientes al modelo se pueden diferenciar aún más, en función de si explican el comportamiento global del modelo en todas las observaciones o si explican predicciones individuales. Los siguientes métodos explican el comportamiento general del modelo: Gráficos de dependencia parcial, Efectos locales acumulados, Interacción de características, Importancia de características , Modelos sustitutos globales y Prototipos y críticas. Para explicar las predicciones individuales, en cambio, tenemos Modelos sustitutos locales, Explicaciones del valor de Shapley, Explicaciones contrafactuales (y estrechamente relacionados: Ejemplos adversos). Algunos métodos se pueden usar para explicar ambos aspectos del comportamiento del modelo, tanto el carácter global como las predicciones individuales: Expectativa condicional individual e Instancias influyentes. El libro termina con una perspectiva optimista sobre cómo podría ser el futuro del aprendizaje automático interpretable. Podés leer el libro de principio a fin o saltar directamente a los métodos que le interesen. ¡Espero que disfrutes la lectura! ﻿ 1.1 Hora del cuento Comenzaremos con algunas historias cortas. Cada historia es una llamada algo exagerada para el aprendizaje automático interpretable. Si tenés prisa, puede saltear las historias. Si querés entretenerte y (des)motivarte, ¡sigue leyendo! El formato está inspirado en los cuentos técnicos de Jack Clark en su Boletín informativo Importación AI. Si te gustan este tipo de historias o si estás interesado en la inteligencia artificial, te recomiendo que te registres. Un rayo nunca golpea dos veces 2030: un laboratorio médico en Suiza “¡Definitivamente no es la peor forma de morir!” Tom resumió, tratando de encontrar algo positivo en la tragedia. Sacó la bomba del poste intravenoso. “Simplemente murió por las razones equivocadas”, agregó Lena. “¡Y ciertamente con la bomba de morfina equivocada! ¡Solo estamos creando más trabajo para nosotros!”, Se quejó Tom mientras desenroscaba la placa posterior de la bomba. Después de quitar todos los tornillos, levantó la placa y la dejó a un lado. Conectó un cable al puerto de diagnóstico. “No solo te quejaste de tener un trabajo, ¿verdad?” Lena le dedicó una sonrisa burlona. “Por supuesto que no. ¡Nunca!” exclamó con un tono sarcástico. Arrancó la computadora de la bomba. Lena enchufó el otro extremo del cable a su tableta. “Muy bien, los diagnósticos se están ejecutando”, anunció. “Tengo mucha curiosidad por saber qué salió mal”. “Ciertamente le disparó a nuestro John Doe al Nirvana. Fue la alta concentración de este material de morfina. Hombre. Quiero decir, normalmente, una bomba rota emite muy poco o nada en absoluto. Pero nunca, ya sabes, algo así”, explicó Tom. “Lo sé. No tienes que convencerme … Oye, mira eso”. Lena levantó su tableta. “¿Ves este pico aquí? Esa es la potencia de la mezcla de analgésicos. ¡Mira! Esta línea muestra el nivel de referencia. El pobre tipo tenía una mezcla de analgésicos en su sistema sanguíneo que podría matarlo 17 veces. Inyectado por nuestra bomba aquí. Y aquí …”se deslizó,&quot; aquí puedes ver el momento de la muerte del paciente“.”Entonces, ¿alguna idea de lo que pasó, jefe?&quot; Tom le preguntó a su supervisor. “Hmm… Los sensores parecen estar bien. Frecuencia cardíaca, niveles de oxígeno, glucosa, … Los datos se recopilaron como se esperaba. Algunos valores faltantes en los datos de oxígeno en la sangre, pero eso no es inusual. Mira aquí. Los sensores también han detectado la frecuencia cardíaca lenta del paciente y los niveles extremadamente bajos de cortisol causados por el derivado de la morfina y otros agentes bloqueadores del dolor”. Ella continuó pasando el informe de diagnóstico. Tom miraba cautivado la pantalla. Fue su primera investigación de una falla real del dispositivo. “Ok, aquí está nuestra primera pieza del rompecabezas. El sistema no pudo enviar una advertencia al canal de comunicación del hospital. La advertencia se activó, pero se rechazó a nivel de protocolo. Podría ser culpa nuestra, pero también podría ser culpa del hospital. Envíe los registros al equipo de IT”, le dijo Lena a Tom. Tom asintió con los ojos todavía fijos en la pantalla. Lena continuó: “Es extraño. La advertencia también debería haber causado que la bomba se apagara. Pero obviamente no lo hizo. Eso debe ser un error. Algo que control de calidad perdió. Algo realmente malo. Tal vez esté relacionado con el problema del protocolo”. “Entonces, el sistema de emergencia de la bomba de alguna manera se averió, pero ¿por qué la bomba se llenó de bananas e inyectó tanto analgésico en John Doe?” Tom se preguntó. “Buena pregunta. Tienes razón. Dejando de lado la falla de emergencia del protocolo, la bomba no debería haber administrado esa cantidad de medicamento. El algoritmo debería haberse detenido por sí solo mucho antes, dado el bajo nivel de cortisol y otras señales de advertencia”, explicó Lena. “¿Quizás algo de mala suerte, como una cosa entre un millón, como ser alcanzado por un rayo?” Tom le preguntó. “No, Tom. Si hubiera leído la documentación que le envié, habría sabido que la bomba se entrenó primero en experimentos con animales, luego en humanos, para aprender a inyectar la cantidad perfecta de analgésicos en función de la información sensorial. El algoritmo de la bomba puede ser opaco y complejo, pero no es aleatorio. Eso significa que en la misma situación la bomba se comportaría exactamente de la misma manera nuevamente. Nuestro paciente moriría de nuevo. Una combinación o interacción no deseada de las entradas sensoriales debe haber desencadenado el comportamiento erróneo de la bomba. Es por eso que tenemos que profundizar más y descubrir qué sucedió aquí”, explicó Lena. “Ya veo …”, respondió Tom, perdido en sus pensamientos. “¿No iba a morir el paciente pronto de todos modos? ¿Por cáncer o algo así?” Lena asintió mientras leía el informe del análisis. Tom se levantó y fue a la ventana. Miró hacia afuera, con los ojos fijos en un punto a la distancia. “Tal vez la máquina le hizo un favor, ya sabes, al liberarlo del dolor. No más sufrimiento. Tal vez simplemente hizo lo correcto. Como un rayo, pero, ya sabes, uno bueno. Me refiero a la lotería, pero no al azar. Pero por una razón. Si yo fuera la bomba, habría hecho lo mismo”. Finalmente levantó la cabeza y lo miró. Seguía mirando algo afuera. Ambos guardaron silencio por unos momentos. Lena volvió a bajar la cabeza y continuó el análisis. “No, Tom. Es un error … Solo un maldito error”. Perder confianza 2050: una estación de metro en Singapur Se apresuró a la estación Bishan. Sus pensamientos ya estaban en el trabajo. Las pruebas para la nueva arquitectura neuronal deberían completarse por ahora. Ella dirigió el rediseño del “Sistema fiscal de predicción de afinidad para entidades individuales” del gobierno, que predice si una persona esconderá dinero que debe pagar en impuestos. Su equipo ha creado una elegante pieza de ingeniería. Si tiene éxito, el sistema no solo serviría a la oficina de impuestos, sino que también se incorporaría a otros sistemas, como el sistema de alarma contra el terrorismo y el registro comercial. Un día, el gobierno podría incluso integrar las predicciones en el Civic Trust Score. El Civic Trust Score estima cuán confiable es una persona. La estimación afecta cada parte de su vida diaria, como obtener un préstamo o cuánto tiempo tiene que esperar para obtener un nuevo pasaporte. Mientras bajaba la escalera mecánica, se imaginó cómo se vería una integración del sistema de su equipo en el sistema de puntuación de confianza cívica. Rutinariamente pasaba la mano por el lector sin reducir su velocidad de marcha. Su mente estaba ocupada, hasta que sonaron las alarmas en su cerebro. Demasiado tarde. Con la nariz primero, corrió hacia la puerta de entrada del metro y cayó con el trasero al suelo. Se suponía que la puerta se abría … pero no fue así. Atónita, se levantó y miró la pantalla junto a la puerta. “Por favor, inténtalo en otro momento”, sugirió una carita sonriente en la pantalla. Una persona pasó y, ignorándola, pasó la mano sobre el lector. La puerta se abrió y él entró. La puerta se cerró de nuevo. Se limpió la nariz. Le dolía, pero al menos no sangraba. Ella trató de abrir la puerta, pero fue rechazada nuevamente. Fue extraño Tal vez su cuenta de transporte público no tenía suficiente crédito. Miró su reloj inteligente para verificar el saldo de la cuenta. &quot;Inicio de sesión denegado. ¡Comuníquese con su Oficina de asesoramiento para ciudadanos! su reloj le informó. Una sensación de náuseas la golpeó como un puño en el estómago. Ella sospechaba lo que había sucedido. Para confirmar su teoría, abrió “Sniper Guild”, un juego de disparos en primera persona. La aplicación se cerró de nuevo automáticamente, lo que confirmó su teoría. Se mareó y volvió a sentarse en el suelo. Solo había una explicación posible: Su puntuación de confianza cívica había bajado. Sustancialmente. Una pequeña caída significaba inconvenientes menores, como no obtener vuelos de primera clase o tener que esperar un poco más para obtener los documentos oficiales. Un puntaje de confianza bajo era raro y quien lo tenía estaba clasificado como una amenaza para la sociedad: Una medida para tratar con estas personas era mantenerlas alejadas de lugares públicos como el metro. El gobierno, además, restringió las transacciones financieras de sujetos con bajos puntajes de confianza cívica. También comenzaron a monitorear activamente su comportamiento en las redes sociales e incluso llegaron a restringir cierto contenido, como los juegos violentos. Se había vuelto más difícil aumentar el puntaje de confianza cívica cuanto más bajo era éste. Las personas con un puntaje muy bajo generalmente nunca se recuperaban. No podía pensar en ninguna razón por la cual su puntuación debería haber caído. La puntuación se basó en el aprendizaje automático. El Civic Trust Score System funcionó como un motor bien engrasado que dirigía la sociedad. El rendimiento del sistema de puntuación de confianza siempre se supervisó de cerca. El aprendizaje automático había mejorado mucho desde principios de siglo. Se había vuelto tan eficiente que las decisiones tomadas por el Trust Score System ya no podían ser cuestionadas. Un sistema infalible. Rió desesperada. Sistema infalible. El sistema rara vez había fallado. Pero falló. Ella debe ser uno de esos casos especiales; un error del sistema; a partir de ahora un paria. Nadie se atrevió a cuestionar el sistema. Estaba demasiado integrado en el gobierno, en la sociedad misma, para ser cuestionado. En los pocos países democráticos restantes, estaba prohibido formar movimientos antidemocráticos, no porque fueran inherentemente maliciosos, sino porque desestabilizarían el sistema actual. La misma lógica se aplica a las ahora algocracias. La crítica en los algoritmos estaba prohibida debido al peligro para el status quo. La confianza algorítmica era el tejido del orden social. Por el bien común, se aceptaron tácitamente raras puntuaciones falsas de confianza. Cientos de otros sistemas de predicción y bases de datos ingresaron al puntaje, lo que hace imposible saber qué causó la caída en su puntaje. Sintió que un gran agujero oscuro se abría dentro y debajo de ella. Con horror, miró al vacío. Su sistema de afinidad fiscal finalmente se integró en el Sistema de puntuación de confianza cívica, pero nunca llegó a saberlo. Clips de papel de Fermi Año 612 AMS (después del asentamiento en Marte): un museo en Marte “La historia es aburrida”, le susurró Xola a su amiga. Xola, una chica de cabello azul, perseguía uno de los drones del proyector que zumbaba en la habitación con la mano izquierda. “La historia es importante”, dijo la maestra con voz molesta, mirando a las chicas. Xola se sonrojó. No esperaba que su maestra la escuchara. “Xola, ¿qué acabas de aprender?” la maestra le preguntó. “¿Que la gente antigua usó todos los recursos del Planeta Terrestre y luego murió?” ella preguntó cuidadosamente. “No. Calentaron el clima y no fueron las personas, fueron las computadoras y las máquinas. Y es el planeta Tierra, no el planeta Terrestre”, agregó otra chica llamada Lin. Xola asintió de acuerdo. Con un toque de orgullo, la maestra sonrió y asintió. “Ambos tienen razón. ¿Sabes por qué sucedió?” “¿Porque la gente era miope y codiciosa?” Xola preguntó. “¡La gente no podía parar sus máquinas!” Espetó Lin. “Una vez más, ambos tienen razón”, decidió la maestra, “Pero es mucho más complicado que eso. La mayoría de las personas en ese momento no sabían lo que estaba sucediendo. Algunos vieron los cambios drásticos, pero no pudieron revertirlos. La pieza más famosa de este período es un poema de un autor anónimo. Captura mejor lo que sucedió en ese momento. ¡Escucha cuidadosamente!” La maestra comenzó el poema. Una docena de pequeños drones se reposicionaron frente a los niños y comenzaron a proyectar el video directamente en sus ojos. Mostraba a una persona en un traje de pie en un bosque con solo tocones de árboles. La persona comenzó a hablar: Las máquinas computan; las máquinas predicen. Somos parte de esto. Perseguimos un óptimo entrenado. Lo óptimo es unidimensional, local y sin restricciones. Silicio y carne, persiguiendo la exponencialidad. El crecimiento es nuestra mentalidad. Cuando todas las recompensas sean recogidas, y sus efectos secundarios descuidados; Cuando se extraigan todas las monedas, y la naturaleza haya quedado atrás; Estaremos en problemas, Después de todo, el crecimiento exponencial es una burbuja. La tragedia del desarrollo de los comunes, Explotando, Ante nuestros ojos. Cálculos fríos y avaricia helada, Llena la tierra de calor. Todo se está muriendo, Y estamos cumpliendo. Al igual que los caballos con anteojeras, corremos la carrera de nuestra propia creación, Hacia el gran filtro de la civilización. Y así marchamos sin descanso. Como somos parte de la máquina. Abrazando la entropía. “Un recuerdo oscuro”, dijo la maestra para romper el silencio en la sala. “Se cargará en su biblioteca. Tu tarea es memorizarla hasta la próxima semana.” Xola suspiró. Ella logró atrapar uno de los pequeños drones. El dron estaba caliente por la CPU y los motores. A Xola le gustó cómo le calentó las manos. ﻿ 1.2 ¿Qué es el aprendizaje automático? El aprendizaje automático es un conjunto de métodos que usan las computadoras para hacer y mejorar predicciones o comportamientos basados en datos. Por ejemplo, para predecir el valor de una casa, la computadora puede aprender patrones de ventas pasadas de casas. El libro se centra en el aprendizaje automático supervisado, que cubre todos los problemas de predicción en los que tenemos un conjunto de datos para el que ya conocemos el resultado de interés (por ejemplo, precios anteriores de la vivienda) y queremos predecir el resultado de los nuevos datos. Se excluyen del aprendizaje supervisado, por ejemplo, las tareas de agrupación (aprendizaje no supervisado) donde no tenemos un resultado específico de interés, pero queremos encontrar grupos de observaciones. También se excluyen cosas como el aprendizaje por refuerzo, donde un agente aprende a optimizar cierta recompensa actuando en un entorno (por ejemplo, una computadora que juega Tetris). El objetivo del aprendizaje supervisado es aprender un modelo predictivo que relacione características de los datos (por ejemplo, tamaño de la casa, ubicación, tipo de piso, …) con una salida (por ejemplo, el precio de la casa). Si el resultado es categórico, el objetivo se llama clasificación, y si es numérico, se llama regresión. El algoritmo de aprendizaje automático aprende un modelo mediante la estimación de parámetros (como pesos) o estructuras de aprendizaje (como árboles). El algoritmo se guía por una función de puntuación o pérdida que se minimiza. En el ejemplo del valor de la vivienda, la máquina minimiza la diferencia entre el precio estimado de la vivienda y el precio previsto. Un modelo de aprendizaje automático totalmente entrenado se puede utilizar para hacer predicciones para nuevas instancias. Estimación de precios de la vivienda, recomendaciones de productos, detección de letreros, predicción de incumplimiento crediticio y detección de fraude: Todos estos ejemplos tienen en común que pueden resolverse mediante el aprendizaje automático. Las tareas son diferentes, pero el enfoque es el mismo: Paso 1: recopilación de datos. Mientras más, mejor. Los datos deben contener el resultado que desea predecir e información adicional a partir de la cual realizar la predicción. Para un detector de letrero de calle (“¿Hay un letrero de calle en la imagen?”), debes recopilar imágenes de la calle y etiquetar si un letrero de calle es visible o no. Para un predictor de incumplimiento de crédito, necesitas datos pasados sobre préstamos reales, información sobre si los clientes estaban en incumplimiento con sus préstamos y datos que lo ayudarán a hacer predicciones, como ingresos, incumplimientos de créditos pasados, etc. Para un programa de estimación automática del valor de la vivienda, podés recopilar datos de ventas de viviendas anteriores e información sobre los bienes inmuebles, como el tamaño, la ubicación, etc. Paso 2: ingreso de esta información en un algoritmo de aprendizaje automático que genera un modelo de detector de signos, un modelo de calificación crediticia o un estimador del valor de la vivienda. Paso 3: uso del modelo con nuevos datos. Integrar el modelo en un producto o proceso, como un automóvil sin conductor, un proceso de solicitud de crédito o un sitio web del mercado inmobiliario. Las máquinas superan a los humanos en muchas tareas, como jugar al ajedrez (o más recientemente Go) o predecir el clima. Incluso si la máquina es tan buena como un ser humano o un poco peor en una tarea, sigue habiendo grandes ventajas en términos de velocidad, reproducibilidad y escala. Una vez implementado, un modelo de aprendizaje automático puede completar una tarea mucho más rápido que los humanos, ofrece resultados consistentes y se puede copiar infinitamente. La replicación de un modelo de aprendizaje automático en otra máquina es rápida y barata. El entrenamiento de un humano para una tarea puede llevar décadas (especialmente cuando son jóvenes) y es muy costoso. Una desventaja importante del uso del aprendizaje automático es que los conocimientos sobre los datos y la tarea que resuelve la máquina están ocultos en modelos cada vez más complejos. Necesita millones de números para describir una red neuronal profunda, y no hay forma de entender el modelo en su totalidad. Otros modelos, como el random forest, consisten en cientos de árboles de decisión que “votan” por predicciones. Para comprender cómo se tomó la decisión, debería examinar los votos y las estructuras de cada uno de los cientos de árboles. Eso simplemente no funciona, no importa cuán inteligente sea o cuán buena sea su memoria de trabajo. Los modelos con mejor rendimiento son a menudo mezclas de varios modelos (también llamados conjuntos) imposibles de interpretar, aún bajo la posibilidad de que cada modelo se pudiera interpretar. Si te enfocas solo en el rendimiento, obtendrás automáticamente modelos cada vez más opacos. Solo echa un vistazo a entrevistas con ganadores en la plataforma de competencia de aprendizaje automático kaggle.com: Los modelos ganadores eran en su mayoría conjuntos de modelos o modelos muy complejos, como árboles potenciados o redes neuronales profundas. 1.3 Terminología Para evitar confusiones debido a la ambigüedad, aquí hay algunas definiciones de los términos utilizados en este libro: Un Algoritmo es un conjunto de reglas que una máquina sigue para lograr un objetivo particular2. Un algoritmo puede considerarse como una receta que define las entradas, la salida y todos los pasos necesarios para pasar de las entradas a la salida. Las recetas de cocción son algoritmos en los que los ingredientes son las entradas, la comida cocida es la salida y los pasos de preparación y cocción son las instrucciones del algoritmo. Aprendizaje automático es un conjunto de métodos que permiten a las computadoras aprender de los datos para hacer y mejorar predicciones (por ejemplo, cáncer, ventas semanales, incumplimiento de crédito). El aprendizaje automático es un cambio de paradigma de la “programación normal”, donde todas las instrucciones se deben dar explícitamente a la computadora a la “programación indirecta” que se realiza mediante el suministro de datos. Un Aprendiz o Algoritmo de aprendizaje automático es el programa utilizado para aprender un modelo de aprendizaje automático a partir de datos. Otro nombre es “inductor” (por ejemplo, “inductor de árbol”). Un Modelo de aprendizaje automático es el programa aprendido que asigna entradas a predicciones. Esto puede ser un conjunto de pesos para un modelo lineal o para una red neuronal. Otros nombres para la palabra bastante inespecífica “modelo” son “predictor” o, según la tarea, “clasificador” o “modelo de regresión”. En las fórmulas, el modelo de aprendizaje automático entrenado se llama \\(\\hat{f}\\) o \\(\\hat{f}(x)\\). FIGURE 1.1: A learner learns a model from labeled training data. The model is used to make predictions. Un Modelo de caja negra es un sistema que no revela sus mecanismos internos. En el aprendizaje automático, la “caja negra” describe modelos que no se pueden entender al observar sus parámetros (por ejemplo, una red neuronal). El opuesto de una caja negra a veces se denomina caja blanca, y es llamada en este libro como modelo interpretable. Los métodos modelo-agnósticos para la interpretabilidad tratan los modelos de aprendizaje automático como cajas negras, incluso si no lo son. Aprendizaje automático interpretable se refiere a métodos y modelos que hacen que el comportamiento y las predicciones de los sistemas de aprendizaje automático sean comprensibles para los humanos. Un conjunto de datos es una tabla con los datos de los cuales la máquina aprende. El conjunto de datos contiene las características y el objetivo a predecir. Cuando se usa para el aprendizaje de un modelo, el conjunto de datos se denomina datos de entrenamiento. Una observación es una fila en el conjunto de datos. Otros nombres para ‘observación’ son: punto (datos), ejemplo, instancia. Una instancia consta de los valores de característica \\(x^{(i)}\\) y, si se conoce, el resultado objetivo \\(y_i\\). Las características son las entradas utilizadas para la predicción o clasificación. Una característica es una columna en el conjunto de datos. A lo largo del libro, se supone que las características son interpretables, lo que significa que es fácil entender lo que significan, como la temperatura en un día determinado o la altura de una persona. La interpretabilidad de las características es una gran suposición, pero si es difícil entender las características de entrada, es aún más difícil entender lo que hace el modelo. La matriz con todas las características se llama X y \\(x^{(i)}\\) para instancia individual. El vector de una sola característica para todas las instancias es \\(x_j\\) y el valor para la característica j y la instancia i es \\(x^{(i)}_j\\). El Objetivo (o target) es la columna que la máquina aprende a predecir. En las fórmulas matemáticas, el objetivo generalmente se llama y o \\(y_i\\) para una sola instancia. Una Tarea de aprendizaje automático es la combinación de un conjunto de datos con características y un objetivo. Dependiendo del tipo de objetivo, la tarea puede ser, por ejemplo, clasificación, regresión, análisis de supervivencia, agrupamiento o detección de valores atípicos. La Predicción es el valor que el modelo de aprendizaje automático pronostica, en función de las características dadas. En este libro, la predicción del modelo se denota por \\(\\hat{f}(x^{(i)})\\) o \\(\\hat{y}\\). ﻿{r, message = FALSE, warning = FALSE, echo = FALSE} devtools::load_all() Chapter 2 Interpretabilidad No existe una definición matemática de interpretabilidad. Una definición (no matemática) que me gusta de Miller (2017)3 es: Interpretabilidad es el grado en que un humano puede comprender la causa de una decisión. Otra es: Interpretabilidad es el grado a lo que un humano puede predecir constantemente el resultado del modelo[^crítica]. Cuanto mayor sea la interpretabilidad de un modelo de aprendizaje automático, más fácil será para alguien comprender por qué se han tomado ciertas decisiones o predicciones. Un modelo es más interpretable que otro si sus decisiones son más fáciles de comprender para un humano que las de otro modelo. Usaré los términos interpretable y explicable en forma indistinta. Al igual que Miller (2017), creo que tiene sentido distinguir entre los términos interpretabilidad / explicabilidad y explicación. Usaré “explicación” para explicaciones de predicciones individuales. Ve la sección sobre explicaciones para aprender lo que los humanos vemos como una buena explicación. 2.1 Importancia de la interpretabilidad Si un modelo de aprendizaje automático funciona bien, ¿por qué no confiamos en el modelo e ignoramos por qué tomó una determinada decisión? “El problema es que una sola métrica, como la precisión de la clasificación, es una descripción incompleta de la mayoría de las tareas del mundo real”. (Doshi-Velez y Kim 20174) Profundicemos en las razones por las que la interpretabilidad es tan importante. Cuando se trata de modelado predictivo, debe hacer una compensación: ¿solo desea saber qué se predice? Por ejemplo, la probabilidad de que un cliente abandone un servicio o qué tan efectivo será un medicamento para un paciente. ¿O quieres saber por qué se hizo la predicción y posiblemente pagar la interpretabilidad con una caída en el rendimiento predictivo? En algunos casos, no le importa por qué se tomó una decisión, es suficiente saber que el rendimiento predictivo en un conjunto de datos de prueba fue bueno. Pero en otros casos, conocer el ‘por qué’ puede ayudarlo a aprender más sobre el problema, los datos y la razón por la cual un modelo puede fallar. Es posible que algunos modelos no requieran explicaciones porque se usan en un entorno de bajo riesgo, lo que significa que un error no tendrá consecuencias graves (por ejemplo, un sistema de recomendación de películas) o que el método ya ha sido ampliamente estudiado y evaluado (por ejemplo, reconocimiento óptico de caracteres). La necesidad de interpretabilidad surge de una incompletitud en la formalización del problema (Doshi-Velez y Kim 2017), lo que significa que para ciertos problemas o tareas no es suficiente obtener la predicción (el qué). El modelo también debe explicar cómo llegó a la predicción (el por qué), porque una predicción correcta solo resuelve parcialmente su problema original. Las siguientes razones impulsan la demanda de interpretabilidad y explicaciones (Doshi-Velez y Kim 2017 y Miller 2017). Curiosidad y aprendizaje humanos: los humanos tienen un modelo mental de su entorno que se actualiza cuando ocurre algo inesperado. Esta actualización se realiza buscando una explicación para el evento inesperado. Por ejemplo, un humano se siente enfermo y pregunta: “¿Por qué me siento tan enfermo?”. Se entera de que se enferma cada vez que come frutos rojos. Actualiza su modelo mental y decide que esos frutos causaron la enfermedad y, por lo tanto, deben evitarse. Cuando se usan modelos opacos de aprendizaje automático en la investigación, los hallazgos científicos permanecen completamente ocultos si el modelo solo da predicciones sin explicaciones. Para facilitar el aprendizaje y satisfacer la curiosidad de por qué ciertas predicciones o comportamientos son creados por máquinas, la interpretación y las explicaciones son cruciales. Por supuesto, los humanos no necesitan explicaciones para todo lo que sucede. Para la mayoría de las personas, no hay problemas en no entender cómo funciona una computadora. Eventos inesperados nos hacen curiosos. Por ejemplo: ¿Por qué mi computadora se apaga inesperadamente? Estrechamente relacionado con el aprendizaje está el deseo humano de encontrar significado en el mundo. Queremos armonizar las contradicciones o inconsistencias entre los elementos de nuestras estructuras de conocimiento. “¿Por qué mi perro me mordió a pesar de que nunca antes lo había hecho?” Un humano podría preguntar. Existe una contradicción entre el conocimiento del comportamiento pasado del perro y la experiencia desagradable recién hecha de la mordedura. La explicación del veterinario concilia la contradicción del dueño del perro: “El perro estaba estresado y mordido”. Cuanto más la decisión de una máquina afecta la vida de una persona, más importante es que la máquina explique su comportamiento. Si un modelo de aprendizaje automático rechaza una solicitud de préstamo, esto puede ser completamente inesperado para los solicitantes. Solo pueden conciliar esta inconsistencia entre la expectativa y la realidad con algún tipo de explicación. En realidad, las explicaciones no tienen que explicar completamente la situación, sino que deben abordar una causa principal. Otro ejemplo es la recomendación algorítmica del producto. Personalmente, siempre pienso en por qué ciertos productos o películas me han sido recomendados algorítmicamente. A menudo es bastante claro: la publicidad me sigue en Internet porque recientemente compré una lavadora, y sé que en los próximos días me seguirán anuncios de lavadoras. Sí, tiene sentido sugerir guantes si ya tengo un gorro de nieve en mi carrito de compras. El algoritmo recomienda esta película, porque los usuarios a quienes les gustaron otras películas que me gustaron también disfrutaron la película recomendada. Cada vez más, las compañías de Internet están agregando explicaciones a sus recomendaciones. Un buen ejemplo es la recomendación de productos de Amazon, que se basa en combinaciones de productos que se compran con frecuencia: knitr::include_graphics(&quot;images/amazon-freq-bought-together.png&quot;) FIGURE 2.1: Productos recomendados cuando se compra pintura en Amazon. En muchas disciplinas científicas hay un cambio de métodos cualitativos a cuantitativos (por ejemplo, sociología, psicología), y también hacia el aprendizaje automático (biología, genómica) El objetivo de la ciencia es obtener conocimiento, pero muchos problemas se resuelven con grandes conjuntos de datos y modelos de aprendizaje automático de caja negra. El modelo en sí se convierte en la fuente de conocimiento en lugar de los datos. La interpretabilidad hace posible extraer este conocimiento adicional capturado por el modelo. Los modelos de aprendizaje automático asumen tareas del mundo real que requieren medidas de seguridad y pruebas. Imagina que un automóvil autónomo detecta automáticamente a los ciclistas en función de un sistema de aprendizaje profundo. Deseas estar 100% seguro de que la abstracción que ha aprendido el sistema está libre de errores, porque atropellar a los ciclistas es bastante malo. Una explicación podría revelar que la característica aprendida más importante es reconocer las dos ruedas de una bicicleta, pero existen casos de borde, como bicicletas con bolsas laterales que cubren parcialmente las ruedas. Por defecto, los modelos de aprendizaje automático recogen sesgos de los datos de entrenamiento. Esto puede convertir sus modelos de aprendizaje automático en racistas que discriminan determinados grupos. La interpretabilidad es una herramienta de depuración útil para detectar sesgos en modelos de aprendizaje automático. Puede suceder que el modelo de aprendizaje automático que usted haya entrenado para la aprobación automática o el rechazo de las solicitudes de crédito discrimine a una minoría. Su objetivo principal es otorgar préstamos solo a personas que eventualmente los pagarán. Lo incompleto de la formulación del problema en este caso radica en el hecho de que no solo desea minimizar los impagos de préstamos, sino que también está obligado a no discriminar sobre la base de ciertos datos demográficos. Esta es una restricción adicional que forma parte de la formulación de su problema (otorgar préstamos de manera riesgosa y conforme) que no está cubierta por la función de pérdida para la que se optimizó el modelo de aprendizaje automático. El proceso de integración de máquinas y algoritmos en nuestra vida diaria requiere interpretabilidad para aumentar la aceptación social. Las personas atribuyen creencias, deseos, intenciones, etc. a los objetos. En un famoso experimento, Heider y Simmel (1944)5 mostraron a los participantes videos de formas en las que un círculo abría una “puerta” para ingresar a una “habitación” (que era simplemente un rectángulo). Los participantes describieron las acciones de las formas como describirían las acciones de un agente humano, asignando intenciones e incluso emociones y rasgos de personalidad a las formas. Los robots son un buen ejemplo, como mi aspiradora, a la que llamé “Doge”. Si Doge se atasca, pienso: “Doge quiere seguir limpiando, pero me pide ayuda porque se atascó”. Más tarde, cuando Doge termina de limpiar y busca en la base de operaciones para recargar, pienso: “Doge desea recargar y tiene la intención de encontrar la base de operaciones”. También atribuyo rasgos de personalidad: “Doge es un poco tonto, pero de una manera linda”. Estos son mis pensamientos, especialmente cuando descubro que Doge ha derribado una planta mientras aspiraba la casa. Una máquina o algoritmo que explica sus predicciones encontrará más aceptación. Véase también el capítulo sobre explicaciones, que argumenta que las explicaciones son un proceso social. Las explicaciones se utilizan para gestionar las interacciones sociales. Al crear un significado compartido de algo, el explicador influye en las acciones, emociones y creencias del receptor de la explicación. Para que una máquina interactúe con nosotros, puede que tenga que moldear nuestras emociones y creencias. Las máquinas tienen que “persuadirnos” para que puedan lograr su objetivo. No aceptaría completamente mi robot aspirador si no explicara su comportamiento, al menos hasta cierto punto. La aspiradora crea un significado compartido de, por ejemplo, un “accidente” (como quedarse atascado en la alfombra del baño … otra vez) al explicar que se atascó en lugar de simplemente detenerse a trabajar sin comentarios. Curiosamente, puede haber un desalineamiento entre el objetivo de la máquina explicadora (crear confianza) y el objetivo del destinatario (comprender la predicción o el comportamiento). Quizás la explicación completa de por qué Doge se atascó podría ser que la batería estaba muy baja, que una de las ruedas no funciona correctamente y que hay un error que hace que el robot vaya al mismo lugar una y otra vez a pesar de que había un obstaculo. Estas razones (y algunas más) hicieron que el robot se atascara, aunque algo estaba en el camino, y eso fue suficiente para que confiara en su comportamiento y se produjera el accidente. Por cierto, Doge se quedó atascado en el baño nuevamente. Tenemos que quitar las alfombras cada vez antes de dejar que Doge aspire. knitr::include_graphics(&quot;images/doge-stuck.jpg&quot;) FIGURE 2.2: Doge, nuestra aspiradora, se atascó. Como explicación del accidente, Doge nos dijo que debe estar en una superficie plana. Los modelos de aprendizaje automático solo se pueden depurar y auditar cuando se pueden interpretar. Incluso en entornos de bajo riesgo, como las recomendaciones de películas, la capacidad de interpretación es valiosa en la fase de investigación y desarrollo, así como después de la implementación. Más tarde, cuando se usa un modelo en un producto, las cosas pueden salir mal. Una interpretación para una predicción errónea ayuda a comprender la causa del error. Ofrece una dirección sobre cómo arreglar el sistema. Considere un ejemplo de un clasificador entre perros siberianos y lobos, que clasifica erróneamente a algunos siberianos como lobos. Al utilizar métodos de aprendizaje automático interpretables, descubrirá que la clasificación errónea se debió a la nieve en la imagen. El clasificador aprendió a usar la nieve como una característica para clasificar las imágenes como “lobo”, lo que podría tener sentido en términos de separar a los lobos de los siberianos en el conjunto de datos de entrenamiento, pero no en el uso en el mundo real. Si puede asegurarse de que el modelo de aprendizaje automático pueda explicar las decisiones, también puede verificar los siguientes rasgos con mayor facilidad (Doshi-Velez y Kim 2017): Equidad: garantizar que las predicciones sean imparciales y no discriminen implícita o explícitamente a ciertos grupos. Un modelo interpretable puede decirle por qué ha decidido que cierta persona no debería obtener un préstamo, y es más fácil para un humano juzgar si la decisión se basa en un sesgo demográfico aprendido (por ejemplo, racial). Privacidad: garantizar que la información confidencial de los datos esté protegida. Fiabilidad o robustez: garantizar que pequeños cambios en la entrada no conduzcan a grandes cambios en la predicción. Causalidad: compruebe que solo se recogen las relaciones causales. Confianza: es más fácil para los humanos confiar en un sistema que explica sus decisiones en comparación con una caja negra. Cuando no necesitamos interpretabilidad. Los siguientes escenarios ilustran cuando no necesitamos o incluso no queremos la interpretabilidad de los modelos de aprendizaje automático. La interpretabilidad no es necesaria si el modelo no tiene un impacto significativo. Imagine a alguien llamado Mike trabajando en un proyecto paralelo de aprendizaje automático para predecir a dónde irán sus amigos para sus próximas vacaciones en base a datos de Facebook. A Mike le gusta sorprender a sus amigos con suposiciones educadas sobre dónde irán de vacaciones. No hay ningún problema real si el modelo está equivocado (en el peor de los casos, solo un poco de vergüenza para Mike), ni hay un problema si Mike no puede explicar el resultado de su modelo. Está perfectamente bien no tener interpretabilidad en este caso. La situación cambiaría si Mike comenzara a construir un negocio en torno a estas predicciones de destinos de vacaciones. Si el modelo está equivocado, el negocio podría perder dinero, o el modelo podría funcionar peor para algunas personas debido al prejuicio racial aprendido. Tan pronto como el modelo tenga un impacto significativo, ya sea financiero o social, la interpretabilidad se vuelve relevante. La interpretabilidad no es necesaria cuando el problema está bien estudiado. Algunas aplicaciones se han estudiado lo suficientemente bien como para que haya suficiente experiencia práctica con el modelo y los problemas con el modelo se hayan resuelto con el tiempo. Un buen ejemplo es un modelo de aprendizaje automático para el reconocimiento óptico de caracteres que procesa imágenes de sobres y extrae direcciones. Hay años de experiencia con estos sistemas y está claro que funcionan. Además, no estamos realmente interesados en obtener información adicional sobre la tarea en cuestión. La interpretabilidad podría permitir a las personas o programas manipular el sistema. Los problemas con los usuarios que engañan a un sistema son el resultado de una falta de coincidencia entre los objetivos del creador y el usuario de un modelo. La calificación crediticia es un sistema de este tipo porque los bancos quieren asegurarse de que los préstamos solo se otorguen a los solicitantes que puedan devolverlos, y los solicitantes aspiran a obtener el préstamo incluso si el banco no quiere darles uno. Este desajuste entre los objetivos introduce incentivos para que los solicitantes jueguen con el sistema para aumentar sus posibilidades de obtener un préstamo. Si un solicitante sabe que tener más de dos tarjetas de crédito afecta negativamente su puntaje, simplemente devuelve su tercera tarjeta de crédito para mejorar su puntaje y solicita una nueva tarjeta después de que el préstamo haya sido aprobado. Si bien su puntaje mejoró, la probabilidad real de pagar el préstamo se mantuvo sin cambios. El sistema solo se puede manipular si las entradas son representantes de una característica causal, pero en realidad no causan el resultado. Siempre que sea posible, se deben evitar las funciones de proxy ya que hacen que los modelos sean manipulables. Por ejemplo, Google desarrolló un sistema llamado Google Flu Trends para predecir los brotes de gripe. El sistema correlacionó las búsquedas de Google con los brotes de gripe, y ha tenido un mal desempeño. La distribución de las consultas de búsqueda cambió y Google Flu Trends perdió muchos brotes de gripe. Las búsquedas en Google no causan la gripe. Cuando las personas buscan síntomas como “fiebre”, se trata simplemente de una correlación con los brotes de gripe reales. Idealmente, los modelos solo usarían características causales porque no serían manipulables. 2.2 Taxonomía de los métodos de interpretación Los métodos para la interpretación de aprendizaje automático se pueden clasificar de acuerdo con varios criterios. ¿Intrínseco o post hoc? Este criterio distingue si la interpretabilidad se logra restringiendo la complejidad del modelo de aprendizaje automático (intrínseco) o aplicando métodos que analizan el modelo después del entrenamiento (post hoc). La interpretabilidad intrínseca se refiere a modelos de aprendizaje automático que se consideran interpretables debido a su estructura simple, como árboles de decisión cortos o modelos lineales dispersos. La interpretabilidad post hoc se refiere a la aplicación de métodos de interpretación después del entrenamiento modelo. La importancia de la característica de permutación es, por ejemplo, un método de interpretación post hoc. Los métodos post hoc también se pueden aplicar a modelos intrínsecamente interpretables. Por ejemplo, la importancia de la característica de permutación se puede calcular para los árboles de decisión. La organización de los capítulos de este libro está determinada por la distinción entre [modelos intrínsecamente interpretables] (#simple) y [métodos de interpretación post hoc (y modelo-agnósticos)] (#agnóstico). Resultado del método de interpretación Los diversos métodos de interpretación pueden diferenciarse aproximadamente de acuerdo con sus resultados. Estadística de resumen de características: Muchos métodos de interpretación proporcionan estadísticas de resumen para cada covariable. Algunos métodos devuelven un solo número por característica, como la importancia de la característica, o un resultado más complejo, como las fortalezas de interacción de features por pares. Visualización de resumen de características: La mayoría de las estadísticas de resumen de características también se pueden visualizar. Algunos resúmenes de características en realidad solo tienen sentido si se visualizan y una tabla sería una elección incorrecta. La dependencia parcial de una característica es tal caso. Las gráficas de dependencia parcial son curvas que muestran una característica y el resultado promedio pronosticado. La mejor manera de presentar dependencias parciales es dibujar la curva en lugar de imprimir las coordenadas. Elementos internos del modelo (p. Ej., Pesos aprendidos): La interpretación de modelos intrínsecamente interpretables entra en esta categoría. Algunos ejemplos son los pesos en modelos lineales o la estructura de árbol aprendida (las características y los umbrales utilizados para las divisiones) de los árboles de decisión. Las líneas se desdibujan entre las partes internas del modelo y la estadística de resumen de características en, por ejemplo, modelos lineales, porque los pesos son tanto las partes internas del modelo como las estadísticas de resumen de las características al mismo tiempo. Otro método que genera modelos internos es la visualización de detectores de características aprendidos en redes neuronales convolucionales. Los métodos de interpretación que generan elementos internos del modelo son, por definición, específicos del modelo (consulte el siguiente criterio). Punto de datos : Esta categoría incluye todos los métodos que devuelven observaciones (ya existentes o recién creados) para hacer que un modelo sea interpretable. Un método se llama explicaciones contrafácticas. Para explicar la predicción de una instancia de datos, el método encuentra una observación similar al cambiar algunas de las características para las cuales el resultado predicho cambia de manera relevante. Otro ejemplo es la identificación de prototipos de clases predichas. Para ser útiles, los métodos de interpretación que generan nuevos puntos de datos requieren que los propios puntos de datos puedan ser interpretados. Esto funciona bien para imágenes y textos, pero es menos útil para datos tabulares con cientos de características. Modelo intrínsecamente interpretable: Una solución para interpretar modelos de caja negra es aproximarlos (global o localmente) con un modelo interpretable. El modelo interpretable en sí mismo se interpreta mirando los parámetros internos del modelo o las estadísticas de resumen de sus características. Modelo específico o modelo agnóstico? Las herramientas de interpretación modelo-específicas están limitadas a esos modelos. La interpretación de los pesos de regresión en un modelo lineal es una interpretación de este tipo ya que, por definición, siempre es específica del modelo. Herramientas que solo funcionan para la interpretación de, por ejemplo, las redes neuronales son específicas del modelo. Las herramientas independientes del modelo se pueden usar en cualquier modelo de aprendizaje automático y se aplican después de que el modelo haya sido entrenado (post hoc). Estos métodos generalmente funcionan mediante el análisis de pares de entrada y salida de características. Por definición, estos métodos no pueden tener acceso a los modelos internos, como los pesos o la información estructural. ¿Local o global? ¿El método de interpretación explica una predicción individual o el comportamiento completo del modelo? ¿O algún punto intermedio? Lea más sobre el criterio de alcance en la siguiente sección. 2.3 Alcance de la interpretabilidad Un algoritmo entrena un modelo que produce las predicciones. Cada paso puede evaluarse en términos de transparencia o interpretabilidad. 2.3.1 Transparencia del algoritmo ¿Cómo crea el algoritmo el modelo? La transparencia del algoritmo se trata de cómo el algoritmo aprende un modelo desde los datos, y de qué tipo de relaciones puede incorporar. Si utilizas redes neuronales convolucionales para clasificar imágenes, puedes explicar que el algoritmo aprende detectores de borde y filtros en las capas más bajas. Esto es una comprensión de cómo funciona el algoritmo, pero no para el modelo específico que se aprende al final, y tampoco para la forma en la que se hacen las predicciones individuales. La transparencia del algoritmo solo requiere el conocimiento del algoritmo y no de los datos o el modelo aprendido. Este libro se centra en la interpretabilidad del modelo y no en la transparencia del algoritmo. Algoritmos como el método de mínimos cuadrados para modelos lineales están bien estudiados y entendidos. Se caracterizan por una alta transparencia. Los enfoques de aprendizaje profundo (empujar un gradiente a través de una red con millones de pesos) se entienden menos y el funcionamiento interno es el foco de la investigación en curso. Se consideran menos transparentes. 2.3.2 Interpretabilidad global y holística del modelo ¿Cómo hace predicciones el modelo entrenado? Podrías describir un modelo como interpretable si puedes comprender todo el modelo de una vez (Lipton 20166). Para explicar el resultado del modelo global necesitas el modelo entrenado, el conocimiento del algoritmo y los datos. Este nivel de interpretabilidad se trata de comprender cómo toma decisiones el modelo, en función de una visión holística de sus características y de cada uno de los componentes aprendidos, como los pesos, parámetros y estructuras. ¿Qué características son importantes y qué tipo de interacciones entre ellas tienen lugar? La interpretación global del modelo ayuda a comprender la distribución de su resultado objetivo en función de las características. La interpretabilidad del modelo global es muy difícil de lograr en la práctica. Es improbable que cualquier modelo que exceda un puñado de parámetros o pesos se ajuste a la memoria a corto plazo del ser humano promedio. Sostengo que realmente no puedes imaginar un modelo lineal con 5 características, porque significaría dibujar mentalmente el hiperplano estimado en un espacio de 5 dimensiones. Cualquier espacio de características con más de 3 dimensiones es simplemente inconcebible para los humanos. Por lo general, cuando las personas intentan comprender un modelo, solo consideran partes de él, como los pesos en los modelos lineales. 2.3.3 Interpretabilidad del modelo global en un nivel modular ¿Cómo afectan las predicciones las partes del modelo? Un modelo de Naive Bayes con cientos de características sería demasiado grande para mantenerlo en nuestra memoria de trabajo. E incluso si logramos memorizar todos los pesos, no podríamos hacer predicciones rápidamente para nuevas observaciones. Además, debes tener la distribución conjunta de todas las características en su cabeza para estimar la importancia de cada característica y cómo las características afectan las predicciones en promedio. Una tarea imposible. Pero puedes entender fácilmente un solo peso. Si bien la interpretación global del modelo generalmente está fuera del alcance, existe una buena posibilidad de comprender al menos algunos modelos a nivel modular. No todos los modelos son interpretables a nivel de parámetro. Para los modelos lineales, las partes interpretables son los pesos, para los árboles serían las divisiones (características seleccionadas más puntos de corte) y las predicciones de los nodos foliares. Los modelos lineales, por ejemplo, se ven como si pudieran interpretarse perfectamente en un nivel modular, pero la interpretación de un solo peso está entrelazada con todos los demás pesos. La interpretación de un solo peso siempre viene con la nota al pie de página de que las otras características de entrada permanecen en el mismo valor, que no es el caso con muchas aplicaciones reales. Un modelo lineal que predice el valor de una casa, que tiene en cuenta tanto el tamaño de la casa como el número de habitaciones, puede tener un peso negativo para la característica de la cantidad de habitaciones. Puede suceder porque ya existe la característica de tamaño de la casa altamente correlacionada. En un mercado donde la gente prefiere habitaciones más grandes, una casa con menos habitaciones podría valer más que una casa con más habitaciones si ambas tienen el mismo tamaño. Los pesos solo tienen sentido en el contexto de las otras características del modelo. Pero los pesos en un modelo lineal aún se pueden interpretar mejor que los pesos de una red neuronal profunda. 2.3.4 Interpretabilidad local para una única predicción ¿Por qué el modelo hizo una cierta predicción para una instancia? Puedes ampliar una sola instancia y examinar lo que el modelo predice para esta entrada, y explicar por qué. Si observas una predicción individual, el comportamiento del modelo complejo podría comportarse de manera más agradable. Localmente, la predicción podría depender solo linealmente o monotónicamente de algunas características, en lugar de tener una dependencia compleja de ellas. Por ejemplo, el valor de una casa puede depender no linealmente de su tamaño. Pero si solo estás mirando una casa particular de 100 metros cuadrados, existe la posibilidad de que para ese subconjunto de datos, la predicción de su modelo dependa linealmente del tamaño. Puedes descubrir esto simulando cómo cambia el precio previsto cuando aumenta o disminuye el tamaño en 10 metros cuadrados. Por lo tanto, las explicaciones locales pueden ser más precisas que las explicaciones globales. Este libro presenta métodos que pueden hacer que las predicciones individuales sean más interpretables en la [sección sobre métodos modelo-agnósticos] (#agnostico). 2.3.5 Interpretabilidad local para un grupo de predicciones ¿Por qué el modelo hizo predicciones específicas para un grupo de instancias? Las predicciones del modelo para varias observaciones pueden explicarse con métodos de interpretación de modelos globales (a nivel modular) o con explicaciones particulares por observación. Los métodos globales se pueden aplicar tomando el grupo de observaciones, tratándolos como si fuera el conjunto de datos completo y utilizando los métodos globales con este subconjunto. Los métodos de explicación individuales se pueden utilizar en cada instancia y luego enumerar o agregar para todo el grupo. 2.4 Evaluación de la interpretabilidad No existe un consenso real sobre qué interpretabilidad es en el aprendizaje automático. Tampoco está claro cómo medirla. Pero hay una investigación inicial sobre esto y un intento de formular algunos enfoques para la evaluación, como se describe en la siguiente sección. Doshi-Velez y Kim (2017) proponen tres niveles principales para la evaluación de la interpretabilidad: Evaluación del nivel de aplicación (tarea real): Ponga la explicación en el producto y haga que el usuario final lo pruebe. Imagine un software de detección de fracturas con un componente de aprendizaje automático que localiza y marca fracturas en rayos X. A nivel de aplicación, los radiólogos probarían el software de detección de fracturas directamente para evaluar el modelo. Esto requiere una buena configuración experimental y una comprensión de cómo evaluar la calidad. Una buena base para esto es siempre qué tan bueno sería un humano para explicar la misma decisión. Evaluación a nivel humano (tarea simple) Es una evaluación de nivel de aplicación simplificada. La diferencia es que estos experimentos no se llevan a cabo con expertos en el dominio, sino con personas ‘laicas’. Esto hace que los experimentos sean más baratos (especialmente si los expertos en el dominio son radiólogos) y es más fácil encontrar más evaluadores. Un ejemplo sería mostrarle a un usuario diferentes explicaciones y el usuario elegiría la mejor. La evaluación del nivel de función (tarea proxy) no requiere humanos. Esto funciona mejor cuando la clase de modelo utilizada ya ha sido evaluada por otra persona en una evaluación a nivel humano. Por ejemplo, podría saberse que los usuarios finales entienden los árboles de decisión. En este caso, un proxy para la calidad de la explicación puede ser la profundidad del árbol. Los árboles más cortos obtendrían una mejor puntuación de explicabilidad. Tendría sentido agregar la restricción de que el rendimiento predictivo del árbol sigue siendo bueno y no disminuye demasiado en comparación con un árbol más grande. El próximo capítulo se centra en la evaluación de explicaciones para predicciones individuales en el nivel de función. ¿Cuáles son las propiedades relevantes de las explicaciones que consideraríamos para su evaluación? 2.5 Propiedades de las explicaciones Queremos explicar las predicciones de un modelo de aprendizaje automático. Para lograr esto, confiamos en algún método de explicación, que es un algoritmo que genera explicaciones. Una explicación generalmente relaciona los valores de características de una instancia con la predicción de su modelo de una manera humanamente comprensible. Otros tipos de explicaciones consisten en un conjunto de instancias de datos (por ejemplo, en el caso del modelo vecino k-más cercano). Por ejemplo, podríamos predecir el riesgo de cáncer utilizando una SVM y explicar las predicciones utilizando el método sustituto local, que genera árboles de decisión como explicaciones. O podríamos usar un modelo de regresión lineal en lugar de una SVM. El modelo de regresión lineal ya está equipado con un método de explicación (interpretación de los pesos). Echamos un vistazo más de cerca a las propiedades de los métodos de explicación y explicaciones (Robnik-Sikonja y Bohanec, 20187). Estas propiedades se pueden usar para juzgar qué tan bueno es un método. No está claro para todas estas propiedades cómo medirlas correctamente, por lo que uno de los desafíos es formalizar cómo podrían calcularse. Propiedades de los métodos de explicación Poder expresivo es el “lenguaje” o estructura de las explicaciones que el método puede generar. Un método de explicación podría generar reglas IF-THEN, árboles de decisión, una suma ponderada, lenguaje natural u otra cosa. Translucidez describe cuánto se basa el método de explicación en analizar el modelo de aprendizaje automático, como sus parámetros. Por ejemplo, los métodos de explicación que se basan en modelos intrínsecamente interpretables como el modelo de regresión lineal (específico del modelo) son altamente translúcidos. Los métodos que solo se basan en manipular entradas y observar las predicciones tienen cero translucidez. Dependiendo del escenario, diferentes niveles de translucidez pueden ser deseables. La ventaja de la alta translucidez es que el método puede confiar en más información para generar explicaciones. La ventaja de la baja translucidez es que el método de explicación es más portátil. Portabilidad describe la gama de modelos de aprendizaje automático con los que se puede utilizar el método de explicación. Los métodos con baja translucidez tienen una mayor portabilidad porque tratan el modelo de aprendizaje automático como una caja negra. Los modelos sustitutos pueden ser el método de explicación con la mayor portabilidad. Métodos que solo funcionan, por ejemplo, para explicar las redes neuronales tienen baja portabilidad. Complejidad algorítmica describe la complejidad computacional del método que genera la explicación. Es importante tener en cuenta esta propiedad cuando el tiempo de cálculo es un cuello de botella en la generación de explicaciones. Propiedades de explicaciones individuales Precisión: ¿Qué tan bien una explicación predice datos nuevos? La alta precisión es especialmente importante si la explicación se usa para las predicciones, y no para el modelo en sí. La baja precisión puede estar bien si la precisión del modelo de aprendizaje automático también es baja, y si el objetivo es explicar lo que hace el modelo de caja negra. En este caso, solo la fidelidad es importante. Fidelity: ¿Qué tan bien se aproxima la explicación a la predicción del modelo de caja negra? La alta fidelidad es una de las propiedades más importantes de una explicación, porque una explicación con baja fidelidad es inútil para explicar el modelo de aprendizaje automático. La precisión y la fidelidad están estrechamente relacionadas. Si el modelo de caja negra tiene una alta precisión y la explicación tiene una alta fidelidad, la explicación también tiene una alta precisión. Algunas explicaciones ofrecen solo fidelidad local, lo que significa que la explicación solo se aproxima bien a la predicción del modelo para un subconjunto de datos (por ejemplo, modelos sustitutos locales) o incluso solo para una observación individual (por ejemplo, [Valores de Shapley] ( # shapley)). Consistencia: ¿Cuánto difiere una explicación entre los modelos que han sido entrenados en la misma tarea y que producen predicciones similares? Por ejemplo, entreno una SVM y un modelo de regresión lineal en la misma tarea y ambos producen predicciones muy similares. Calculo explicaciones usando un método de mi elección y analizo cuán diferentes son las explicaciones. Si las explicaciones son muy similares, las explicaciones son muy consistentes. Encuentro esta propiedad algo complicada, ya que los dos modelos podrían usar características diferentes, pero obtener predicciones similares (también llamado “Efecto Rashomon”). En este caso, no es deseable una alta consistencia porque las explicaciones tienen que ser muy diferentes. Es deseable una alta consistencia si los modelos realmente dependen de relaciones similares. Estabilidad: ¿Qué tan similares son las explicaciones para instancias similares? Mientras que la coherencia compara explicaciones entre modelos, la estabilidad compara explicaciones entre instancias similares para un modelo fijo. Alta estabilidad significa que ligeras variaciones en las características de una observación no cambian sustancialmente la explicación (a menos que estas ligeras variaciones también cambien fuertemente la predicción). La falta de estabilidad puede ser el resultado de una alta variación del método de explicación. En otras palabras, el método de explicación se ve fuertemente afectado por ligeros cambios en los valores de las características de la observación a explicar. La falta de estabilidad también puede ser causada por componentes no deterministas del método de explicación, como un paso de muestreo de datos, como el uso del método sustituto local. La alta estabilidad siempre es deseable. Comprensibilidad: ¿Qué tan bien entienden los humanos las explicaciones? Esto se parece a una propiedad más entre muchas, pero es el elefante en la habitación. Difícil de definir y medir, pero extremadamente importante para acertar. Muchas personas están de acuerdo en que la comprensión depende de la audiencia. Las ideas para medir la comprensibilidad incluyen medir el tamaño de la explicación (número de características con un peso distinto de cero en un modelo lineal, número de reglas de decisión, …) o probar qué tan bien las personas pueden predecir el comportamiento del modelo de aprendizaje automático a partir de las explicaciones. También se debe considerar la comprensión de las características utilizadas en la explicación. Una transformación compleja de características podría ser menos comprensible que las características originales. Certeza: ¿La explicación refleja la certeza del modelo de aprendizaje automático? Muchos modelos de aprendizaje automático solo dan predicciones sin una declaración sobre la confianza de los modelos de que la predicción es correcta. Si el modelo predice un 4% de probabilidad de cáncer para un paciente, ¿es igual de cierto que un 4% de probabilidad para otro paciente con diferentes valores de características, pero igual valor predicho? Una explicación que incluye la certeza del modelo es muy útil. Grado de importancia: ¿Qué tan bien refleja la explicación la importancia de las características o partes de la explicación? Por ejemplo, si se genera una regla de decisión como explicación para una predicción individual, ¿está claro cuál de las condiciones de la regla fue la más importante? Novedad: ¿La explicación refleja si una instancia de datos a explicar proviene de una “nueva” región muy alejada de la distribución de datos de capacitación? En tales casos, el modelo puede ser inexacto y la explicación puede ser inútil. El concepto de novedad está relacionado con el concepto de certeza. Cuanto mayor sea la novedad, más probable es que el modelo tenga poca certeza debido a la falta de datos. Representatividad: ¿Cuántas instancias cubre una explicación? Las explicaciones pueden abarcar todo el modelo (p. Ej., Interpretación de pesos en un modelo de regresión lineal) o representar solo una predicción individual (p. Ej., Valores de Shapley). 2.6 Explicaciones amigables para los humanos Profundicemos y descubramos lo que los humanos vemos como “buenas” explicaciones y cuáles son las implicaciones para el aprendizaje automático interpretable. La investigación en humanidades puede ayudarnos a descubrirlo. Miller (2017) ha realizado una gran encuesta de publicaciones sobre explicaciones, y este capítulo se basa en su resumen. En este capítulo quiero convencerlo de lo siguiente: Como explicación de un evento, los humanos prefieren explicaciones cortas (solo 1 o 2 causas) que contrastan la situación actual con una situación en la que el evento no hubiera ocurrido. Las causas especialmente anormales proporcionan buenas explicaciones. Las explicaciones son interacciones sociales entre el explicador y el explicado (receptor de la explicación) y, por lo tanto, el contexto social tiene una gran influencia en el contenido real de la explicación. Cuando necesitas explicaciones con TODOS los factores para una predicción o comportamiento particular, no deseas una explicación amigable para los humanos, sino una atribución causal completa. Probablemente desees una atribución causal si estás legalmente obligado a especificar todas las características influyentes o si depuras el modelo de aprendizaje automático. En este caso, ignora los siguientes puntos. En todos los demás casos, donde los ‘laicos’ o las personas con poco tiempo son los destinatarios de la explicación, las siguientes secciones deberían de serte interesantes. 2.6.1 ¿Qué es una explicación? Una explicación es la respuesta a una pregunta de por qué (Miller 2017). ¿Por qué el tratamiento no funcionó en el paciente? ¿Por qué fue rechazado mi préstamo? ¿Por qué todavía no hemos sido contactados por la vida alienígena? Las dos primeras preguntas pueden responderse con una explicación “cotidiana”, mientras que la tercera proviene de la categoría “Fenómenos científicos más generales y preguntas filosóficas”. Nos centramos en las explicaciones de tipo “cotidiano”, porque son relevantes para el aprendizaje automático interpretable. Las preguntas que comienzan con “cómo” generalmente se pueden reformular como preguntas de “por qué”: “¿Cómo se rechazó mi préstamo?” puede convertirse en “¿Por qué se rechazó mi préstamo?”. A continuación, el término “explicación” se refiere al proceso social y cognitivo de explicación, pero también al producto de estos procesos. El explicador puede ser un ser humano o una máquina. 2.6.2 ¿Qué es una buena explicación? Esta sección condensa aún más el resumen de Miller sobre explicaciones “buenas” y agrega implicaciones concretas para el aprendizaje automático interpretable. Las explicaciones son contrastantes (Lipton 19908). Los humanos generalmente no preguntan por qué se hizo una determinada predicción, sino por qué se hizo esta predicción en lugar de otra predicción. Tendemos a pensar en casos contrafácticos, es decir, “¿Cómo habría sido la predicción si la entrada X hubiera sido diferente?”. Para una predicción del precio de la vivienda, el propietario podría estar interesado en saber por qué el precio previsto fue alto, en comparación con el precio más bajo que esperaba. Si mi solicitud de préstamo es rechazada, no me importa escuchar todos los factores que generalmente hablan a favor o en contra de un rechazo. Estoy interesado en los factores en mi solicitud que tendrían que cambiar para obtener el préstamo. Quiero saber el contraste entre mi aplicación y la versión de mi solicitud que sería aceptada. El reconocimiento de que las explicaciones contrastantes importan es un hallazgo importante para el aprendizaje automático explicable. De la mayoría de los modelos interpretables, puede extraer una explicación que contrasta implícitamente una predicción de una instancia con la predicción de una instancia de datos artificiales o un promedio de instancias. Los médicos podrían preguntar: “¿Por qué el medicamento no funcionó para mi paciente?”. Y podrían querer una explicación que contraste a su paciente con un paciente para quien el medicamento funcionó y que sea similar al paciente que no responde. Las explicaciones contrastantes son más fáciles de entender que explicaciones completas. Una explicación completa de la pregunta del médico de por qué el medicamento no funciona puede incluir: El paciente ha tenido la enfermedad durante 10 años, 11 genes se sobreexpresan, el cuerpo del paciente es muy rápido en descomponer el medicamento en químicos ineficaces. Una explicación contrastante podría ser mucho más simple: en contraste con el paciente que responde, el paciente que no responde tiene una cierta combinación de genes que hacen que el medicamento sea menos efectivo. La mejor explicación es la que destaca la mayor diferencia entre el objeto de interés y el objeto de referencia. Lo que significa para el aprendizaje automático interpretable: los humanos no quieren una explicación completa para una predicción, sino comparar las diferencias con la predicción de otra observación (que puede ser artificial). La creación de explicaciones contrastantes depende de la aplicación, porque requiere un punto de referencia para la comparación. Y esto puede depender del punto de datos a explicar, pero también del usuario que recibe la explicación. Un usuario de un sitio web de predicción del precio de la vivienda puede querer tener una explicación de una predicción del precio de la vivienda en contraste con su propia casa o tal vez con otra casa en el sitio web o tal vez con una casa promedio en el vecindario. La solución para la creación automatizada de explicaciones contrastantes también podría implicar la búsqueda de prototipos o arquetipos en los datos. Las explicaciones se seleccionan. La gente no espera explicaciones que cubran la lista real y completa de causas de un evento. Estamos acostumbrados a seleccionar una o dos causas de una variedad de causas posibles como LA explicación. Como prueba, encienda las noticias de TV: “El descenso en los precios de las acciones se atribuye a una creciente reacción contra el producto de la compañía debido a problemas con la última actualización de software”. “Tsubasa y su equipo perdieron el partido debido a una defensa débil: dieron a sus oponentes demasiado espacio para desarrollar su estrategia”. “La creciente desconfianza de las instituciones establecidas y nuestro gobierno son los principales factores que han reducido la participación electoral”. El hecho de que un evento puede explicarse por varias causas se llama Efecto Rashomon. Rashomon es una película japonesa que cuenta historias alternativas y contradictorias (explicaciones) sobre la muerte de un samurai. Para los modelos de aprendizaje automático, es ventajoso si se puede hacer una buena predicción a partir de diferentes características. Los métodos de conjunto que combinan múltiples modelos con diferentes características (diferentes explicaciones) generalmente funcionan bien porque promediar esas “historias” hace que las predicciones sean más sólidas y precisas. Pero también significa que hay más de una explicación selectiva de por qué se hizo una determinada predicción. Lo que significa para el aprendizaje automático interpretable: Haga la explicación muy breve, dé solo 1 a 3 razones, incluso si el mundo es más complejo. El método LIME hace un buen trabajo con esto. Las explicaciones son sociales. Son parte de una conversación o interacción entre el explicador y el receptor de la explicación. El contexto social determina el contenido y la naturaleza de las explicaciones. Si quisiera explicarle a una persona técnica por qué las criptomonedas digitales valen tanto, diría cosas como: “La contabilidad descentralizada, distribuida y basada en blockchain, que no puede ser controlado por una entidad central, resuena con las personas que desean asegurarse su riqueza, lo que explica la alta demanda y el precio”. Pero a mi abuela le diría: “Mira, abuela: las criptomonedas son un poco como el oro de la computadora. A la gente le gusta y paga mucho por el oro, y a los jóvenes les gusta y pagan mucho por el oro de la computadora”. Lo que significa para el aprendizaje automático interpretable: Presta atención al entorno social de su aplicación de aprendizaje automático y al público objetivo. Obtener la parte social del modelo de aprendizaje automático correcto depende completamente de su aplicación específica. Encuentra expertos de las humanidades (por ejemplo, psicólogos y sociólogos) para que te ayuden. Las explicaciones se centran en lo anormal. Las personas se enfocan más en causas anormales para explicar los eventos (Kahnemann y Tversky, 19819). Estas son causas que tenían una pequeña probabilidad pero que, sin embargo, ocurrieron. La eliminación de estas causas anormales habría cambiado mucho el resultado (explicación contrafáctica). Los humanos consideran este tipo de causas “anormales” como buenas explicaciones. Un ejemplo de Štrumbelj y Kononenko (2011)10 es: Supongamos que tenemos un conjunto de datos de situaciones de prueba entre profesores y alumnos. Los estudiantes asisten a un curso y lo aprueban directamente después de una presentación exitosa. El maestro tiene la opción de hacer preguntas adicionales al alumno para evaluar su conocimiento. Los estudiantes que no puedan responder estas preguntas reprobarán el curso. Los estudiantes pueden tener diferentes niveles de preparación, lo que se traduce en diferentes probabilidades de responder correctamente las preguntas del maestro (si deciden evaluar al estudiante). Queremos predecir si un alumno aprobará el curso y explicar nuestra predicción. La posibilidad de aprobar es del 100% si el maestro no hace preguntas adicionales; de lo contrario, la probabilidad de aprobar depende del nivel de preparación del alumno y la probabilidad resultante de responder las preguntas correctamente. Escenario 1: el maestro generalmente hace preguntas adicionales a los estudiantes (por ejemplo, 95 de cada 100 veces). Un estudiante que no estudió (10% de posibilidades de aprobar la parte de la pregunta) no fue uno de los afortunados y recibe preguntas adicionales que no responde correctamente. ¿Por qué el alumno reprobó el curso? Yo diría que fue culpa del estudiante por no estudiar. Escenario 2: el profesor rara vez hace preguntas adicionales (por ejemplo, 2 de cada 100 veces). Para un estudiante que no ha estudiado las preguntas, predeciríamos una alta probabilidad de aprobar el curso porque las preguntas son poco probables. Por supuesto, uno de los estudiantes no se preparó para las preguntas, lo que le da un 10% de posibilidades de aprobar las preguntas. No tiene suerte y el profesor hace preguntas adicionales que el alumno no puede responder y no aprueba el curso. ¿Cuál es la razón del fracaso? Yo diría que ahora, la mejor explicación es “porque el profesor evaluó al alumno”. Era poco probable que el maestro hiciera la prueba, por lo que se comportó de manera anormal. Lo que significa para el aprendizaje automático interpretable: Si una de las características de entrada para una predicción fue anormal en algún sentido (como una categoría rara de una característica categórica) y la característica influyó en la predicción, debe incluirse en una explicación, incluso si otras características ‘normales’ tienen la misma influencia en la predicción que la anormal. Una característica anormal en nuestro ejemplo de predicción del precio de la vivienda podría ser que una vivienda bastante cara tiene dos balcones. Incluso si algún método de atribución determina que los dos balcones contribuyen tanto a la diferencia de precio como el tamaño promedio de la casa, el vecindario bueno o la reciente renovación, la característica anormal “dos balcones” podría ser la mejor explicación de por qué la casa es tan costosa. Las explicaciones son verdaderas. Las buenas explicaciones demuestran ser ciertas en la realidad (es decir, en otras situaciones). Pero inquietantemente, este no es el factor más importante para una “buena” explicación. Por ejemplo, la selectividad parece ser más importante que la veracidad. Una explicación que selecciona solo una o dos causas posibles rara vez cubre la lista completa de causas relevantes. La selectividad omite parte de la verdad. No es cierto que solo uno o dos factores, por ejemplo, hayan causado un colapso del mercado de valores: la verdad es que hay millones de causas que influyen en millones de personas para que actúen de tal manera que al final se causó un colapso. Lo que significa para el aprendizaje automático interpretable: La explicación debe predecir el evento con la mayor veracidad posible, que en el aprendizaje automático a veces se llama fidelidad. Entonces, si decimos que un segundo balcón aumenta el precio de una casa, eso también debería aplicarse a otras casas (o al menos a casas similares). Para los humanos, la fidelidad de una explicación no es tan importante como su selectividad, su contraste y su aspecto social. Las buenas explicaciones son consistentes con las creencias previas del explicado. Los humanos tienden a ignorar la información que es inconsistente con sus creencias anteriores. Este efecto se llama sesgo de confirmación (Nickerson 1998 11). Las explicaciones no se salvan de este tipo de sesgo. La gente tenderá a devaluar o ignorar explicaciones que no concuerden con sus creencias. El conjunto de creencias varía de persona a persona, pero también hay creencias previas basadas en grupos, como las cosmovisiones políticas. Lo que significa para el aprendizaje automático interpretable: Las buenas explicaciones son consistentes con las creencias anteriores. Esto es difícil de integrar en el aprendizaje automático y probablemente comprometería drásticamente el rendimiento predictivo. Nuestra creencia previa sobre el efecto del tamaño de la casa en el precio previsto es que cuanto más grande sea la casa, mayor será el precio. Supongamos que un modelo también muestra un efecto negativo del tamaño de la casa en el precio previsto para algunas casas. El modelo ha aprendido esto porque mejora el rendimiento predictivo (debido a algunas interacciones complejas), pero este comportamiento contradice fuertemente nuestras creencias anteriores. Puede aplicar restricciones de monotonicidad (una característica solo puede afectar la predicción en una dirección) o usar algo como un modelo lineal que tenga esta propiedad. Las buenas explicaciones son generales y probables. Una causa que puede explicar muchos eventos es muy general y podría considerarse una buena explicación. Ten en cuenta que esto contradice la afirmación de que las causas anormales son buenas explicaciones. A mi entender, las causas anormales superan a las causas generales. Las causas anormales son, por definición, raras en el escenario dado. En ausencia de un evento anormal, una explicación general se considera una buena explicación. También recuerda que las personas tienden a juzgar mal las probabilidades de eventos conjuntos. (Joe es bibliotecario. ¿Es más probable que sea una persona tímida o una persona tímida a la que le gusta leer libros?) Un buen ejemplo es “La casa es cara porque es grande”, lo cual es una buena explicación de por qué las casas son caras o baratas. Lo que significa para el aprendizaje automático interpretable: La generalidad se puede medir fácilmente con el soporte de la función, que es el número de instancias a las que se aplica la explicación dividido por el número total de instancias. Chapter 3 Conjuntos de datos A lo largo del libro, todos los modelos y técnicas se aplican a conjuntos de datos reales que están disponibles gratuitamente en línea. Utilizaremos diferentes conjuntos de datos para diferentes tareas: Clasificación, regresión y clasificación de textos. 3.1 Alquiler de bicicletas (Regresión) Este conjunto de datos contiene recuentos diarios de bicicletas alquiladas de la empresa de alquiler de bicicletas Capital-Bikeshare en Washington D.C., junto con información meteorológica y estacional. Los datos se pusieron a disposición de Capital-Bikeshare. Fanaee-T y Gama (2013)12 agregaron datos meteorológicos e información sobre la temporada. El objetivo es predecir cuántas bicicletas se alquilarán dependiendo del clima y el día. Los datos se pueden descargar del Depósito de aprendizaje automático de UCI. Se agregaron nuevas características al conjunto de datos y no se usaron todas las características originales para los ejemplos de este libro. Aquí está la lista de características que se utilizaron: Recuento de bicicletas, incluidos usuarios ocasionales y registrados. El recuento se utiliza como objetivo en la tarea de regresión. La temporada, ya sea primavera, verano, otoño o invierno. Indicador de si el día fue feriado o no. El año, 2011 o 2012. Número de días desde el 01.01.2011 (el primer día en el conjunto de datos). Esta característica se introdujo para tener en cuenta la tendencia a lo largo del tiempo. Indicador de si el día fue un día laboral o un fin de semana. La situación climática de ese día. Uno de: - despejado, pocas nubes, parcialmente nublado, nublado - niebla + nubes, niebla + nubes rotas, niebla + pocas nubes, niebla - nieve ligera, lluvia ligera + tormenta eléctrica + nubes dispersas, lluvia ligera + nubes dispersas - fuertes lluvias + paletas de hielo + tormenta eléctrica + niebla, nieve + niebla Temperatura en grados Celsius. Humedad relativa en porcentaje (0 a 100). Velocidad del viento en km por hora. Para los ejemplos en este libro, los datos han sido ligeramente procesados. Puede encontrar el script R de procesamiento en el repositorio de Github junto con el archivo RData final. 3.2 Comentarios de spam de YouTube (clasificación de texto) Como ejemplo de clasificación de texto, trabajamos con 1956 comentarios de 5 videos de YouTube diferentes. Afortunadamente, los autores que utilizaron este conjunto de datos en un artículo sobre clasificación de spam hicieron que los datos estén disponibles gratuitamente (Alberto, Lochter y Almeida (2015)13). Los comentarios se recopilaron a través de la API de YouTube de cinco de los diez videos más vistos en YouTube en el primer semestre de 2015. Los 5 son videos musicales. Uno de ellos es “Gangnam Style” del artista coreano Psy. Los otros artistas fueron Katy Perry, LMFAO, Eminem y Shakira. Revisa algunos de los comentarios. Los comentarios fueron etiquetados manualmente como spam o legítimos. El spam se codificó con un “1” y los comentarios legítimos con un “0”. CONTENT CLASS Huh, anyway check out this you[tube] channel: kobyoshi02 1 Hey guys check out my new channel and our first vid THIS IS US THE MONKEYS!!! I’m the monkey in the white shirt,please leave a like comment and please subscribe!!!! 1 just for test I have to say murdev.com 1 me shaking my sexy ass on my channel enjoy ^_^ 1 watch?v=vtaRGgvGtWQ Check this out . 1 Hey, check out my new website!! This site is about kids stuff. kidsmediausa . com 1 Subscribe to my channel 1 i turned it on mute as soon is i came on i just wanted to check the views… 0 You should check my channel for Funny VIDEOS!! 1 and u should.d check my channel and tell me what I should do next! 1 También puedes ir a YouTube y echar un vistazo a la sección de comentarios. Pero no te dejes atrapar en el infierno de YouTube, y por favor no termines viendo videos de monos robando y bebiendo cócteles de turistas en la playa. El detector de spam de Google también ha cambiado mucho desde 2015. Vea el video de ruptura de récords “Gangnam Style” aquí. Si deseas jugar con los datos, puedes encontrar el archivo RData junto con R -script con algunas funciones convenientes en el repositorio de Github del libro. 3.3 Factores de riesgo para el cáncer de cuello uterino (Clasificación) El conjunto de datos sobre el cáncer cervical contiene indicadores y factores de riesgo para predecir si una mujer tendrá cáncer cervical. Las características incluyen datos demográficos (como edad), estilo de vida e historial médico. Los datos se pueden descargar desde el repositorio de UCI Machine Learning y Fernandes, Cardoso y Fernandes lo describen. (2017)14. El subconjunto de características utilizadas en los ejemplos del libro son: Edad en años Número de parejas sexuales Primera relación sexual (edad en años) Número de embarazos Fumar o no Años fumando Anticonceptivos hormonales si o no Anticonceptivos hormonales (en años) Dispositivo intrauterino sí o no (DIU) Número de años con un dispositivo intrauterino (DIU) ¿Ha tenido alguna vez una enfermedad de transmisión sexual (ETS) sí o no? Número de diagnósticos de ETS Tiempo desde el primer diagnóstico de ETS Tiempo desde el último diagnóstico de ETS La biopsia resulta “Saludable” o “Cáncer”. Objetivo de clasificación. La biopsia sirve como estádndar para diagnosticar el cáncer cervical. Para los ejemplos en este libro, el resultado de la biopsia se utilizó como objetivo. Los valores faltantes para cada columna fueron imputados por la moda (valor más frecuente), que probablemente sea una mala solución, ya que la respuesta verdadera podría estar correlacionada con la probabilidad de que falte un valor. Probablemente hay un sesgo porque las preguntas son de naturaleza muy privada. Pero este no es un libro sobre la imputación de datos faltantes, por lo que la imputación por la moda tendrá que ser suficiente para los ejemplos. Para reproducir los ejemplos de este libro con este conjunto de datos, busque el preprocesamiento de script R y el archivo RData final en el repositorio de Github del libro. Friedman, Jerome, Trevor Hastie y Robert Tibshirani. “Los elementos del aprendizaje estadístico”. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).↩ “Definición de algoritmo”. https://www.merriam-webster.com/dictionary/algorithm. (2017)↩ Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017).↩ Doshi-Velez, Finale, y Been Kim. “Towards a rigorous science of interpretable machine learning,” nu. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017).↩ Heider, Fritz, y Marianne Simmel. “An experimental study of apparent behavior.” The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).↩ Lipton, Zachary C. “The mythos of model interpretability.” arXiv preprint arXiv:1606.03490, (2016).↩ Robnik-Sikonja, Marko, y Marko Bohanec. “Perturbation-based explanations of prediction models.” Human and Machine Learning. Springer, Cham. 159-175. (2018).↩ Lipton, Peter. “Contrastive explanation.” Royal Institute of Philosophy Supplements 27 (1990): 247-266.↩ Kahneman, Daniel, y Amos Tversky. “The Simulation Heuristic.” Stanford Univ CA Dept of Psychology. (1981).↩ Štrumbelj, Erik, y Igor Kononenko. “A general method for visualizing and explaining black-box regression models.” En International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).↩ Nickerson, Raymond S. “Confirmation Bias: A ubiquitous phenomenon in many guises.” Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).↩ Fanaee-T, Hadi y Joao Gama. “Etiquetado de eventos que combina detectores de conjunto y conocimientos básicos”. Progreso en Inteligencia Artificial. Springer Berlin Heidelberg, 1–15. doi: 10.1007 / s13748-013-0040-3. (2013)↩ Alberto, Túlio C, Johannes V Lochter y Tiago A Almeida. “Tubespam: filtro de spam de comentarios en YouTube”. En Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 138–43. IEEE (2015)↩ Fernandes, Kelwin, Jaime S Cardoso y Jessica Fernandes. “Transferir el aprendizaje con observabilidad parcial aplicada a la detección del cáncer cervical”. Conferencia ibérica sobre reconocimiento de patrones y análisis de imágenes, 243–50. Saltador. (2017)↩ "]
]
