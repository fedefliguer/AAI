<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.6 Explicaciones amigables para los humanos | Aprendizaje automatico interpretable</title>
  <meta name="description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="2.6 Explicaciones amigables para los humanos | Aprendizaje automatico interpretable" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.6 Explicaciones amigables para los humanos | Aprendizaje automatico interpretable" />
  
  <meta name="twitter:description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2020-03-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="properties.html"/>
<link rel="next" href="conjuntosdedatos.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159445204-1', 'https://fedefliguer.github.io/AAI/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="style.css+" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Bookdown Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="horadelcuento.html"><a href="horadelcuento.html"><i class="fa fa-check"></i><b>1.1</b> Hora del cuento</a><ul>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#un-rayo-nunca-golpea-dos-veces"><i class="fa fa-check"></i>Un rayo nunca golpea dos veces</a></li>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#perder-confianza"><i class="fa fa-check"></i>Perder confianza</a></li>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#clips-de-papel-de-fermi"><i class="fa fa-check"></i>Clips de papel de Fermi</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="que-es-el-aprendizaje-automatico.html"><a href="que-es-el-aprendizaje-automatico.html"><i class="fa fa-check"></i><b>1.2</b> ¿Qué es el aprendizaje automático?</a></li>
<li class="chapter" data-level="1.3" data-path="terminología.html"><a href="terminología.html"><i class="fa fa-check"></i><b>1.3</b> Terminología</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretabilidad.html"><a href="interpretabilidad.html"><i class="fa fa-check"></i><b>2</b> Interpretabilidad</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretabilidad-importancia.html"><a href="interpretabilidad-importancia.html"><i class="fa fa-check"></i><b>2.1</b> Importancia de la interpretabilidad</a></li>
<li class="chapter" data-level="2.2" data-path="taxonomia-de-los-metodos-de-interpretacion.html"><a href="taxonomia-de-los-metodos-de-interpretacion.html"><i class="fa fa-check"></i><b>2.2</b> Taxonomía de los métodos de interpretación</a></li>
<li class="chapter" data-level="2.3" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html"><i class="fa fa-check"></i><b>2.3</b> Alcance de la interpretabilidad</a><ul>
<li class="chapter" data-level="2.3.1" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#transparencia-del-algoritmo"><i class="fa fa-check"></i><b>2.3.1</b> Transparencia del algoritmo</a></li>
<li class="chapter" data-level="2.3.2" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-global-y-holistica-del-modelo"><i class="fa fa-check"></i><b>2.3.2</b> Interpretabilidad global y holística del modelo</a></li>
<li class="chapter" data-level="2.3.3" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-del-modelo-global-en-un-nivel-modular"><i class="fa fa-check"></i><b>2.3.3</b> Interpretabilidad del modelo global en un nivel modular</a></li>
<li class="chapter" data-level="2.3.4" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-local-para-una-unica-prediccion"><i class="fa fa-check"></i><b>2.3.4</b> Interpretabilidad local para una única predicción</a></li>
<li class="chapter" data-level="2.3.5" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-local-para-un-grupo-de-predicciones"><i class="fa fa-check"></i><b>2.3.5</b> Interpretabilidad local para un grupo de predicciones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="evaluacion-de-la-interpretabilidad.html"><a href="evaluacion-de-la-interpretabilidad.html"><i class="fa fa-check"></i><b>2.4</b> Evaluación de la interpretabilidad</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> Propiedades de las explicaciones</a></li>
<li class="chapter" data-level="2.6" data-path="amigables.html"><a href="amigables.html"><i class="fa fa-check"></i><b>2.6</b> Explicaciones amigables para los humanos</a><ul>
<li class="chapter" data-level="2.6.1" data-path="amigables.html"><a href="amigables.html#que-es-una-explicacion"><i class="fa fa-check"></i><b>2.6.1</b> ¿Qué es una explicación?</a></li>
<li class="chapter" data-level="2.6.2" data-path="amigables.html"><a href="amigables.html#buenaexplicación"><i class="fa fa-check"></i><b>2.6.2</b> ¿Qué es una buena explicación?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conjuntosdedatos.html"><a href="conjuntosdedatos.html"><i class="fa fa-check"></i><b>3</b> Conjuntos de datos</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Alquiler de bicicletas (Regresión)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> Comentarios de spam de YouTube (clasificación de texto)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> Factores de riesgo para el cáncer de cuello uterino (Clasificación)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje automatico interpretable</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="amigables" class="section level2">
<h2><span class="header-section-number">2.6</span> Explicaciones amigables para los humanos</h2>
<p>Profundicemos y descubramos lo que los humanos vemos como “buenas” explicaciones y cuáles son las implicaciones para el aprendizaje automático interpretable.
La investigación en humanidades puede ayudarnos a descubrirlo.
Miller (2017) ha realizado una gran encuesta de publicaciones sobre explicaciones, y este capítulo se basa en su resumen.</p>
<p>En este capítulo quiero convencerlo de lo siguiente:
Como explicación de un evento, los humanos prefieren explicaciones cortas (solo 1 o 2 causas) que contrastan la situación actual con una situación en la que el evento no hubiera ocurrido.
Las causas especialmente anormales proporcionan buenas explicaciones.
Las explicaciones son interacciones sociales entre el explicador y el explicado (receptor de la explicación) y, por lo tanto, el contexto social tiene una gran influencia en el contenido real de la explicación.</p>
<p>Cuando necesitas explicaciones con TODOS los factores para una predicción o comportamiento particular, no deseas una explicación amigable para los humanos, sino una atribución causal completa.
Probablemente desees una atribución causal si estás legalmente obligado a especificar todas las características influyentes o si depuras el modelo de aprendizaje automático.
En este caso, ignora los siguientes puntos.
En todos los demás casos, donde los ‘laicos’ o las personas con poco tiempo son los destinatarios de la explicación, las siguientes secciones deberían de serte interesantes.</p>
<div id="que-es-una-explicacion" class="section level3">
<h3><span class="header-section-number">2.6.1</span> ¿Qué es una explicación?</h3>
<p>Una explicación es la <strong>respuesta a una pregunta de por qué</strong> (Miller 2017).</p>
<ul>
<li>¿Por qué el tratamiento no funcionó en el paciente?</li>
<li>¿Por qué fue rechazado mi préstamo?</li>
<li>¿Por qué todavía no hemos sido contactados por la vida alienígena?</li>
</ul>
<p>Las dos primeras preguntas pueden responderse con una explicación “cotidiana”, mientras que la tercera proviene de la categoría “Fenómenos científicos más generales y preguntas filosóficas”.
Nos centramos en las explicaciones de tipo “cotidiano”, porque son relevantes para el aprendizaje automático interpretable.
Las preguntas que comienzan con “cómo” generalmente se pueden reformular como preguntas de “por qué”:
“¿Cómo se rechazó mi préstamo?” puede convertirse en “¿Por qué se rechazó mi préstamo?”.</p>
<p>A continuación, el término “explicación” se refiere al proceso social y cognitivo de explicación, pero también al producto de estos procesos.
El explicador puede ser un ser humano o una máquina.</p>
</div>
<div id="buenaexplicación" class="section level3">
<h3><span class="header-section-number">2.6.2</span> ¿Qué es una buena explicación?</h3>
<p>Esta sección condensa aún más el resumen de Miller sobre explicaciones “buenas” y agrega implicaciones concretas para el aprendizaje automático interpretable.</p>
<p><strong>Las explicaciones son contrastantes</strong> (Lipton 1990<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>).
Los humanos generalmente no preguntan por qué se hizo una determinada predicción, sino por qué se hizo esta predicción <em>en lugar de otra predicción</em>.
Tendemos a pensar en casos contrafácticos, es decir, “¿Cómo habría sido la predicción si la entrada X hubiera sido diferente?”.
Para una predicción del precio de la vivienda, el propietario podría estar interesado en saber por qué el precio previsto fue alto, en comparación con el precio más bajo que esperaba.
Si mi solicitud de préstamo es rechazada, no me importa escuchar todos los factores que generalmente hablan a favor o en contra de un rechazo.
Estoy interesado en los factores en mi solicitud que tendrían que cambiar para obtener el préstamo.
Quiero saber el contraste entre mi aplicación y la versión de mi solicitud que sería aceptada.
El reconocimiento de que las explicaciones contrastantes importan es un hallazgo importante para el aprendizaje automático explicable.
De la mayoría de los modelos interpretables, puede extraer una explicación que contrasta implícitamente una predicción de una instancia con la predicción de una instancia de datos artificiales o un promedio de instancias.
Los médicos podrían preguntar: “¿Por qué el medicamento no funcionó para mi paciente?”.
Y podrían querer una explicación que contraste a su paciente con un paciente para quien el medicamento funcionó y que sea similar al paciente que no responde.
Las explicaciones contrastantes son más fáciles de entender que explicaciones completas.
Una explicación completa de la pregunta del médico de por qué el medicamento no funciona puede incluir:
El paciente ha tenido la enfermedad durante 10 años, 11 genes se sobreexpresan, el cuerpo del paciente es muy rápido en descomponer el medicamento en químicos ineficaces.
Una explicación contrastante podría ser mucho más simple:
en contraste con el paciente que responde, el paciente que no responde tiene una cierta combinación de genes que hacen que el medicamento sea menos efectivo.
La mejor explicación es la que destaca la mayor diferencia entre el objeto de interés y el objeto de referencia.
<strong>Lo que significa para el aprendizaje automático interpretable</strong>:
los humanos no quieren una explicación completa para una predicción, sino comparar las diferencias con la predicción de otra observación (que puede ser artificial).
La creación de explicaciones contrastantes depende de la aplicación, porque requiere un punto de referencia para la comparación.
Y esto puede depender del punto de datos a explicar, pero también del usuario que recibe la explicación.
Un usuario de un sitio web de predicción del precio de la vivienda puede querer tener una explicación de una predicción del precio de la vivienda en contraste con su propia casa o tal vez con otra casa en el sitio web o tal vez con una casa promedio en el vecindario.
La solución para la creación automatizada de explicaciones contrastantes también podría implicar la búsqueda de prototipos o arquetipos en los datos.</p>
<p><strong>Las explicaciones se seleccionan</strong>.
La gente no espera explicaciones que cubran la lista real y completa de causas de un evento.
Estamos acostumbrados a seleccionar una o dos causas de una variedad de causas posibles como LA explicación.
Como prueba, encienda las noticias de TV: “El descenso en los precios de las acciones se atribuye a una creciente reacción contra el producto de la compañía debido a problemas con la última actualización de software”.
“Tsubasa y su equipo perdieron el partido debido a una defensa débil: dieron a sus oponentes demasiado espacio para desarrollar su estrategia”.
“La creciente desconfianza de las instituciones establecidas y nuestro gobierno son los principales factores que han reducido la participación electoral”.
El hecho de que un evento puede explicarse por varias causas se llama Efecto Rashomon.
Rashomon es una película japonesa que cuenta historias alternativas y contradictorias (explicaciones) sobre la muerte de un samurai.
Para los modelos de aprendizaje automático, es ventajoso si se puede hacer una buena predicción a partir de diferentes características.
Los métodos de conjunto que combinan múltiples modelos con diferentes características (diferentes explicaciones) generalmente funcionan bien porque promediar esas “historias” hace que las predicciones sean más sólidas y precisas.
Pero también significa que hay más de una explicación selectiva de por qué se hizo una determinada predicción.
<strong>Lo que significa para el aprendizaje automático interpretable</strong>:
Haga la explicación muy breve, dé solo 1 a 3 razones, incluso si el mundo es más complejo.
El <a href="#lime">método LIME</a> hace un buen trabajo con esto.</p>
<p><strong>Las explicaciones son sociales</strong>.
Son parte de una conversación o interacción entre el explicador y el receptor de la explicación.
El contexto social determina el contenido y la naturaleza de las explicaciones.
Si quisiera explicarle a una persona técnica por qué las criptomonedas digitales valen tanto, diría cosas como:
“La contabilidad descentralizada, distribuida y basada en blockchain, que no puede ser controlado por una entidad central, resuena con las personas que desean asegurarse su riqueza, lo que explica la alta demanda y el precio”.
Pero a mi abuela le diría:
“Mira, abuela: las criptomonedas son un poco como el oro de la computadora. A la gente le gusta y paga mucho por el oro, y a los jóvenes les gusta y pagan mucho por el oro de la computadora”.
<strong>Lo que significa para el aprendizaje automático interpretable</strong>:
Presta atención al entorno social de su aplicación de aprendizaje automático y al público objetivo.
Obtener la parte social del modelo de aprendizaje automático correcto depende completamente de su aplicación específica.
Encuentra expertos de las humanidades (por ejemplo, psicólogos y sociólogos) para que te ayuden.</p>
<p><strong>Las explicaciones se centran en lo anormal</strong>.
Las personas se enfocan más en causas anormales para explicar los eventos (Kahnemann y Tversky, 1981<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>).
Estas son causas que tenían una pequeña probabilidad pero que, sin embargo, ocurrieron.
La eliminación de estas causas anormales habría cambiado mucho el resultado (explicación contrafáctica).
Los humanos consideran este tipo de causas “anormales” como buenas explicaciones.
Un ejemplo de Štrumbelj y Kononenko (2011)<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> es:
Supongamos que tenemos un conjunto de datos de situaciones de prueba entre profesores y alumnos.
Los estudiantes asisten a un curso y lo aprueban directamente después de una presentación exitosa.
El maestro tiene la opción de hacer preguntas adicionales al alumno para evaluar su conocimiento.
Los estudiantes que no puedan responder estas preguntas reprobarán el curso.
Los estudiantes pueden tener diferentes niveles de preparación, lo que se traduce en diferentes probabilidades de responder correctamente las preguntas del maestro (si deciden evaluar al estudiante).
Queremos predecir si un alumno aprobará el curso y explicar nuestra predicción.
La posibilidad de aprobar es del 100% si el maestro no hace preguntas adicionales; de lo contrario, la probabilidad de aprobar depende del nivel de preparación del alumno y la probabilidad resultante de responder las preguntas correctamente.
Escenario 1: el maestro generalmente hace preguntas adicionales a los estudiantes (por ejemplo, 95 de cada 100 veces).
Un estudiante que no estudió (10% de posibilidades de aprobar la parte de la pregunta) no fue uno de los afortunados y recibe preguntas adicionales que no responde correctamente.
¿Por qué el alumno reprobó el curso? Yo diría que fue culpa del estudiante por no estudiar.
Escenario 2: el profesor rara vez hace preguntas adicionales (por ejemplo, 2 de cada 100 veces).
Para un estudiante que no ha estudiado las preguntas, predeciríamos una alta probabilidad de aprobar el curso porque las preguntas son poco probables.
Por supuesto, uno de los estudiantes no se preparó para las preguntas, lo que le da un 10% de posibilidades de aprobar las preguntas.
No tiene suerte y el profesor hace preguntas adicionales que el alumno no puede responder y no aprueba el curso.
¿Cuál es la razón del fracaso?
Yo diría que ahora, la mejor explicación es “porque el profesor evaluó al alumno”.
Era poco probable que el maestro hiciera la prueba, por lo que se comportó de manera anormal.
<strong>Lo que significa para el aprendizaje automático interpretable</strong>:
Si una de las características de entrada para una predicción fue anormal en algún sentido (como una categoría rara de una característica categórica) y la característica influyó en la predicción, debe incluirse en una explicación, incluso si otras características ‘normales’ tienen la misma influencia en la predicción que la anormal.
Una característica anormal en nuestro ejemplo de predicción del precio de la vivienda podría ser que una vivienda bastante cara tiene dos balcones.
Incluso si algún método de atribución determina que los dos balcones contribuyen tanto a la diferencia de precio como el tamaño promedio de la casa, el vecindario bueno o la reciente renovación, la característica anormal “dos balcones” podría ser la mejor explicación de por qué la casa es tan costosa.</p>
<p><strong>Las explicaciones son verdaderas</strong>.
Las buenas explicaciones demuestran ser ciertas en la realidad (es decir, en otras situaciones).
Pero inquietantemente, este no es el factor más importante para una “buena” explicación.
Por ejemplo, la selectividad parece ser más importante que la veracidad.
Una explicación que selecciona solo una o dos causas posibles rara vez cubre la lista completa de causas relevantes.
La selectividad omite parte de la verdad.
No es cierto que solo uno o dos factores, por ejemplo, hayan causado un colapso del mercado de valores: la verdad es que hay millones de causas que influyen en millones de personas para que actúen de tal manera que al final se causó un colapso.
<strong>Lo que significa para el aprendizaje automático interpretable</strong>:
La explicación debe predecir el evento con la mayor veracidad posible, que en el aprendizaje automático a veces se llama <strong>fidelidad</strong>.
Entonces, si decimos que un segundo balcón aumenta el precio de una casa, eso también debería aplicarse a otras casas (o al menos a casas similares).
Para los humanos, la fidelidad de una explicación no es tan importante como su selectividad, su contraste y su aspecto social.</p>
<p><strong>Las buenas explicaciones son consistentes con las creencias previas del explicado</strong>.
Los humanos tienden a ignorar la información que es inconsistente con sus creencias anteriores.
Este efecto se llama sesgo de confirmación (Nickerson 1998 <a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>).
Las explicaciones no se salvan de este tipo de sesgo.
La gente tenderá a devaluar o ignorar explicaciones que no concuerden con sus creencias.
El conjunto de creencias varía de persona a persona, pero también hay creencias previas basadas en grupos, como las cosmovisiones políticas.
<strong>Lo que significa para el aprendizaje automático interpretable</strong>:
Las buenas explicaciones son consistentes con las creencias anteriores.
Esto es difícil de integrar en el aprendizaje automático y probablemente comprometería drásticamente el rendimiento predictivo.
Nuestra creencia previa sobre el efecto del tamaño de la casa en el precio previsto es que cuanto más grande sea la casa, mayor será el precio.
Supongamos que un modelo también muestra un efecto negativo del tamaño de la casa en el precio previsto para algunas casas.
El modelo ha aprendido esto porque mejora el rendimiento predictivo (debido a algunas interacciones complejas), pero este comportamiento contradice fuertemente nuestras creencias anteriores.
Puede aplicar restricciones de monotonicidad (una característica solo puede afectar la predicción en una dirección) o usar algo como un modelo lineal que tenga esta propiedad.</p>
<p><strong>Las buenas explicaciones son generales y probables</strong>.
Una causa que puede explicar muchos eventos es muy general y podría considerarse una buena explicación.
Ten en cuenta que esto contradice la afirmación de que las causas anormales son buenas explicaciones.
A mi entender, las causas anormales superan a las causas generales.
Las causas anormales son, por definición, raras en el escenario dado.
En ausencia de un evento anormal, una explicación general se considera una buena explicación.
También recuerda que las personas tienden a juzgar mal las probabilidades de eventos conjuntos.
(Joe es bibliotecario. ¿Es más probable que sea una persona tímida o una persona tímida a la que le gusta leer libros?)
Un buen ejemplo es “La casa es cara porque es grande”, lo cual es una buena explicación de por qué las casas son caras o baratas.
<strong>Lo que significa para el aprendizaje automático interpretable</strong>:
La generalidad se puede medir fácilmente con el soporte de la función, que es el número de instancias a las que se aplica la explicación dividido por el número total de instancias.</p>

</div>
</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>Lipton, Peter. “Contrastive explanation.” Royal Institute of Philosophy Supplements 27 (1990): 247-266.<a href="amigables.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Kahneman, Daniel, y Amos Tversky. “The Simulation Heuristic.” Stanford Univ CA Dept of Psychology. (1981).<a href="amigables.html#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>Štrumbelj, Erik, y Igor Kononenko. “A general method for visualizing and explaining black-box regression models.” En International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).<a href="amigables.html#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Nickerson, Raymond S. “Confirmation Bias: A ubiquitous phenomenon in many guises.” Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).<a href="amigables.html#fnref11" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="properties.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conjuntosdedatos.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
