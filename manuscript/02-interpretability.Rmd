```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
```

# Interpretabilidad {#interpretabilidad}

No existe una definición matemática de interpretabilidad. 
Una definición (no matemática) que me gusta de Miller (2017)[^Miller2017] es: 
**Interpretabilidad es el grado en que un humano puede comprender la causa de una decisión.**
Otra es: 
**Interpretabilidad es el grado a lo que un humano puede predecir constantemente el resultado del modelo**[^crítica]. 
Cuanto mayor sea la interpretabilidad de un modelo de aprendizaje automático, más fácil será para alguien comprender por qué se han tomado ciertas decisiones o predicciones.
Un modelo es más interpretable que otro si sus decisiones son más fáciles de comprender para un humano que las de otro modelo. 
Usaré los términos interpretable y explicable en forma indistinta. 
Al igual que Miller (2017), creo que tiene sentido distinguir entre los términos interpretabilidad / explicabilidad y explicación. 
Usaré "explicación" para explicaciones de predicciones individuales. 
Ve la [sección sobre explicaciones](#explicación) para aprender lo que los humanos vemos como una buena explicación. 

## Importancia de la interpretabilidad {#interpretabilidad-importancia} 

Si un modelo de aprendizaje automático funciona bien, **¿por qué no confiamos en el modelo** e ignoramos **por qué** tomó una determinada decisión? 
"El problema es que una sola métrica, como la precisión de la clasificación, es una descripción incompleta de la mayoría de las tareas del mundo real". (Doshi-Velez y Kim 2017[^Doshi2017])

Profundicemos en las razones por las que la interpretabilidad es tan importante. 
Cuando se trata de modelado predictivo, debe hacer una compensación: 
¿solo desea saber **qué** se predice? 
Por ejemplo, la probabilidad de que un cliente abandone un servicio o qué tan efectivo será un medicamento para un paciente. 
¿O quieres saber **por qué** se hizo la predicción y posiblemente pagar la interpretabilidad con una caída en el rendimiento predictivo? 
En algunos casos, no le importa por qué se tomó una decisión, es suficiente saber que el rendimiento predictivo en un conjunto de datos de prueba fue bueno. 
Pero en otros casos, conocer el 'por qué' puede ayudarlo a aprender más sobre el problema, los datos y la razón por la cual un modelo puede fallar. 
Es posible que algunos modelos no requieran explicaciones porque se usan en un entorno de bajo riesgo, lo que significa que un error no tendrá consecuencias graves (por ejemplo, un sistema de recomendación de películas) o que el método ya ha sido ampliamente estudiado y evaluado (por ejemplo, reconocimiento óptico de caracteres).
La necesidad de interpretabilidad surge de una incompletitud en la formalización del problema (Doshi-Velez y Kim 2017), lo que significa que para ciertos problemas o tareas no es suficiente obtener la predicción (el **qué**). 
El modelo también debe explicar cómo llegó a la predicción (el **por qué**), porque una predicción correcta solo resuelve parcialmente su problema original.
Las siguientes razones impulsan la demanda de interpretabilidad y explicaciones (Doshi-Velez y Kim 2017 y Miller 2017). 

**Curiosidad y aprendizaje humanos**: los humanos tienen un modelo mental de su entorno que se actualiza cuando ocurre algo inesperado.
Esta actualización se realiza buscando una explicación para el evento inesperado.
Por ejemplo, un humano se siente enfermo y pregunta: "¿Por qué me siento tan enfermo?".
Se entera de que se enferma cada vez que come frutos rojos. Actualiza su modelo mental y decide que esos frutos causaron la enfermedad y, por lo tanto, deben evitarse. 
Cuando se usan modelos opacos de aprendizaje automático en la investigación, los hallazgos científicos permanecen completamente ocultos si el modelo solo da predicciones sin explicaciones.
Para facilitar el aprendizaje y satisfacer la curiosidad de por qué ciertas predicciones o comportamientos son creados por máquinas, la interpretación y las explicaciones son cruciales. 
Por supuesto, los humanos no necesitan explicaciones para todo lo que sucede.
Para la mayoría de las personas, no hay problemas en no entender cómo funciona una computadora.
Eventos inesperados nos hacen curiosos.
Por ejemplo: ¿Por qué mi computadora se apaga inesperadamente?


Estrechamente relacionado con el aprendizaje está el deseo humano de **encontrar significado en el mundo**. 
Queremos armonizar las contradicciones o inconsistencias entre los elementos de nuestras estructuras de conocimiento. 
"¿Por qué mi perro me mordió a pesar de que nunca antes lo había hecho?" Un humano podría preguntar. 
Existe una contradicción entre el conocimiento del comportamiento pasado del perro y la experiencia desagradable recién hecha de la mordedura.
La explicación del veterinario concilia la contradicción del dueño del perro: 
"El perro estaba estresado y mordido". 
Cuanto más la decisión de una máquina afecta la vida de una persona, más importante es que la máquina explique su comportamiento. 
Si un modelo de aprendizaje automático rechaza una solicitud de préstamo, esto puede ser completamente inesperado para los solicitantes. 
Solo pueden conciliar esta inconsistencia entre la expectativa y la realidad con algún tipo de explicación. 
En realidad, las explicaciones no tienen que explicar completamente la situación, sino que deben abordar una causa principal. 
Otro ejemplo es la recomendación algorítmica del producto. 
Personalmente, siempre pienso en por qué ciertos productos o películas me han sido recomendados algorítmicamente. 
A menudo es bastante claro: 
la publicidad me sigue en Internet porque recientemente compré una lavadora, y sé que en los próximos días me seguirán anuncios de lavadoras. 
Sí, tiene sentido sugerir guantes si ya tengo un gorro de nieve en mi carrito de compras. 
El algoritmo recomienda esta película, porque los usuarios a quienes les gustaron otras películas que me gustaron también disfrutaron la película recomendada. 
Cada vez más, las compañías de Internet están agregando explicaciones a sus recomendaciones. 
Un buen ejemplo es la recomendación de productos de Amazon, que se basa en combinaciones de productos que se compran con frecuencia: 

```{r amazon-recommendation, fig.cap='Productos recomendados cuando se compra pintura en Amazon.', out.width=500}
knitr::include_graphics("images/amazon-freq-bought-together.png")
```


En muchas disciplinas científicas hay un cambio de métodos cualitativos a cuantitativos (por ejemplo, sociología, psicología), y también hacia el aprendizaje automático (biología, genómica)
El **objetivo de la ciencia** es obtener conocimiento, pero muchos problemas se resuelven con grandes conjuntos de datos y modelos de aprendizaje automático de caja negra.
El modelo en sí se convierte en la fuente de conocimiento en lugar de los datos.
La interpretabilidad hace posible extraer este conocimiento adicional capturado por el modelo.


Los modelos de aprendizaje automático asumen tareas del mundo real que requieren **medidas de seguridad** y pruebas. 
Imagina que un automóvil autónomo detecta automáticamente a los ciclistas en función de un sistema de aprendizaje profundo.
Deseas estar 100% seguro de que la abstracción que ha aprendido el sistema está libre de errores, porque atropellar a los ciclistas es bastante malo.
Una explicación podría revelar que la característica aprendida más importante es reconocer las dos ruedas de una bicicleta, pero existen casos de borde, como bicicletas con bolsas laterales que cubren parcialmente las ruedas. 


Por defecto, los modelos de aprendizaje automático recogen sesgos de los datos de entrenamiento.
Esto puede convertir sus modelos de aprendizaje automático en racistas que discriminan determinados grupos.
La interpretabilidad es una herramienta de depuración útil para **detectar sesgos** en modelos de aprendizaje automático.
Puede suceder que el modelo de aprendizaje automático que usted haya entrenado para la aprobación automática o el rechazo de las solicitudes de crédito discrimine a una minoría.
Su objetivo principal es otorgar préstamos solo a personas que eventualmente los pagarán.
Lo incompleto de la formulación del problema en este caso radica en el hecho de que no solo desea minimizar los impagos de préstamos, sino que también está obligado a no discriminar sobre la base de ciertos datos demográficos.
Esta es una restricción adicional que forma parte de la formulación de su problema (otorgar préstamos de manera riesgosa y conforme) que no está cubierta por la función de pérdida para la que se optimizó el modelo de aprendizaje automático. 


El proceso de integración de máquinas y algoritmos en nuestra vida diaria requiere interpretabilidad para aumentar la **aceptación social**.
Las personas atribuyen creencias, deseos, intenciones, etc. a los objetos.
En un famoso experimento, Heider y Simmel (1944)[^Heider] mostraron a los participantes videos de formas en las que un círculo abría una "puerta" para ingresar a una "habitación" (que era simplemente un rectángulo).
Los participantes describieron las acciones de las formas como describirían las acciones de un agente humano, asignando intenciones e incluso emociones y rasgos de personalidad a las formas. 
Los robots son un buen ejemplo, como mi aspiradora, a la que llamé "Doge".
Si Doge se atasca, pienso:
"Doge quiere seguir limpiando, pero me pide ayuda porque se atascó".
Más tarde, cuando Doge termina de limpiar y busca en la base de operaciones para recargar, pienso:
"Doge desea recargar y tiene la intención de encontrar la base de operaciones".
También atribuyo rasgos de personalidad:
"Doge es un poco tonto, pero de una manera linda".
Estos son mis pensamientos, especialmente cuando descubro que Doge ha derribado una planta mientras aspiraba la casa. 
Una máquina o algoritmo que explica sus predicciones encontrará más aceptación.
Véase también el [capítulo sobre explicaciones](#explicación), que argumenta que las explicaciones son un proceso social.


Las explicaciones se utilizan para **gestionar las interacciones sociales**.
Al crear un significado compartido de algo, el explicador influye en las acciones, emociones y creencias del receptor de la explicación.
Para que una máquina interactúe con nosotros, puede que tenga que moldear nuestras emociones y creencias.
Las máquinas tienen que "persuadirnos" para que puedan lograr su objetivo. 
No aceptaría completamente mi robot aspirador si no explicara su comportamiento, al menos hasta cierto punto.
La aspiradora crea un significado compartido de, por ejemplo, un "accidente" (como quedarse atascado en la alfombra del baño ... otra vez) al explicar que se atascó en lugar de simplemente detenerse a trabajar sin comentarios.
Curiosamente, puede haber un desalineamiento entre el objetivo de la máquina explicadora (crear confianza) y el objetivo del destinatario (comprender la predicción o el comportamiento).
Quizás la explicación completa de por qué Doge se atascó podría ser que la batería estaba muy baja, que una de las ruedas no funciona correctamente y que hay un error que hace que el robot vaya al mismo lugar una y otra vez a pesar de que había un obstaculo.
Estas razones (y algunas más) hicieron que el robot se atascara, aunque algo estaba en el camino, y eso fue suficiente para que confiara en su comportamiento y se produjera el accidente.
Por cierto, Doge se quedó atascado en el baño nuevamente.
Tenemos que quitar las alfombras cada vez antes de dejar que Doge aspire. 

```{r doge-stuck, fig.cap="Doge, nuestra aspiradora, se atascó. Como explicación del accidente, Doge nos dijo que debe estar en una superficie plana.", out.width=800}
knitr::include_graphics("images/doge-stuck.jpg")
```

Los modelos de aprendizaje automático solo se pueden **depurar y auditar** cuando se pueden interpretar. 
Incluso en entornos de bajo riesgo, como las recomendaciones de películas, la capacidad de interpretación es valiosa en la fase de investigación y desarrollo, así como después de la implementación. 
Más tarde, cuando se usa un modelo en un producto, las cosas pueden salir mal.
Una interpretación para una predicción errónea ayuda a comprender la causa del error.
Ofrece una dirección sobre cómo arreglar el sistema.
Considere un ejemplo de un clasificador entre perros siberianos y lobos, que clasifica erróneamente a algunos siberianos como lobos.
Al utilizar métodos de aprendizaje automático interpretables, descubrirá que la clasificación errónea se debió a la nieve en la imagen.
El clasificador aprendió a usar la nieve como una característica para clasificar las imágenes como "lobo", lo que podría tener sentido en términos de separar a los lobos de los siberianos en el conjunto de datos de entrenamiento, pero no en el uso en el mundo real. 

Si puede asegurarse de que el modelo de aprendizaje automático pueda explicar las decisiones, también puede verificar los siguientes rasgos con mayor facilidad (Doshi-Velez y Kim 2017): 

- Equidad: garantizar que las predicciones sean imparciales y no discriminen implícita o explícitamente a ciertos grupos.
Un modelo interpretable puede decirle por qué ha decidido que cierta persona no debería obtener un préstamo, y es más fácil para un humano juzgar si la decisión se basa en un sesgo demográfico aprendido (por ejemplo, racial).
- Privacidad: garantizar que la información confidencial de los datos esté protegida. 
- Fiabilidad o robustez: garantizar que pequeños cambios en la entrada no conduzcan a grandes cambios en la predicción. 
- Causalidad: compruebe que solo se recogen las relaciones causales. 
- Confianza: es más fácil para los humanos confiar en un sistema que explica sus decisiones en comparación con una caja negra. 

**Cuando no necesitamos interpretabilidad.** 

Los siguientes escenarios ilustran cuando no necesitamos o incluso no queremos la interpretabilidad de los modelos de aprendizaje automático. 

La interpretabilidad no es necesaria si el modelo **no tiene un impacto significativo**.
Imagine a alguien llamado Mike trabajando en un proyecto paralelo de aprendizaje automático para predecir a dónde irán sus amigos para sus próximas vacaciones en base a datos de Facebook.
A Mike le gusta sorprender a sus amigos con suposiciones educadas sobre dónde irán de vacaciones.
No hay ningún problema real si el modelo está equivocado (en el peor de los casos, solo un poco de vergüenza para Mike), ni hay un problema si Mike no puede explicar el resultado de su modelo.
Está perfectamente bien no tener interpretabilidad en este caso. 
La situación cambiaría si Mike comenzara a construir un negocio en torno a estas predicciones de destinos de vacaciones. 
Si el modelo está equivocado, el negocio podría perder dinero, o el modelo podría funcionar peor para algunas personas debido al prejuicio racial aprendido.
Tan pronto como el modelo tenga un impacto significativo, ya sea financiero o social, la interpretabilidad se vuelve relevante. 

La interpretabilidad no es necesaria cuando el **problema está bien estudiado**.
Algunas aplicaciones se han estudiado lo suficientemente bien como para que haya suficiente experiencia práctica con el modelo y los problemas con el modelo se hayan resuelto con el tiempo.
Un buen ejemplo es un modelo de aprendizaje automático para el reconocimiento óptico de caracteres que procesa imágenes de sobres y extrae direcciones.
Hay años de experiencia con estos sistemas y está claro que funcionan.
Además, no estamos realmente interesados en obtener información adicional sobre la tarea en cuestión. 

La interpretabilidad podría permitir a las personas o programas **manipular el sistema**.
Los problemas con los usuarios que engañan a un sistema son el resultado de una falta de coincidencia entre los objetivos del creador y el usuario de un modelo.
La calificación crediticia es un sistema de este tipo porque los bancos quieren asegurarse de que los préstamos solo se otorguen a los solicitantes que puedan devolverlos, y los solicitantes aspiran a obtener el préstamo incluso si el banco no quiere darles uno.
Este desajuste entre los objetivos introduce incentivos para que los solicitantes jueguen con el sistema para aumentar sus posibilidades de obtener un préstamo. 
Si un solicitante sabe que tener más de dos tarjetas de crédito afecta negativamente su puntaje, simplemente devuelve su tercera tarjeta de crédito para mejorar su puntaje y solicita una nueva tarjeta después de que el préstamo haya sido aprobado.
Si bien su puntaje mejoró, la probabilidad real de pagar el préstamo se mantuvo sin cambios. 
El sistema solo se puede manipular si las entradas son representantes de una característica causal, pero en realidad no causan el resultado.
Siempre que sea posible, se deben evitar las funciones de proxy ya que hacen que los modelos sean manipulables.
Por ejemplo, Google desarrolló un sistema llamado Google Flu Trends para predecir los brotes de gripe.
El sistema correlacionó las búsquedas de Google con los brotes de gripe, y ha tenido un mal desempeño.
La distribución de las consultas de búsqueda cambió y Google Flu Trends perdió muchos brotes de gripe.
Las búsquedas en Google no causan la gripe.
Cuando las personas buscan síntomas como "fiebre", se trata simplemente de una correlación con los brotes de gripe reales.
Idealmente, los modelos solo usarían características causales porque no serían manipulables. 

<!--{pagebreak}-->

## Taxonomía de los métodos de interpretación 

Los métodos para la interpretación de aprendizaje automático se pueden clasificar de acuerdo con varios criterios. 

**¿Intrínseco o post hoc?** 
Este criterio distingue si la interpretabilidad se logra restringiendo la complejidad del modelo de aprendizaje automático (intrínseco) o aplicando métodos que analizan el modelo después del entrenamiento (post hoc). 
La interpretabilidad intrínseca se refiere a modelos de aprendizaje automático que se consideran interpretables debido a su estructura simple, como árboles de decisión cortos o modelos lineales dispersos. 
La interpretabilidad post hoc se refiere a la aplicación de métodos de interpretación después del entrenamiento modelo.
La importancia de la característica de permutación es, por ejemplo, un método de interpretación post hoc.
Los métodos post hoc también se pueden aplicar a modelos intrínsecamente interpretables.
Por ejemplo, la importancia de la característica de permutación se puede calcular para los árboles de decisión.
La organización de los capítulos de este libro está determinada por la distinción entre [modelos intrínsecamente interpretables] (#simple) y [métodos de interpretación post hoc (y modelo-agnósticos)] (#agnóstico). 

**Resultado del método de interpretación** 
Los diversos métodos de interpretación pueden diferenciarse aproximadamente de acuerdo con sus resultados. 

- **Estadística de resumen de características**:
Muchos métodos de interpretación proporcionan estadísticas de resumen para cada covariable.
Algunos métodos devuelven un solo número por característica, como la importancia de la característica, o un resultado más complejo, como las fortalezas de interacción de features por pares.
- **Visualización de resumen de características**:
La mayoría de las estadísticas de resumen de características también se pueden visualizar.
Algunos resúmenes de características en realidad solo tienen sentido si se visualizan y una tabla sería una elección incorrecta.
La dependencia parcial de una característica es tal caso.
Las gráficas de dependencia parcial son curvas que muestran una característica y el resultado promedio pronosticado. La mejor manera de presentar dependencias parciales es dibujar la curva en lugar de imprimir las coordenadas. 
- **Elementos internos del modelo (p. Ej., Pesos aprendidos)**: 
La interpretación de modelos intrínsecamente interpretables entra en esta categoría.
Algunos ejemplos son los pesos en modelos lineales o la estructura de árbol aprendida (las características y los umbrales utilizados para las divisiones) de los árboles de decisión.
Las líneas se desdibujan entre las partes internas del modelo y la estadística de resumen de características en, por ejemplo, modelos lineales, porque los pesos son tanto las partes internas del modelo como las estadísticas de resumen de las características al mismo tiempo.
Otro método que genera modelos internos es la visualización de detectores de características aprendidos en redes neuronales convolucionales. Los métodos de interpretación que generan elementos internos del modelo son, por definición, específicos del modelo (consulte el siguiente criterio). 
- **Punto de datos **: 
Esta categoría incluye todos los métodos que devuelven observaciones (ya existentes o recién creados) para hacer que un modelo sea interpretable.
Un método se llama explicaciones contrafácticas.
Para explicar la predicción de una instancia de datos, el método encuentra una observación similar al cambiar algunas de las características para las cuales el resultado predicho cambia de manera relevante.
Otro ejemplo es la identificación de prototipos de clases predichas.
Para ser útiles, los métodos de interpretación que generan nuevos puntos de datos requieren que los propios puntos de datos puedan ser interpretados.
Esto funciona bien para imágenes y textos, pero es menos útil para datos tabulares con cientos de características. 
- **Modelo intrínsecamente interpretable**:
Una solución para interpretar modelos de caja negra es aproximarlos (global o localmente) con un modelo interpretable.
El modelo interpretable en sí mismo se interpreta mirando los parámetros internos del modelo o las estadísticas de resumen de sus características. 


**Modelo específico o modelo agnóstico?**
Las herramientas de interpretación modelo-específicas están limitadas a esos modelos.
La interpretación de los pesos de regresión en un modelo lineal es una interpretación de este tipo ya que, por definición, siempre es específica del modelo.
Herramientas que solo funcionan para la interpretación de, por ejemplo, las redes neuronales son específicas del modelo.
Las herramientas independientes del modelo se pueden usar en cualquier modelo de aprendizaje automático y se aplican después de que el modelo haya sido entrenado (post hoc). 
Estos métodos generalmente funcionan mediante el análisis de pares de entrada y salida de características.
Por definición, estos métodos no pueden tener acceso a los modelos internos, como los pesos o la información estructural. 

**¿Local o global?**
¿El método de interpretación explica una predicción individual o el comportamiento completo del modelo?
¿O algún punto intermedio? Lea más sobre el criterio de alcance en la siguiente sección. 

<!--{pagebreak}-->

## Alcance de la interpretabilidad 

Un algoritmo entrena un modelo que produce las predicciones.
Cada paso puede evaluarse en términos de transparencia o interpretabilidad. 

### Transparencia del algoritmo 
*¿Cómo crea el algoritmo el modelo?* 

La transparencia del algoritmo se trata de cómo el algoritmo aprende un modelo desde los datos, y de qué tipo de relaciones puede incorporar.
Si utilizas redes neuronales convolucionales para clasificar imágenes, puedes explicar que el algoritmo aprende detectores de borde y filtros en las capas más bajas.
Esto es una comprensión de cómo funciona el algoritmo, pero no para el modelo específico que se aprende al final, y tampoco para la forma en la que se hacen las predicciones individuales.
La transparencia del algoritmo solo requiere el conocimiento del algoritmo y no de los datos o el modelo aprendido.
Este libro se centra en la interpretabilidad del modelo y no en la transparencia del algoritmo.
Algoritmos como el método de mínimos cuadrados para modelos lineales están bien estudiados y entendidos.
Se caracterizan por una alta transparencia.
Los enfoques de aprendizaje profundo (empujar un gradiente a través de una red con millones de pesos) se entienden menos y el funcionamiento interno es el foco de la investigación en curso.
Se consideran menos transparentes. 

### Interpretabilidad global y holística del modelo 
*¿Cómo hace predicciones el modelo entrenado?* 

Podrías describir un modelo como interpretable si puedes comprender todo el modelo de una vez (Lipton 2016[^Lipton2016]).
Para explicar el resultado del modelo global necesitas el modelo entrenado, el conocimiento del algoritmo y los datos.
Este nivel de interpretabilidad se trata de comprender cómo toma decisiones el modelo, en función de una visión holística de sus características y de cada uno de los componentes aprendidos, como los pesos, parámetros y estructuras.
¿Qué características son importantes y qué tipo de interacciones entre ellas tienen lugar? 
La interpretación global del modelo ayuda a comprender la distribución de su resultado objetivo en función de las características.
La interpretabilidad del modelo global es muy difícil de lograr en la práctica.
Es improbable que cualquier modelo que exceda un puñado de parámetros o pesos se ajuste a la memoria a corto plazo del ser humano promedio.
Sostengo que realmente no puedes imaginar un modelo lineal con 5 características, porque significaría dibujar mentalmente el hiperplano estimado en un espacio de 5 dimensiones.
Cualquier espacio de características con más de 3 dimensiones es simplemente inconcebible para los humanos.
Por lo general, cuando las personas intentan comprender un modelo, solo consideran partes de él, como los pesos en los modelos lineales. 

### Interpretabilidad del modelo global en un nivel modular 
*¿Cómo afectan las predicciones las partes del modelo?* 

Un modelo de Naive Bayes con cientos de características sería demasiado grande para mantenerlo en nuestra memoria de trabajo.
E incluso si logramos memorizar todos los pesos, no podríamos hacer predicciones rápidamente para nuevas observaciones.
Además, debes tener la distribución conjunta de todas las características en su cabeza para estimar la importancia de cada característica y cómo las características afectan las predicciones en promedio.
Una tarea imposible.
Pero puedes entender fácilmente un solo peso. 
Si bien la interpretación global del modelo generalmente está fuera del alcance, existe una buena posibilidad de comprender al menos algunos modelos a nivel modular. 
No todos los modelos son interpretables a nivel de parámetro.
Para los modelos lineales, las partes interpretables son los pesos, para los árboles serían las divisiones (características seleccionadas más puntos de corte) y las predicciones de los nodos foliares.
Los modelos lineales, por ejemplo, se ven como si pudieran interpretarse perfectamente en un nivel modular, pero la interpretación de un solo peso está entrelazada con todos los demás pesos.
La interpretación de un solo peso siempre viene con la nota al pie de página de que las otras características de entrada permanecen en el mismo valor, que no es el caso con muchas aplicaciones reales.
Un modelo lineal que predice el valor de una casa, que tiene en cuenta tanto el tamaño de la casa como el número de habitaciones, puede tener un peso negativo para la característica de la cantidad de habitaciones.
Puede suceder porque ya existe la característica de tamaño de la casa altamente correlacionada.
En un mercado donde la gente prefiere habitaciones más grandes, una casa con menos habitaciones podría valer más que una casa con más habitaciones si ambas tienen el mismo tamaño.
Los pesos solo tienen sentido en el contexto de las otras características del modelo.
Pero los pesos en un modelo lineal aún se pueden interpretar mejor que los pesos de una red neuronal profunda. 

### Interpretabilidad local para una única predicción 
*¿Por qué el modelo hizo una cierta predicción para una instancia?* 

Puedes ampliar una sola instancia y examinar lo que el modelo predice para esta entrada, y explicar por qué.
Si observas una predicción individual, el comportamiento del modelo complejo podría comportarse de manera más agradable.
Localmente, la predicción podría depender solo linealmente o monotónicamente de algunas características, en lugar de tener una dependencia compleja de ellas.
Por ejemplo, el valor de una casa puede depender no linealmente de su tamaño.
Pero si solo estás mirando una casa particular de 100 metros cuadrados, existe la posibilidad de que para ese subconjunto de datos, la predicción de su modelo dependa linealmente del tamaño.
Puedes descubrir esto simulando cómo cambia el precio previsto cuando aumenta o disminuye el tamaño en 10 metros cuadrados.
Por lo tanto, las explicaciones locales pueden ser más precisas que las explicaciones globales.
Este libro presenta métodos que pueden hacer que las predicciones individuales sean más interpretables en la [sección sobre métodos modelo-agnósticos] (#agnostico). 

### Interpretabilidad local para un grupo de predicciones 
*¿Por qué el modelo hizo predicciones específicas para un grupo de instancias?* 

Las predicciones del modelo para varias observaciones pueden explicarse con métodos de interpretación de modelos globales (a nivel modular) o con explicaciones particulares por observación.
Los métodos globales se pueden aplicar tomando el grupo de observaciones, tratándolos como si fuera el conjunto de datos completo y utilizando los métodos globales con este subconjunto.
Los métodos de explicación individuales se pueden utilizar en cada instancia y luego enumerar o agregar para todo el grupo. 

<!--{pagebreak}-->

## Evaluación de la interpretabilidad 

No existe un consenso real sobre qué interpretabilidad es en el aprendizaje automático.
Tampoco está claro cómo medirla.
Pero hay una investigación inicial sobre esto y un intento de formular algunos enfoques para la evaluación, como se describe en la siguiente sección.
Doshi-Velez y Kim (2017) proponen tres niveles principales para la evaluación de la interpretabilidad: 

**Evaluación del nivel de aplicación (tarea real)**:
Ponga la explicación en el producto y haga que el usuario final lo pruebe.
Imagine un software de detección de fracturas con un componente de aprendizaje automático que localiza y marca fracturas en rayos X.
A nivel de aplicación, los radiólogos probarían el software de detección de fracturas directamente para evaluar el modelo.
Esto requiere una buena configuración experimental y una comprensión de cómo evaluar la calidad.
Una buena base para esto es siempre qué tan bueno sería un humano para explicar la misma decisión. 

**Evaluación a nivel humano (tarea simple)**
Es una evaluación de nivel de aplicación simplificada.
La diferencia es que estos experimentos no se llevan a cabo con expertos en el dominio, sino con personas 'laicas'.
Esto hace que los experimentos sean más baratos (especialmente si los expertos en el dominio son radiólogos) y es más fácil encontrar más evaluadores.
Un ejemplo sería mostrarle a un usuario diferentes explicaciones y el usuario elegiría la mejor. 

**La evaluación del nivel de función (tarea proxy)** no requiere humanos.
Esto funciona mejor cuando la clase de modelo utilizada ya ha sido evaluada por otra persona en una evaluación a nivel humano.
Por ejemplo, podría saberse que los usuarios finales entienden los árboles de decisión.
En este caso, un proxy para la calidad de la explicación puede ser la profundidad del árbol.
Los árboles más cortos obtendrían una mejor puntuación de explicabilidad.
Tendría sentido agregar la restricción de que el rendimiento predictivo del árbol sigue siendo bueno y no disminuye demasiado en comparación con un árbol más grande.

El próximo capítulo se centra en la evaluación de explicaciones para predicciones individuales en el nivel de función.
¿Cuáles son las propiedades relevantes de las explicaciones que consideraríamos para su evaluación? 

<!--{pagebreak}-->

## Propiedades de las explicaciones {#properties} 

Queremos explicar las predicciones de un modelo de aprendizaje automático.
Para lograr esto, confiamos en algún método de explicación, que es un algoritmo que genera explicaciones.
**Una explicación generalmente relaciona los valores de características de una instancia con la predicción de su modelo de una manera humanamente comprensible.**
Otros tipos de explicaciones consisten en un conjunto de instancias de datos (por ejemplo, en el caso del modelo vecino k-más cercano).
Por ejemplo, podríamos predecir el riesgo de cáncer utilizando una SVM y explicar las predicciones utilizando el [método sustituto local](#LIME), que genera árboles de decisión como explicaciones.
O podríamos usar un modelo de regresión lineal en lugar de una SVM. El modelo de regresión lineal ya está equipado con un método de explicación (interpretación de los pesos).

Echamos un vistazo más de cerca a las propiedades de los métodos de explicación y explicaciones (Robnik-Sikonja y Bohanec, 2018[^human-ml]).
Estas propiedades se pueden usar para juzgar qué tan bueno es un método.
No está claro para todas estas propiedades cómo medirlas correctamente, por lo que uno de los desafíos es formalizar cómo podrían calcularse. 

**Propiedades de los métodos de explicación** 

- **Poder expresivo** es el "lenguaje" o estructura de las explicaciones que el método puede generar.
Un método de explicación podría generar reglas IF-THEN, árboles de decisión, una suma ponderada, lenguaje natural u otra cosa. 
- **Translucidez** describe cuánto se basa el método de explicación en analizar el modelo de aprendizaje automático, como sus parámetros.
Por ejemplo, los métodos de explicación que se basan en modelos intrínsecamente interpretables como el modelo de regresión lineal (específico del modelo) son altamente translúcidos.
Los métodos que solo se basan en manipular entradas y observar las predicciones tienen cero translucidez.
Dependiendo del escenario, diferentes niveles de translucidez pueden ser deseables.
La ventaja de la alta translucidez es que el método puede confiar en más información para generar explicaciones.
La ventaja de la baja translucidez es que el método de explicación es más portátil. 
- **Portabilidad** describe la gama de modelos de aprendizaje automático con los que se puede utilizar el método de explicación.
Los métodos con baja translucidez tienen una mayor portabilidad porque tratan el modelo de aprendizaje automático como una caja negra.
Los modelos sustitutos pueden ser el método de explicación con la mayor portabilidad.
Métodos que solo funcionan, por ejemplo, para explicar las redes neuronales tienen baja portabilidad. 
- **Complejidad algorítmica** describe la complejidad computacional del método que genera la explicación.
Es importante tener en cuenta esta propiedad cuando el tiempo de cálculo es un cuello de botella en la generación de explicaciones. 

**Propiedades de explicaciones individuales**

- **Precisión**: ¿Qué tan bien una explicación predice datos nuevos?
La alta precisión es especialmente importante si la explicación se usa para las predicciones, y no para el modelo en sí.
La baja precisión puede estar bien si la precisión del modelo de aprendizaje automático también es baja, y si el objetivo es explicar lo que hace el modelo de caja negra.
En este caso, solo la fidelidad es importante. 
- **Fidelity**: ¿Qué tan bien se aproxima la explicación a la predicción del modelo de caja negra?
La alta fidelidad es una de las propiedades más importantes de una explicación, porque una explicación con baja fidelidad es inútil para explicar el modelo de aprendizaje automático.
La precisión y la fidelidad están estrechamente relacionadas.
Si el modelo de caja negra tiene una alta precisión y la explicación tiene una alta fidelidad, la explicación también tiene una alta precisión. Algunas explicaciones ofrecen solo fidelidad local, lo que significa que la explicación solo se aproxima bien a la predicción del modelo para un subconjunto de datos (por ejemplo, [modelos sustitutos locales](# LIME)) o incluso solo para una observación individual (por ejemplo, [Valores de Shapley] ( # shapley)). 
- **Consistencia**: ¿Cuánto difiere una explicación entre los modelos que han sido entrenados en la misma tarea y que producen predicciones similares?
Por ejemplo, entreno una SVM y un modelo de regresión lineal en la misma tarea y ambos producen predicciones muy similares.
Calculo explicaciones usando un método de mi elección y analizo cuán diferentes son las explicaciones.
Si las explicaciones son muy similares, las explicaciones son muy consistentes.
Encuentro esta propiedad algo complicada, ya que los dos modelos podrían usar características diferentes, pero obtener predicciones similares (también llamado ["Efecto Rashomon"](https://en.wikipedia.org/wiki/Rashomon_effect)).
En este caso, no es deseable una alta consistencia porque las explicaciones tienen que ser muy diferentes.
Es deseable una alta consistencia si los modelos realmente dependen de relaciones similares. 
- **Estabilidad**: ¿Qué tan similares son las explicaciones para instancias similares?
Mientras que la coherencia compara explicaciones entre modelos, la estabilidad compara explicaciones entre instancias similares para un modelo fijo.
Alta estabilidad significa que ligeras variaciones en las características de una observación no cambian sustancialmente la explicación (a menos que estas ligeras variaciones también cambien fuertemente la predicción).
La falta de estabilidad puede ser el resultado de una alta variación del método de explicación.
En otras palabras, el método de explicación se ve fuertemente afectado por ligeros cambios en los valores de las características de la observación a explicar.
La falta de estabilidad también puede ser causada por componentes no deterministas del método de explicación, como un paso de muestreo de datos, como el uso del [método sustituto local](#local).
La alta estabilidad siempre es deseable. 
- **Comprensibilidad**: ¿Qué tan bien entienden los humanos las explicaciones?
Esto se parece a una propiedad más entre muchas, pero es el elefante en la habitación.
Difícil de definir y medir, pero extremadamente importante para acertar.
Muchas personas están de acuerdo en que la comprensión depende de la audiencia.
Las ideas para medir la comprensibilidad incluyen medir el tamaño de la explicación (número de características con un peso distinto de cero en un modelo lineal, número de reglas de decisión, ...) o probar qué tan bien las personas pueden predecir el comportamiento del modelo de aprendizaje automático a partir de las explicaciones.
También se debe considerar la comprensión de las características utilizadas en la explicación. Una transformación compleja de características podría ser menos comprensible que las características originales. 
- **Certeza**: ¿La explicación refleja la certeza del modelo de aprendizaje automático?
Muchos modelos de aprendizaje automático solo dan predicciones sin una declaración sobre la confianza de los modelos de que la predicción es correcta.
Si el modelo predice un 4% de probabilidad de cáncer para un paciente, ¿es igual de cierto que un 4% de probabilidad para otro paciente con diferentes valores de características, pero igual valor predicho?
Una explicación que incluye la certeza del modelo es muy útil. 
- **Grado de importancia**: ¿Qué tan bien refleja la explicación la importancia de las características o partes de la explicación?
Por ejemplo, si se genera una regla de decisión como explicación para una predicción individual, ¿está claro cuál de las condiciones de la regla fue la más importante? 
- **Novedad**: ¿La explicación refleja si una instancia de datos a explicar proviene de una "nueva" región muy alejada de la distribución de datos de capacitación?
En tales casos, el modelo puede ser inexacto y la explicación puede ser inútil.
El concepto de novedad está relacionado con el concepto de certeza.
Cuanto mayor sea la novedad, más probable es que el modelo tenga poca certeza debido a la falta de datos. 
- **Representatividad**: ¿Cuántas instancias cubre una explicación?
Las explicaciones pueden abarcar todo el modelo (p. Ej., Interpretación de pesos en un modelo de regresión lineal) o representar solo una predicción individual (p. Ej., [Valores de Shapley](#shapley)). 

<!--{pagebreak}-->

## Explicaciones amigables para los humanos {#amigables} 

Profundicemos y descubramos lo que los humanos vemos como "buenas" explicaciones y cuáles son las implicaciones para el aprendizaje automático interpretable.
La investigación en humanidades puede ayudarnos a descubrirlo.
Miller (2017) ha realizado una gran encuesta de publicaciones sobre explicaciones, y este capítulo se basa en su resumen. 

En este capítulo quiero convencerlo de lo siguiente:
Como explicación de un evento, los humanos prefieren explicaciones cortas (solo 1 o 2 causas) que contrastan la situación actual con una situación en la que el evento no hubiera ocurrido.
Las causas especialmente anormales proporcionan buenas explicaciones. 
Las explicaciones son interacciones sociales entre el explicador y el explicado (receptor de la explicación) y, por lo tanto, el contexto social tiene una gran influencia en el contenido real de la explicación.

Cuando necesitas explicaciones con TODOS los factores para una predicción o comportamiento particular, no deseas una explicación amigable para los humanos, sino una atribución causal completa.
Probablemente desees una atribución causal si estás legalmente obligado a especificar todas las características influyentes o si depuras el modelo de aprendizaje automático.
En este caso, ignora los siguientes puntos.
En todos los demás casos, donde los 'laicos' o las personas con poco tiempo son los destinatarios de la explicación, las siguientes secciones deberían de serte interesantes. 

### ¿Qué es una explicación? 

Una explicación es la **respuesta a una pregunta de por qué** (Miller 2017). 

- ¿Por qué el tratamiento no funcionó en el paciente? 
- ¿Por qué fue rechazado mi préstamo? 
- ¿Por qué todavía no hemos sido contactados por la vida alienígena? 

Las dos primeras preguntas pueden responderse con una explicación "cotidiana", mientras que la tercera proviene de la categoría "Fenómenos científicos más generales y preguntas filosóficas".
Nos centramos en las explicaciones de tipo "cotidiano", porque son relevantes para el aprendizaje automático interpretable.
Las preguntas que comienzan con "cómo" generalmente se pueden reformular como preguntas de "por qué":
"¿Cómo se rechazó mi préstamo?" puede convertirse en "¿Por qué se rechazó mi préstamo?". 

A continuación, el término "explicación" se refiere al proceso social y cognitivo de explicación, pero también al producto de estos procesos.
El explicador puede ser un ser humano o una máquina. 

### ¿Qué es una buena explicación? {#buenaexplicación} 

Esta sección condensa aún más el resumen de Miller sobre explicaciones "buenas" y agrega implicaciones concretas para el aprendizaje automático interpretable. 

**Las explicaciones son contrastantes** (Lipton 1990[^lipton2]).
Los humanos generalmente no preguntan por qué se hizo una determinada predicción, sino por qué se hizo esta predicción *en lugar de otra predicción*.
Tendemos a pensar en casos contrafácticos, es decir, "¿Cómo habría sido la predicción si la entrada X hubiera sido diferente?".
Para una predicción del precio de la vivienda, el propietario podría estar interesado en saber por qué el precio previsto fue alto, en comparación con el precio más bajo que esperaba.
Si mi solicitud de préstamo es rechazada, no me importa escuchar todos los factores que generalmente hablan a favor o en contra de un rechazo.
Estoy interesado en los factores en mi solicitud que tendrían que cambiar para obtener el préstamo.
Quiero saber el contraste entre mi aplicación y la versión de mi solicitud que sería aceptada.
El reconocimiento de que las explicaciones contrastantes importan es un hallazgo importante para el aprendizaje automático explicable.
De la mayoría de los modelos interpretables, puede extraer una explicación que contrasta implícitamente una predicción de una instancia con la predicción de una instancia de datos artificiales o un promedio de instancias.
Los médicos podrían preguntar: "¿Por qué el medicamento no funcionó para mi paciente?".
Y podrían querer una explicación que contraste a su paciente con un paciente para quien el medicamento funcionó y que sea similar al paciente que no responde.
Las explicaciones contrastantes son más fáciles de entender que explicaciones completas.
Una explicación completa de la pregunta del médico de por qué el medicamento no funciona puede incluir:
El paciente ha tenido la enfermedad durante 10 años, 11 genes se sobreexpresan, el cuerpo del paciente es muy rápido en descomponer el medicamento en químicos ineficaces.
Una explicación contrastante podría ser mucho más simple:
en contraste con el paciente que responde, el paciente que no responde tiene una cierta combinación de genes que hacen que el medicamento sea menos efectivo.
La mejor explicación es la que destaca la mayor diferencia entre el objeto de interés y el objeto de referencia. 
**Lo que significa para el aprendizaje automático interpretable**:
los humanos no quieren una explicación completa para una predicción, sino comparar las diferencias con la predicción de otra observación (que puede ser artificial).
La creación de explicaciones contrastantes depende de la aplicación, porque requiere un punto de referencia para la comparación.
Y esto puede depender del punto de datos a explicar, pero también del usuario que recibe la explicación. 
Un usuario de un sitio web de predicción del precio de la vivienda puede querer tener una explicación de una predicción del precio de la vivienda en contraste con su propia casa o tal vez con otra casa en el sitio web o tal vez con una casa promedio en el vecindario.
La solución para la creación automatizada de explicaciones contrastantes también podría implicar la búsqueda de prototipos o arquetipos en los datos. 

**Las explicaciones se seleccionan**.
La gente no espera explicaciones que cubran la lista real y completa de causas de un evento.
Estamos acostumbrados a seleccionar una o dos causas de una variedad de causas posibles como LA explicación.
Como prueba, encienda las noticias de TV: "El descenso en los precios de las acciones se atribuye a una creciente reacción contra el producto de la compañía debido a problemas con la última actualización de software".
"Tsubasa y su equipo perdieron el partido debido a una defensa débil: dieron a sus oponentes demasiado espacio para desarrollar su estrategia".
"La creciente desconfianza de las instituciones establecidas y nuestro gobierno son los principales factores que han reducido la participación electoral".
El hecho de que un evento puede explicarse por varias causas se llama Efecto Rashomon.
Rashomon es una película japonesa que cuenta historias alternativas y contradictorias (explicaciones) sobre la muerte de un samurai.
Para los modelos de aprendizaje automático, es ventajoso si se puede hacer una buena predicción a partir de diferentes características.
Los métodos de conjunto que combinan múltiples modelos con diferentes características (diferentes explicaciones) generalmente funcionan bien porque promediar esas "historias" hace que las predicciones sean más sólidas y precisas.
Pero también significa que hay más de una explicación selectiva de por qué se hizo una determinada predicción. 
**Lo que significa para el aprendizaje automático interpretable**:
Haga la explicación muy breve, dé solo 1 a 3 razones, incluso si el mundo es más complejo.
El [método LIME](#lime) hace un buen trabajo con esto. 

**Las explicaciones son sociales**.
Son parte de una conversación o interacción entre el explicador y el receptor de la explicación.
El contexto social determina el contenido y la naturaleza de las explicaciones.
Si quisiera explicarle a una persona técnica por qué las criptomonedas digitales valen tanto, diría cosas como:
"La contabilidad descentralizada, distribuida y basada en blockchain, que no puede ser controlado por una entidad central, resuena con las personas que desean asegurarse su riqueza, lo que explica la alta demanda y el precio ".
Pero a mi abuela le diría:
"Mira, abuela: las criptomonedas son un poco como el oro de la computadora. A la gente le gusta y paga mucho por el oro, y a los jóvenes les gusta y pagan mucho por el oro de la computadora". 
**Lo que significa para el aprendizaje automático interpretable**:
Presta atención al entorno social de su aplicación de aprendizaje automático y al público objetivo.
Obtener la parte social del modelo de aprendizaje automático correcto depende completamente de su aplicación específica.
Encuentra expertos de las humanidades (por ejemplo, psicólogos y sociólogos) para que te ayuden. 

**Las explicaciones se centran en lo anormal**.
Las personas se enfocan más en causas anormales para explicar los eventos (Kahnemann y Tversky, 1981[^Kahnemann]).
Estas son causas que tenían una pequeña probabilidad pero que, sin embargo, ocurrieron.
La eliminación de estas causas anormales habría cambiado mucho el resultado (explicación contrafáctica).
Los humanos consideran este tipo de causas "anormales" como buenas explicaciones.
Un ejemplo de Štrumbelj y Kononenko (2011)[^Strumbelj2011] es:
Supongamos que tenemos un conjunto de datos de situaciones de prueba entre profesores y alumnos.
Los estudiantes asisten a un curso y lo aprueban directamente después de una presentación exitosa.
El maestro tiene la opción de hacer preguntas adicionales al alumno para evaluar su conocimiento.
Los estudiantes que no puedan responder estas preguntas reprobarán el curso.
Los estudiantes pueden tener diferentes niveles de preparación, lo que se traduce en diferentes probabilidades de responder correctamente las preguntas del maestro (si deciden evaluar al estudiante).
Queremos predecir si un alumno aprobará el curso y explicar nuestra predicción.
La posibilidad de aprobar es del 100% si el maestro no hace preguntas adicionales; de lo contrario, la probabilidad de aprobar depende del nivel de preparación del alumno y la probabilidad resultante de responder las preguntas correctamente. 
Escenario 1: el maestro generalmente hace preguntas adicionales a los estudiantes (por ejemplo, 95 de cada 100 veces).
Un estudiante que no estudió (10% de posibilidades de aprobar la parte de la pregunta) no fue uno de los afortunados y recibe preguntas adicionales que no responde correctamente.
¿Por qué el alumno reprobó el curso? Yo diría que fue culpa del estudiante por no estudiar. 
Escenario 2: el profesor rara vez hace preguntas adicionales (por ejemplo, 2 de cada 100 veces).
Para un estudiante que no ha estudiado las preguntas, predeciríamos una alta probabilidad de aprobar el curso porque las preguntas son poco probables.
Por supuesto, uno de los estudiantes no se preparó para las preguntas, lo que le da un 10% de posibilidades de aprobar las preguntas.
No tiene suerte y el profesor hace preguntas adicionales que el alumno no puede responder y no aprueba el curso.
¿Cuál es la razón del fracaso?
Yo diría que ahora, la mejor explicación es "porque el profesor evaluó al alumno".
Era poco probable que el maestro hiciera la prueba, por lo que se comportó de manera anormal. 
**Lo que significa para el aprendizaje automático interpretable**: 
Si una de las características de entrada para una predicción fue anormal en algún sentido (como una categoría rara de una característica categórica) y la característica influyó en la predicción, debe incluirse en una explicación, incluso si otras características 'normales' tienen la misma influencia en la predicción que la anormal.
Una característica anormal en nuestro ejemplo de predicción del precio de la vivienda podría ser que una vivienda bastante cara tiene dos balcones.
Incluso si algún método de atribución determina que los dos balcones contribuyen tanto a la diferencia de precio como el tamaño promedio de la casa, el vecindario bueno o la reciente renovación, la característica anormal "dos balcones" podría ser la mejor explicación de por qué la casa es tan costosa. 

**Las explicaciones son verdaderas**.
Las buenas explicaciones demuestran ser ciertas en la realidad (es decir, en otras situaciones).
Pero inquietantemente, este no es el factor más importante para una "buena" explicación.
Por ejemplo, la selectividad parece ser más importante que la veracidad.
Una explicación que selecciona solo una o dos causas posibles rara vez cubre la lista completa de causas relevantes.
La selectividad omite parte de la verdad.
No es cierto que solo uno o dos factores, por ejemplo, hayan causado un colapso del mercado de valores: la verdad es que hay millones de causas que influyen en millones de personas para que actúen de tal manera que al final se causó un colapso. 
**Lo que significa para el aprendizaje automático interpretable**:
La explicación debe predecir el evento con la mayor veracidad posible, que en el aprendizaje automático a veces se llama **fidelidad**.
Entonces, si decimos que un segundo balcón aumenta el precio de una casa, eso también debería aplicarse a otras casas (o al menos a casas similares).
Para los humanos, la fidelidad de una explicación no es tan importante como su selectividad, su contraste y su aspecto social. 

**Las buenas explicaciones son consistentes con las creencias previas del explicado**.
Los humanos tienden a ignorar la información que es inconsistente con sus creencias anteriores.
Este efecto se llama sesgo de confirmación (Nickerson 1998 [^Nickerson]).
Las explicaciones no se salvan de este tipo de sesgo.
La gente tenderá a devaluar o ignorar explicaciones que no concuerden con sus creencias.
El conjunto de creencias varía de persona a persona, pero también hay creencias previas basadas en grupos, como las cosmovisiones políticas.
**Lo que significa para el aprendizaje automático interpretable**: 
Las buenas explicaciones son consistentes con las creencias anteriores.
Esto es difícil de integrar en el aprendizaje automático y probablemente comprometería drásticamente el rendimiento predictivo.
Nuestra creencia previa sobre el efecto del tamaño de la casa en el precio previsto es que cuanto más grande sea la casa, mayor será el precio.
Supongamos que un modelo también muestra un efecto negativo del tamaño de la casa en el precio previsto para algunas casas.
El modelo ha aprendido esto porque mejora el rendimiento predictivo (debido a algunas interacciones complejas), pero este comportamiento contradice fuertemente nuestras creencias anteriores.
Puede aplicar restricciones de monotonicidad (una característica solo puede afectar la predicción en una dirección) o usar algo como un modelo lineal que tenga esta propiedad. 

**Las buenas explicaciones son generales y probables**.
Una causa que puede explicar muchos eventos es muy general y podría considerarse una buena explicación.
Ten en cuenta que esto contradice la afirmación de que las causas anormales son buenas explicaciones.
A mi entender, las causas anormales superan a las causas generales.
Las causas anormales son, por definición, raras en el escenario dado.
En ausencia de un evento anormal, una explicación general se considera una buena explicación.
También recuerda que las personas tienden a juzgar mal las probabilidades de eventos conjuntos.
(Joe es bibliotecario. ¿Es más probable que sea una persona tímida o una persona tímida a la que le gusta leer libros?)
Un buen ejemplo es "La casa es cara porque es grande", lo cual es una buena explicación de por qué las casas son caras o baratas. 
**Lo que significa para el aprendizaje automático interpretable**: 
La generalidad se puede medir fácilmente con el soporte de la función, que es el número de instancias a las que se aplica la explicación dividido por el número total de instancias. 

[^Miller2017]: Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).

[^Doshi2017]: Doshi-Velez, Finale, y Been Kim. "Towards a rigorous science of interpretable machine learning," nu. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017).

[^Heider]: Heider, Fritz, y Marianne Simmel. "An experimental study of apparent behavior." The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).

[^Lipton2016]: Lipton, Zachary C. "The mythos of model interpretability." arXiv preprint arXiv:1606.03490, (2016).

[^Kahnemann]: Kahneman, Daniel, y Amos Tversky. "The Simulation Heuristic." Stanford Univ CA Dept of Psychology. (1981).

[^Strumbelj2011]: Štrumbelj, Erik, y Igor Kononenko. "A general method for visualizing and explaining black-box regression models." En International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).

[^Nickerson]: Nickerson, Raymond S. "Confirmation Bias: A ubiquitous phenomenon in many guises." Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).

[^critique]: Kim, Been, Rajiv Khanna, y Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! Criticism for interpretability." Advances in Neural Information Processing Systems (2016).

[^human-ml]: Robnik-Sikonja, Marko, y Marko Bohanec. "Perturbation-based explanations of prediction models." Human and Machine Learning. Springer, Cham. 159-175. (2018).

[^lipton2]: Lipton, Peter. "Contrastive explanation." Royal Institute of Philosophy Supplements 27 (1990): 247-266.
