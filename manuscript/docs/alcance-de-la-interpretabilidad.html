<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Alcance de la interpretabilidad | Aprendizaje automatico interpretable</title>
  <meta name="description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Alcance de la interpretabilidad | Aprendizaje automatico interpretable" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Alcance de la interpretabilidad | Aprendizaje automatico interpretable" />
  
  <meta name="twitter:description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2020-03-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="taxonomia-de-los-metodos-de-interpretacion.html"/>
<link rel="next" href="evaluacion-de-la-interpretabilidad.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159445204-1', 'https://fedefliguer.github.io/AAI/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="style.css+" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Bookdown Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="horadelcuento.html"><a href="horadelcuento.html"><i class="fa fa-check"></i><b>1.1</b> Hora del cuento</a><ul>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#un-rayo-nunca-golpea-dos-veces"><i class="fa fa-check"></i>Un rayo nunca golpea dos veces</a></li>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#perder-confianza"><i class="fa fa-check"></i>Perder confianza</a></li>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#clips-de-papel-de-fermi"><i class="fa fa-check"></i>Clips de papel de Fermi</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="que-es-el-aprendizaje-automatico.html"><a href="que-es-el-aprendizaje-automatico.html"><i class="fa fa-check"></i><b>1.2</b> ¿Qué es el aprendizaje automático?</a></li>
<li class="chapter" data-level="1.3" data-path="terminología.html"><a href="terminología.html"><i class="fa fa-check"></i><b>1.3</b> Terminología</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretabilidad.html"><a href="interpretabilidad.html"><i class="fa fa-check"></i><b>2</b> Interpretabilidad</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretabilidad-importancia.html"><a href="interpretabilidad-importancia.html"><i class="fa fa-check"></i><b>2.1</b> Importancia de la interpretabilidad</a></li>
<li class="chapter" data-level="2.2" data-path="taxonomia-de-los-metodos-de-interpretacion.html"><a href="taxonomia-de-los-metodos-de-interpretacion.html"><i class="fa fa-check"></i><b>2.2</b> Taxonomía de los métodos de interpretación</a></li>
<li class="chapter" data-level="2.3" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html"><i class="fa fa-check"></i><b>2.3</b> Alcance de la interpretabilidad</a><ul>
<li class="chapter" data-level="2.3.1" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#transparencia-del-algoritmo"><i class="fa fa-check"></i><b>2.3.1</b> Transparencia del algoritmo</a></li>
<li class="chapter" data-level="2.3.2" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-global-y-holistica-del-modelo"><i class="fa fa-check"></i><b>2.3.2</b> Interpretabilidad global y holística del modelo</a></li>
<li class="chapter" data-level="2.3.3" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-del-modelo-global-en-un-nivel-modular"><i class="fa fa-check"></i><b>2.3.3</b> Interpretabilidad del modelo global en un nivel modular</a></li>
<li class="chapter" data-level="2.3.4" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-local-para-una-unica-prediccion"><i class="fa fa-check"></i><b>2.3.4</b> Interpretabilidad local para una única predicción</a></li>
<li class="chapter" data-level="2.3.5" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-local-para-un-grupo-de-predicciones"><i class="fa fa-check"></i><b>2.3.5</b> Interpretabilidad local para un grupo de predicciones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="evaluacion-de-la-interpretabilidad.html"><a href="evaluacion-de-la-interpretabilidad.html"><i class="fa fa-check"></i><b>2.4</b> Evaluación de la interpretabilidad</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> Propiedades de las explicaciones</a></li>
<li class="chapter" data-level="2.6" data-path="amigables.html"><a href="amigables.html"><i class="fa fa-check"></i><b>2.6</b> Explicaciones amigables para los humanos</a><ul>
<li class="chapter" data-level="2.6.1" data-path="amigables.html"><a href="amigables.html#que-es-una-explicacion"><i class="fa fa-check"></i><b>2.6.1</b> ¿Qué es una explicación?</a></li>
<li class="chapter" data-level="2.6.2" data-path="amigables.html"><a href="amigables.html#buenaexplicación"><i class="fa fa-check"></i><b>2.6.2</b> ¿Qué es una buena explicación?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conjuntosdedatos.html"><a href="conjuntosdedatos.html"><i class="fa fa-check"></i><b>3</b> Conjuntos de datos</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Alquiler de bicicletas (Regresión)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> Comentarios de spam de YouTube (clasificación de texto)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> Factores de riesgo para el cáncer de cuello uterino (Clasificación)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje automatico interpretable</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="alcance-de-la-interpretabilidad" class="section level2">
<h2><span class="header-section-number">2.3</span> Alcance de la interpretabilidad</h2>
<p>Un algoritmo entrena un modelo que produce las predicciones.
Cada paso puede evaluarse en términos de transparencia o interpretabilidad.</p>
<div id="transparencia-del-algoritmo" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Transparencia del algoritmo</h3>
<p><em>¿Cómo crea el algoritmo el modelo?</em></p>
<p>La transparencia del algoritmo se trata de cómo el algoritmo aprende un modelo desde los datos, y de qué tipo de relaciones puede incorporar.
Si utilizas redes neuronales convolucionales para clasificar imágenes, puedes explicar que el algoritmo aprende detectores de borde y filtros en las capas más bajas.
Esto es una comprensión de cómo funciona el algoritmo, pero no para el modelo específico que se aprende al final, y tampoco para la forma en la que se hacen las predicciones individuales.
La transparencia del algoritmo solo requiere el conocimiento del algoritmo y no de los datos o el modelo aprendido.
Este libro se centra en la interpretabilidad del modelo y no en la transparencia del algoritmo.
Algoritmos como el método de mínimos cuadrados para modelos lineales están bien estudiados y entendidos.
Se caracterizan por una alta transparencia.
Los enfoques de aprendizaje profundo (empujar un gradiente a través de una red con millones de pesos) se entienden menos y el funcionamiento interno es el foco de la investigación en curso.
Se consideran menos transparentes.</p>
</div>
<div id="interpretabilidad-global-y-holistica-del-modelo" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Interpretabilidad global y holística del modelo</h3>
<p><em>¿Cómo hace predicciones el modelo entrenado?</em></p>
<p>Podrías describir un modelo como interpretable si puedes comprender todo el modelo de una vez (Lipton 2016<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>).
Para explicar el resultado del modelo global necesitas el modelo entrenado, el conocimiento del algoritmo y los datos.
Este nivel de interpretabilidad se trata de comprender cómo toma decisiones el modelo, en función de una visión holística de sus características y de cada uno de los componentes aprendidos, como los pesos, parámetros y estructuras.
¿Qué características son importantes y qué tipo de interacciones entre ellas tienen lugar?
La interpretación global del modelo ayuda a comprender la distribución de su resultado objetivo en función de las características.
La interpretabilidad del modelo global es muy difícil de lograr en la práctica.
Es improbable que cualquier modelo que exceda un puñado de parámetros o pesos se ajuste a la memoria a corto plazo del ser humano promedio.
Sostengo que realmente no puedes imaginar un modelo lineal con 5 características, porque significaría dibujar mentalmente el hiperplano estimado en un espacio de 5 dimensiones.
Cualquier espacio de características con más de 3 dimensiones es simplemente inconcebible para los humanos.
Por lo general, cuando las personas intentan comprender un modelo, solo consideran partes de él, como los pesos en los modelos lineales.</p>
</div>
<div id="interpretabilidad-del-modelo-global-en-un-nivel-modular" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Interpretabilidad del modelo global en un nivel modular</h3>
<p><em>¿Cómo afectan las predicciones las partes del modelo?</em></p>
<p>Un modelo de Naive Bayes con cientos de características sería demasiado grande para mantenerlo en nuestra memoria de trabajo.
E incluso si logramos memorizar todos los pesos, no podríamos hacer predicciones rápidamente para nuevas observaciones.
Además, debes tener la distribución conjunta de todas las características en su cabeza para estimar la importancia de cada característica y cómo las características afectan las predicciones en promedio.
Una tarea imposible.
Pero puedes entender fácilmente un solo peso.
Si bien la interpretación global del modelo generalmente está fuera del alcance, existe una buena posibilidad de comprender al menos algunos modelos a nivel modular.
No todos los modelos son interpretables a nivel de parámetro.
Para los modelos lineales, las partes interpretables son los pesos, para los árboles serían las divisiones (características seleccionadas más puntos de corte) y las predicciones de los nodos foliares.
Los modelos lineales, por ejemplo, se ven como si pudieran interpretarse perfectamente en un nivel modular, pero la interpretación de un solo peso está entrelazada con todos los demás pesos.
La interpretación de un solo peso siempre viene con la nota al pie de página de que las otras características de entrada permanecen en el mismo valor, que no es el caso con muchas aplicaciones reales.
Un modelo lineal que predice el valor de una casa, que tiene en cuenta tanto el tamaño de la casa como el número de habitaciones, puede tener un peso negativo para la característica de la cantidad de habitaciones.
Puede suceder porque ya existe la característica de tamaño de la casa altamente correlacionada.
En un mercado donde la gente prefiere habitaciones más grandes, una casa con menos habitaciones podría valer más que una casa con más habitaciones si ambas tienen el mismo tamaño.
Los pesos solo tienen sentido en el contexto de las otras características del modelo.
Pero los pesos en un modelo lineal aún se pueden interpretar mejor que los pesos de una red neuronal profunda.</p>
</div>
<div id="interpretabilidad-local-para-una-unica-prediccion" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Interpretabilidad local para una única predicción</h3>
<p><em>¿Por qué el modelo hizo una cierta predicción para una instancia?</em></p>
<p>Puedes ampliar una sola instancia y examinar lo que el modelo predice para esta entrada, y explicar por qué.
Si observas una predicción individual, el comportamiento del modelo complejo podría comportarse de manera más agradable.
Localmente, la predicción podría depender solo linealmente o monotónicamente de algunas características, en lugar de tener una dependencia compleja de ellas.
Por ejemplo, el valor de una casa puede depender no linealmente de su tamaño.
Pero si solo estás mirando una casa particular de 100 metros cuadrados, existe la posibilidad de que para ese subconjunto de datos, la predicción de su modelo dependa linealmente del tamaño.
Puedes descubrir esto simulando cómo cambia el precio previsto cuando aumenta o disminuye el tamaño en 10 metros cuadrados.
Por lo tanto, las explicaciones locales pueden ser más precisas que las explicaciones globales.
Este libro presenta métodos que pueden hacer que las predicciones individuales sean más interpretables en la [sección sobre métodos modelo-agnósticos] (#agnostico).</p>
</div>
<div id="interpretabilidad-local-para-un-grupo-de-predicciones" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Interpretabilidad local para un grupo de predicciones</h3>
<p><em>¿Por qué el modelo hizo predicciones específicas para un grupo de instancias?</em></p>
<p>Las predicciones del modelo para varias observaciones pueden explicarse con métodos de interpretación de modelos globales (a nivel modular) o con explicaciones particulares por observación.
Los métodos globales se pueden aplicar tomando el grupo de observaciones, tratándolos como si fuera el conjunto de datos completo y utilizando los métodos globales con este subconjunto.
Los métodos de explicación individuales se pueden utilizar en cada instancia y luego enumerar o agregar para todo el grupo.</p>
<!--{pagebreak}-->
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>Lipton, Zachary C. “The mythos of model interpretability.” arXiv preprint arXiv:1606.03490, (2016).<a href="alcance-de-la-interpretabilidad.html#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="taxonomia-de-los-metodos-de-interpretacion.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="evaluacion-de-la-interpretabilidad.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
