<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 Importancia de la interpretabilidad | Aprendizaje automatico interpretable</title>
  <meta name="description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 Importancia de la interpretabilidad | Aprendizaje automatico interpretable" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 Importancia de la interpretabilidad | Aprendizaje automatico interpretable" />
  
  <meta name="twitter:description" content="Los algoritmos de aprendizaje autom?tico generalmente funcionan como cajas negras y no esta claro como determinan sus decisiones. Este libro es una guia para profesionales para hacer que las decisiones de aprendizaje automatico sean interpretables." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2020-03-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interpretabilidad.html"/>
<link rel="next" href="taxonomia-de-los-metodos-de-interpretacion.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159445204-1', 'https://fedefliguer.github.io/AAI/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="style.css+" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Bookdown Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="horadelcuento.html"><a href="horadelcuento.html"><i class="fa fa-check"></i><b>1.1</b> Hora del cuento</a><ul>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#un-rayo-nunca-golpea-dos-veces"><i class="fa fa-check"></i>Un rayo nunca golpea dos veces</a></li>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#perder-confianza"><i class="fa fa-check"></i>Perder confianza</a></li>
<li class="chapter" data-level="" data-path="horadelcuento.html"><a href="horadelcuento.html#clips-de-papel-de-fermi"><i class="fa fa-check"></i>Clips de papel de Fermi</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="que-es-el-aprendizaje-automatico.html"><a href="que-es-el-aprendizaje-automatico.html"><i class="fa fa-check"></i><b>1.2</b> ¿Qué es el aprendizaje automático?</a></li>
<li class="chapter" data-level="1.3" data-path="terminología.html"><a href="terminología.html"><i class="fa fa-check"></i><b>1.3</b> Terminología</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretabilidad.html"><a href="interpretabilidad.html"><i class="fa fa-check"></i><b>2</b> Interpretabilidad</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretabilidad-importancia.html"><a href="interpretabilidad-importancia.html"><i class="fa fa-check"></i><b>2.1</b> Importancia de la interpretabilidad</a></li>
<li class="chapter" data-level="2.2" data-path="taxonomia-de-los-metodos-de-interpretacion.html"><a href="taxonomia-de-los-metodos-de-interpretacion.html"><i class="fa fa-check"></i><b>2.2</b> Taxonomía de los métodos de interpretación</a></li>
<li class="chapter" data-level="2.3" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html"><i class="fa fa-check"></i><b>2.3</b> Alcance de la interpretabilidad</a><ul>
<li class="chapter" data-level="2.3.1" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#transparencia-del-algoritmo"><i class="fa fa-check"></i><b>2.3.1</b> Transparencia del algoritmo</a></li>
<li class="chapter" data-level="2.3.2" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-global-y-holistica-del-modelo"><i class="fa fa-check"></i><b>2.3.2</b> Interpretabilidad global y holística del modelo</a></li>
<li class="chapter" data-level="2.3.3" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-del-modelo-global-en-un-nivel-modular"><i class="fa fa-check"></i><b>2.3.3</b> Interpretabilidad del modelo global en un nivel modular</a></li>
<li class="chapter" data-level="2.3.4" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-local-para-una-unica-prediccion"><i class="fa fa-check"></i><b>2.3.4</b> Interpretabilidad local para una única predicción</a></li>
<li class="chapter" data-level="2.3.5" data-path="alcance-de-la-interpretabilidad.html"><a href="alcance-de-la-interpretabilidad.html#interpretabilidad-local-para-un-grupo-de-predicciones"><i class="fa fa-check"></i><b>2.3.5</b> Interpretabilidad local para un grupo de predicciones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="evaluacion-de-la-interpretabilidad.html"><a href="evaluacion-de-la-interpretabilidad.html"><i class="fa fa-check"></i><b>2.4</b> Evaluación de la interpretabilidad</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> Propiedades de las explicaciones</a></li>
<li class="chapter" data-level="2.6" data-path="amigables.html"><a href="amigables.html"><i class="fa fa-check"></i><b>2.6</b> Explicaciones amigables para los humanos</a><ul>
<li class="chapter" data-level="2.6.1" data-path="amigables.html"><a href="amigables.html#que-es-una-explicacion"><i class="fa fa-check"></i><b>2.6.1</b> ¿Qué es una explicación?</a></li>
<li class="chapter" data-level="2.6.2" data-path="amigables.html"><a href="amigables.html#buenaexplicación"><i class="fa fa-check"></i><b>2.6.2</b> ¿Qué es una buena explicación?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conjuntosdedatos.html"><a href="conjuntosdedatos.html"><i class="fa fa-check"></i><b>3</b> Conjuntos de datos</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Alquiler de bicicletas (Regresión)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> Comentarios de spam de YouTube (clasificación de texto)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> Factores de riesgo para el cáncer de cuello uterino (Clasificación)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje automatico interpretable</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interpretabilidad-importancia" class="section level2">
<h2><span class="header-section-number">2.1</span> Importancia de la interpretabilidad</h2>
<p>Si un modelo de aprendizaje automático funciona bien, <strong>¿por qué no confiamos en el modelo</strong> e ignoramos <strong>por qué</strong> tomó una determinada decisión?
“El problema es que una sola métrica, como la precisión de la clasificación, es una descripción incompleta de la mayoría de las tareas del mundo real”. (Doshi-Velez y Kim 2017<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>)</p>
<p>Profundicemos en las razones por las que la interpretabilidad es tan importante.
Cuando se trata de modelado predictivo, debe hacer una compensación:
¿solo desea saber <strong>qué</strong> se predice?
Por ejemplo, la probabilidad de que un cliente abandone un servicio o qué tan efectivo será un medicamento para un paciente.
¿O quieres saber <strong>por qué</strong> se hizo la predicción y posiblemente pagar la interpretabilidad con una caída en el rendimiento predictivo?
En algunos casos, no le importa por qué se tomó una decisión, es suficiente saber que el rendimiento predictivo en un conjunto de datos de prueba fue bueno.
Pero en otros casos, conocer el ‘por qué’ puede ayudarlo a aprender más sobre el problema, los datos y la razón por la cual un modelo puede fallar.
Es posible que algunos modelos no requieran explicaciones porque se usan en un entorno de bajo riesgo, lo que significa que un error no tendrá consecuencias graves (por ejemplo, un sistema de recomendación de películas) o que el método ya ha sido ampliamente estudiado y evaluado (por ejemplo, reconocimiento óptico de caracteres).
La necesidad de interpretabilidad surge de una incompletitud en la formalización del problema (Doshi-Velez y Kim 2017), lo que significa que para ciertos problemas o tareas no es suficiente obtener la predicción (el <strong>qué</strong>).
El modelo también debe explicar cómo llegó a la predicción (el <strong>por qué</strong>), porque una predicción correcta solo resuelve parcialmente su problema original.
Las siguientes razones impulsan la demanda de interpretabilidad y explicaciones (Doshi-Velez y Kim 2017 y Miller 2017).</p>
<p><strong>Curiosidad y aprendizaje humanos</strong>: los humanos tienen un modelo mental de su entorno que se actualiza cuando ocurre algo inesperado.
Esta actualización se realiza buscando una explicación para el evento inesperado.
Por ejemplo, un humano se siente enfermo y pregunta: “¿Por qué me siento tan enfermo?”.
Se entera de que se enferma cada vez que come frutos rojos. Actualiza su modelo mental y decide que esos frutos causaron la enfermedad y, por lo tanto, deben evitarse.
Cuando se usan modelos opacos de aprendizaje automático en la investigación, los hallazgos científicos permanecen completamente ocultos si el modelo solo da predicciones sin explicaciones.
Para facilitar el aprendizaje y satisfacer la curiosidad de por qué ciertas predicciones o comportamientos son creados por máquinas, la interpretación y las explicaciones son cruciales.
Por supuesto, los humanos no necesitan explicaciones para todo lo que sucede.
Para la mayoría de las personas, no hay problemas en no entender cómo funciona una computadora.
Eventos inesperados nos hacen curiosos.
Por ejemplo: ¿Por qué mi computadora se apaga inesperadamente?</p>
<p>Estrechamente relacionado con el aprendizaje está el deseo humano de <strong>encontrar significado en el mundo</strong>.
Queremos armonizar las contradicciones o inconsistencias entre los elementos de nuestras estructuras de conocimiento.
“¿Por qué mi perro me mordió a pesar de que nunca antes lo había hecho?” Un humano podría preguntar.
Existe una contradicción entre el conocimiento del comportamiento pasado del perro y la experiencia desagradable recién hecha de la mordedura.
La explicación del veterinario concilia la contradicción del dueño del perro:
“El perro estaba estresado y mordido”.
Cuanto más la decisión de una máquina afecta la vida de una persona, más importante es que la máquina explique su comportamiento.
Si un modelo de aprendizaje automático rechaza una solicitud de préstamo, esto puede ser completamente inesperado para los solicitantes.
Solo pueden conciliar esta inconsistencia entre la expectativa y la realidad con algún tipo de explicación.
En realidad, las explicaciones no tienen que explicar completamente la situación, sino que deben abordar una causa principal.
Otro ejemplo es la recomendación algorítmica del producto.
Personalmente, siempre pienso en por qué ciertos productos o películas me han sido recomendados algorítmicamente.
A menudo es bastante claro:
la publicidad me sigue en Internet porque recientemente compré una lavadora, y sé que en los próximos días me seguirán anuncios de lavadoras.
Sí, tiene sentido sugerir guantes si ya tengo un gorro de nieve en mi carrito de compras.
El algoritmo recomienda esta película, porque los usuarios a quienes les gustaron otras películas que me gustaron también disfrutaron la película recomendada.
Cada vez más, las compañías de Internet están agregando explicaciones a sus recomendaciones.
Un buen ejemplo es la recomendación de productos de Amazon, que se basa en combinaciones de productos que se compran con frecuencia:</p>
<div class="figure"><span id="fig:amazon-recommendation"></span>
<img src="images/amazon-freq-bought-together.png" alt="Productos recomendados cuando se compra pintura en Amazon." width="500" />
<p class="caption">
FIGURE 2.1: Productos recomendados cuando se compra pintura en Amazon.
</p>
</div>
<p>En muchas disciplinas científicas hay un cambio de métodos cualitativos a cuantitativos (por ejemplo, sociología, psicología), y también hacia el aprendizaje automático (biología, genómica)
El <strong>objetivo de la ciencia</strong> es obtener conocimiento, pero muchos problemas se resuelven con grandes conjuntos de datos y modelos de aprendizaje automático de caja negra.
El modelo en sí se convierte en la fuente de conocimiento en lugar de los datos.
La interpretabilidad hace posible extraer este conocimiento adicional capturado por el modelo.</p>
<p>Los modelos de aprendizaje automático asumen tareas del mundo real que requieren <strong>medidas de seguridad</strong> y pruebas.
Imagina que un automóvil autónomo detecta automáticamente a los ciclistas en función de un sistema de aprendizaje profundo.
Deseas estar 100% seguro de que la abstracción que ha aprendido el sistema está libre de errores, porque atropellar a los ciclistas es bastante malo.
Una explicación podría revelar que la característica aprendida más importante es reconocer las dos ruedas de una bicicleta, pero existen casos de borde, como bicicletas con bolsas laterales que cubren parcialmente las ruedas.</p>
<p>Por defecto, los modelos de aprendizaje automático recogen sesgos de los datos de entrenamiento.
Esto puede convertir sus modelos de aprendizaje automático en racistas que discriminan determinados grupos.
La interpretabilidad es una herramienta de depuración útil para <strong>detectar sesgos</strong> en modelos de aprendizaje automático.
Puede suceder que el modelo de aprendizaje automático que usted haya entrenado para la aprobación automática o el rechazo de las solicitudes de crédito discrimine a una minoría.
Su objetivo principal es otorgar préstamos solo a personas que eventualmente los pagarán.
Lo incompleto de la formulación del problema en este caso radica en el hecho de que no solo desea minimizar los impagos de préstamos, sino que también está obligado a no discriminar sobre la base de ciertos datos demográficos.
Esta es una restricción adicional que forma parte de la formulación de su problema (otorgar préstamos de manera riesgosa y conforme) que no está cubierta por la función de pérdida para la que se optimizó el modelo de aprendizaje automático.</p>
<p>El proceso de integración de máquinas y algoritmos en nuestra vida diaria requiere interpretabilidad para aumentar la <strong>aceptación social</strong>.
Las personas atribuyen creencias, deseos, intenciones, etc. a los objetos.
En un famoso experimento, Heider y Simmel (1944)<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> mostraron a los participantes videos de formas en las que un círculo abría una “puerta” para ingresar a una “habitación” (que era simplemente un rectángulo).
Los participantes describieron las acciones de las formas como describirían las acciones de un agente humano, asignando intenciones e incluso emociones y rasgos de personalidad a las formas.
Los robots son un buen ejemplo, como mi aspiradora, a la que llamé “Doge”.
Si Doge se atasca, pienso:
“Doge quiere seguir limpiando, pero me pide ayuda porque se atascó”.
Más tarde, cuando Doge termina de limpiar y busca en la base de operaciones para recargar, pienso:
“Doge desea recargar y tiene la intención de encontrar la base de operaciones”.
También atribuyo rasgos de personalidad:
“Doge es un poco tonto, pero de una manera linda”.
Estos son mis pensamientos, especialmente cuando descubro que Doge ha derribado una planta mientras aspiraba la casa.
Una máquina o algoritmo que explica sus predicciones encontrará más aceptación.
Véase también el <a href="#explicación">capítulo sobre explicaciones</a>, que argumenta que las explicaciones son un proceso social.</p>
<p>Las explicaciones se utilizan para <strong>gestionar las interacciones sociales</strong>.
Al crear un significado compartido de algo, el explicador influye en las acciones, emociones y creencias del receptor de la explicación.
Para que una máquina interactúe con nosotros, puede que tenga que moldear nuestras emociones y creencias.
Las máquinas tienen que “persuadirnos” para que puedan lograr su objetivo.
No aceptaría completamente mi robot aspirador si no explicara su comportamiento, al menos hasta cierto punto.
La aspiradora crea un significado compartido de, por ejemplo, un “accidente” (como quedarse atascado en la alfombra del baño … otra vez) al explicar que se atascó en lugar de simplemente detenerse a trabajar sin comentarios.
Curiosamente, puede haber un desalineamiento entre el objetivo de la máquina explicadora (crear confianza) y el objetivo del destinatario (comprender la predicción o el comportamiento).
Quizás la explicación completa de por qué Doge se atascó podría ser que la batería estaba muy baja, que una de las ruedas no funciona correctamente y que hay un error que hace que el robot vaya al mismo lugar una y otra vez a pesar de que había un obstaculo.
Estas razones (y algunas más) hicieron que el robot se atascara, aunque algo estaba en el camino, y eso fue suficiente para que confiara en su comportamiento y se produjera el accidente.
Por cierto, Doge se quedó atascado en el baño nuevamente.
Tenemos que quitar las alfombras cada vez antes de dejar que Doge aspire.</p>
<div class="figure"><span id="fig:doge-stuck"></span>
<img src="images/doge-stuck.jpg" alt="Doge, nuestra aspiradora, se atascó. Como explicación del accidente, Doge nos dijo que debe estar en una superficie plana." width="800" />
<p class="caption">
FIGURE 2.2: Doge, nuestra aspiradora, se atascó. Como explicación del accidente, Doge nos dijo que debe estar en una superficie plana.
</p>
</div>
<p>Los modelos de aprendizaje automático solo se pueden <strong>depurar y auditar</strong> cuando se pueden interpretar.
Incluso en entornos de bajo riesgo, como las recomendaciones de películas, la capacidad de interpretación es valiosa en la fase de investigación y desarrollo, así como después de la implementación.
Más tarde, cuando se usa un modelo en un producto, las cosas pueden salir mal.
Una interpretación para una predicción errónea ayuda a comprender la causa del error.
Ofrece una dirección sobre cómo arreglar el sistema.
Considere un ejemplo de un clasificador entre perros siberianos y lobos, que clasifica erróneamente a algunos siberianos como lobos.
Al utilizar métodos de aprendizaje automático interpretables, descubrirá que la clasificación errónea se debió a la nieve en la imagen.
El clasificador aprendió a usar la nieve como una característica para clasificar las imágenes como “lobo”, lo que podría tener sentido en términos de separar a los lobos de los siberianos en el conjunto de datos de entrenamiento, pero no en el uso en el mundo real.</p>
<p>Si puede asegurarse de que el modelo de aprendizaje automático pueda explicar las decisiones, también puede verificar los siguientes rasgos con mayor facilidad (Doshi-Velez y Kim 2017):</p>
<ul>
<li>Equidad: garantizar que las predicciones sean imparciales y no discriminen implícita o explícitamente a ciertos grupos.
Un modelo interpretable puede decirle por qué ha decidido que cierta persona no debería obtener un préstamo, y es más fácil para un humano juzgar si la decisión se basa en un sesgo demográfico aprendido (por ejemplo, racial).</li>
<li>Privacidad: garantizar que la información confidencial de los datos esté protegida.</li>
<li>Fiabilidad o robustez: garantizar que pequeños cambios en la entrada no conduzcan a grandes cambios en la predicción.</li>
<li>Causalidad: compruebe que solo se recogen las relaciones causales.</li>
<li>Confianza: es más fácil para los humanos confiar en un sistema que explica sus decisiones en comparación con una caja negra.</li>
</ul>
<p><strong>Cuando no necesitamos interpretabilidad.</strong></p>
<p>Los siguientes escenarios ilustran cuando no necesitamos o incluso no queremos la interpretabilidad de los modelos de aprendizaje automático.</p>
<p>La interpretabilidad no es necesaria si el modelo <strong>no tiene un impacto significativo</strong>.
Imagine a alguien llamado Mike trabajando en un proyecto paralelo de aprendizaje automático para predecir a dónde irán sus amigos para sus próximas vacaciones en base a datos de Facebook.
A Mike le gusta sorprender a sus amigos con suposiciones educadas sobre dónde irán de vacaciones.
No hay ningún problema real si el modelo está equivocado (en el peor de los casos, solo un poco de vergüenza para Mike), ni hay un problema si Mike no puede explicar el resultado de su modelo.
Está perfectamente bien no tener interpretabilidad en este caso.
La situación cambiaría si Mike comenzara a construir un negocio en torno a estas predicciones de destinos de vacaciones.
Si el modelo está equivocado, el negocio podría perder dinero, o el modelo podría funcionar peor para algunas personas debido al prejuicio racial aprendido.
Tan pronto como el modelo tenga un impacto significativo, ya sea financiero o social, la interpretabilidad se vuelve relevante.</p>
<p>La interpretabilidad no es necesaria cuando el <strong>problema está bien estudiado</strong>.
Algunas aplicaciones se han estudiado lo suficientemente bien como para que haya suficiente experiencia práctica con el modelo y los problemas con el modelo se hayan resuelto con el tiempo.
Un buen ejemplo es un modelo de aprendizaje automático para el reconocimiento óptico de caracteres que procesa imágenes de sobres y extrae direcciones.
Hay años de experiencia con estos sistemas y está claro que funcionan.
Además, no estamos realmente interesados en obtener información adicional sobre la tarea en cuestión.</p>
<p>La interpretabilidad podría permitir a las personas o programas <strong>manipular el sistema</strong>.
Los problemas con los usuarios que engañan a un sistema son el resultado de una falta de coincidencia entre los objetivos del creador y el usuario de un modelo.
La calificación crediticia es un sistema de este tipo porque los bancos quieren asegurarse de que los préstamos solo se otorguen a los solicitantes que puedan devolverlos, y los solicitantes aspiran a obtener el préstamo incluso si el banco no quiere darles uno.
Este desajuste entre los objetivos introduce incentivos para que los solicitantes jueguen con el sistema para aumentar sus posibilidades de obtener un préstamo.
Si un solicitante sabe que tener más de dos tarjetas de crédito afecta negativamente su puntaje, simplemente devuelve su tercera tarjeta de crédito para mejorar su puntaje y solicita una nueva tarjeta después de que el préstamo haya sido aprobado.
Si bien su puntaje mejoró, la probabilidad real de pagar el préstamo se mantuvo sin cambios.
El sistema solo se puede manipular si las entradas son representantes de una característica causal, pero en realidad no causan el resultado.
Siempre que sea posible, se deben evitar las funciones de proxy ya que hacen que los modelos sean manipulables.
Por ejemplo, Google desarrolló un sistema llamado Google Flu Trends para predecir los brotes de gripe.
El sistema correlacionó las búsquedas de Google con los brotes de gripe, y ha tenido un mal desempeño.
La distribución de las consultas de búsqueda cambió y Google Flu Trends perdió muchos brotes de gripe.
Las búsquedas en Google no causan la gripe.
Cuando las personas buscan síntomas como “fiebre”, se trata simplemente de una correlación con los brotes de gripe reales.
Idealmente, los modelos solo usarían características causales porque no serían manipulables.</p>
<!--{pagebreak}-->
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>Doshi-Velez, Finale, y Been Kim. “Towards a rigorous science of interpretable machine learning,” nu. Ml: 1–13. <a href="http://arxiv.org/abs/1702.08608" class="uri">http://arxiv.org/abs/1702.08608</a> ( 2017).<a href="interpretabilidad-importancia.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Heider, Fritz, y Marianne Simmel. “An experimental study of apparent behavior.” The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).<a href="interpretabilidad-importancia.html#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interpretabilidad.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="taxonomia-de-los-metodos-de-interpretacion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
